[
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      16
    ],
    "titles": [
      "16     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 1,
    "text": "16 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Different types of AI systems raise different policy opportunities and challenges. Section 2 of this report introduces and describes a framework to assess AI systems impact on public policy in areas covered by the OECD AI Principles (OECD, 2019d[1]). Section 3 puts the framework into use to classify specific AI systems and applications. Section 4 discusses how the framework could be used to help assess basic social, physical and ethical risks associated with specific types of AI systems. Introducing the framework and its purpose The framework primary purpose is to characterise the application of an AI system deployed in a specific project and context, although some dimensions are also relevant to generic AI systems. It classifies AI systems and applications along the following dimensions: People & Planet, Economic Context, Data & Input, AI Model and Task & Output (Figure 1). These dimensions build on the conceptual view of a generic AI system established in previous OECD work (see Box 1 later in this section). Figure 1. Key high-level dimensions of the OECD Framework for the Classification of AI Systems The AI Principles as a lens for analysing policy considerations Each of the framework s dimensions has distinct properties and attributes, or sub-dimensions that are relevant to assessing policy considerations associated with a particular AI system. The 10 OECD AI Principles, adopted in 2019, help structure the analysis of policy considerations associated with each dimension and sub-dimension. The Principles cover the following themes: Table 1. The OECD AI Principles Values-based principles for all AI actors Recommendations to policy makers for AI policies Principle 1.1. People and planet Principle 2.1. Investment in R&D Principle 1.2. Human rights, privacy, fairness Principle 2.2. Data, compute, technologies Principle 1.3. Transparency, explainability Principle 2.3. Enabling",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      17
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   17"
    ],
    "chunk_index": 2,
    "text": "policy and regulatory environment Principle 1.4. Robustness, security, safety Principle 2.4. Jobs, automation, skills Principle 1.5. Accountability Principle 2.5. International cooperation Source: (OECD, 2019d[1]) 1 Overview and goal of the framework OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 17 OECD 2022 Balancing user-friendliness and accuracy The framework has been designed to be user-friendly (Table 2). It balances simplicity and useful explanations. It includes the basic criteria for which information is likely to be available and that are essential to obtaining relevant results from using the framework. Additional criteria that provide key information on the AI system in question, but for which it has been and may continue to be challenging to find sufficient objective and consistent information are marked as {where objective and consistent information is available}.2 To date, the framework does not address governance at the corporate, institution or AI systems level; nor does it cover the use of mitigation measures or compliance and enforcement measures along the AI system lifecycle (the subject of a related stream of work). Uses for the framework The framework allows users to zoom in on specific risks that are typical of AI, such as bias, explainability and robustness, yet it is generic in nature. It facilitates nuanced and precise policy debate. The framework can also help develop policies and regulations, since AI system characteristics influence the technical and procedural measures they need for implementation. In particular, the framework serves to: Promote a common understanding of AI and its most important characteristics among a variety of stakeholders so they can tailor policies for specific types of AI systems. Describe AI systems and their basic characteristics in algorithm inventories, or registries of automated decision systems, which are being built in several jurisdictions. Provide the basis for more detailed application or domain-specific catalogues of criteria,",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      18
    ],
    "titles": [
      "18     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 3,
    "text": "e.g. in healthcare, finance or industry. For example, the UK Medicines and Healthcare Products Regulatory Agency (MHRA) and the UK National Institute of Health and Care Excellence (NICE) Health Technology Assessment (HTA) programme are using and adapting the classification framework for AI systems to assist in triaging technologies for health technology assessment.3 Provide the basis for a weighed risk-assessment tool that can help measures for mitigation and minimising risk (see Section 4 Next Steps). Inform related work on mitigation, compliance and enforcement along AI systems lifecycles. Key elements of the framework Each of the framework s dimensions has distinct properties and attributes, or sub-dimensions that are relevant to assessing policy considerations associated with different AI systems (Figure 2). Stakeholders include anyone involved in or affected by AI systems. AI actors are stakeholders who play active roles throughout the AI system lifecycle and can vary according to each dimension (OECD, 2019d[1]) 18 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Table 2. Classification framework dimensions and criteria at a glance PEOPLE & PLANET Criteria Description USERS Users of AI system What is the level of competency of users who interact with the system? STAKEHOLDERS Impacted stakeholders Who is impacted by the system (e.g. consumers, workers, government agencies)? OPTIONALITY Optionality and redress Can users opt out, e.g. switch systems? Can users challenge or correct the output? HUMAN RIGHTS Human rights and democratic values Can the system s outputs impact fundamental human rights (e.g. human dignity, privacy, freedom of expression, non-discrimination, fair trial, remedy, safety)? WELL-BEING & ENVIRONMENT Well-being, society and the environment Can the system s outputs impact areas of life related to well-being (e.g. job quality, the environment, health, social interactions, civic engagement, education)? DISPLACEMENT {Displacement potential} Could the system automate tasks that are or were being executed by",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 4,
    "text": "humans? ECONOMIC CONTEXT Criteria Description SECTOR Industrial sector Which industrial sector is the system deployed in (e.g. finance, agriculture)? BUSINESS FUNCTION & MODEL Business function What business function(s) is the system employed in (e.g. sales, customer service)? Business model Is the system a for-profit use, non-profit use or public service system? CRITICALITY Impacts critical functions / activities Would a disruption of the system s function / activity affect essential services? SCALE & MATURITY Breadth of deployment Is the AI system deployment a pilot, narrow, broad or widespread? {Technical maturity} How technically mature is the system (Technology Readiness Level TRL) DATA & INPUT Criteria Description COLLECTION Detection and collection Are the data and input collected by humans, automated sensors or both? Provenance of data and input Are the data and input from experts; provided, observed, synthetic or derived? Dynamic nature Are the data dynamic, static, dynamic updated from time to time or real-time? RIGHTS & IDENTIFIABILITY Rights Are the data proprietary, public or personal data (related to identifiable individual)? Identifiability of personal data If personal data, are they anonymised; pseudonymised? STRUCTURE & FORMAT {Structure of data and input} Are the data structured, semi-structured, complex structured or unstructured? {Format of data and metadata} Is the format of the data and metadata standardised or non-standardised? SCALE {Scale} What is the dataset s scale? QUALITY AND APPROPRIATENESS {Data quality and appropriateness} Is the dataset fit for purpose? Is the sample size adequate? Is it representative and complete enough? How noisy are the data? AI MODEL Criteria Description MODEL CHARACTERISTICS Model information availability Is any information available about the system s model? AI model type Is the model symbolic (human-generated rules), statistical (uses data) or hybrid? {Rights associated with model} Is the model open-source or proprietary, self or third-party managed? {Discriminative or generative} Is the",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      19
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   19"
    ],
    "chunk_index": 5,
    "text": "model generative, discriminative or both? {Single or multiple model(s)} Is the system composed of one model or several interlinked models? MODEL-BUILDING Model-building from machine or human knowledge Does the system learn based on human-written rules, from data, through supervised learning, through reinforcement learning? Model evolution in the field ML Does the model evolve and / or acquire abilities from interacting with data in the field? Central or federated learning ML Is the model trained centrally or in a number of local servers or edge devices? MODEL INFERENCE {Model development / maintenance} Is the model universal, customisable or tailored to the AI actor s data? {Deterministic and probabilistic} Is the model used in a deterministic or probabilistic manner? Transparency and explainability If information available to users to allow them to understand model outputs? TASK & OUTPUT Criteria Description TASKS Task(s) of the system What tasks does the system perform (e.g. recognition, event detection, forecasting)? {Combining tasks and actions into composite systems} Does the system combine several tasks and actions (e.g. content generation systems, autonomous systems, control systems)? ACTION Action autonomy How autonomous are the system s actions and what role do humans play? APPLICATION AREA Core application area(s) Does the system belong to a core application area such as human language technologies, computer vision, automation and / or optimisation or robotics? EVALUATION {Evaluation methods} Are standards or methods available for evaluating system output? Note: Criteria and descriptions in grey and marked with an {} symbol = those where objective and consistent information is available. ML = for machine learning AI models. OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 19 OECD 2022 Figure 2. Characteristics per classification dimension and key actor(s) involved Note: Actors are illustrative, non-exhaustive and notably relevant to accountability. Source: Based on the work of ONE AI and",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      20
    ],
    "titles": [
      "20     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 6,
    "text": "the AI system lifecycle work of AIGO (OECD, 2019f[2]). People & Planet People & Planet are at the centre of the framework. This dimension considers the potential of AI actors to develop applied AI systems that promote human-centric, trustworthy AI that benefits people and planet. It identifies individuals and groups that interact with or are affected by an applied AI system in a specific context. Core characteristics include users and impacted stakeholders, as well as the application s optionality and how it impacts human rights, the environment, well-being, society and the world of work. This dimension is important for public policy because, for example, AI user competency varies, which matters for accountability, transparency and explainability. AI systems that impact specific stakeholder groups such as consumers raise consumer protection and product safety considerations. Users and stakeholders often have different degrees of choice in whether to be subject to the effects of an AI system, or varying ability to opt-out of or reverse a system s output. More broadly, accountability and transparency are critical in contexts where the outcomes of an AI system can impact human rights, such as in criminal sentencing or determining educational opportunities. Actors in this dimension include end-users and stakeholders who use or are impacted by AI systems. Stakeholders encompass all organisations and individuals involved in or affected by AI systems, directly or indirectly. 20 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Economic Context Economic Context refers to the economic and sectoral environment in which an AI system is implemented. It is usually related to an applied AI system rather than to a generic one, and describes the type of organisation and functional area for which an AI system is developed. Characteristics include the sector in which the system is deployed (e.g. healthcare, finance, manufacturing),",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 7,
    "text": "its business function and model; if its nature is critical or non-critical; its deployment, impact and scale, and its technological maturity. This dimension is important for public policy because AI raises sector-specific considerations. These include patient data privacy in healthcare, safety considerations in transportation, transparency and accountability in public services (particularly in areas like security and law enforcement), and security and robustness considerations in critical functions like energy infrastructure. Other characteristics such as AI system maturity are particularly relevant to accountability, R&D investment, safety, robustness and security. AI actors in this dimension include system operators who plan and design and operate and monitor AI systems. Data & Input Data & Input refers to the data and/or expert input with which an AI model builds a representation of the context or environment (both the Economic Context and People & Planet context). Expert input is typically human knowledge that is codified into rules. Characteristics include the provenance of data and inputs, machine and/or human collection method, data structure and format, and data properties. Data & Input characteristics can pertain to data used to train an AI system ( in the lab ; see next section for additional explanation) and data used in production ( in the field ; see next section). One of the key reasons this dimension is important for public policy is because, for one, systems that process personal or sensitive data generate concerns about privacy, inclusiveness, human rights and bias/fairness. Data that is generated synthetically for scenario simulation (e.g. a car accident) or to supplement non-representative data sets are relevant for safety and inclusiveness. The degree to which data are static or dynamic is relevant for accountability, particularly when assessing AI systems that iterate and evolve over time, or change their behaviour in unforeseen ways. Whether data are proprietary,",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      21
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   21"
    ],
    "chunk_index": 8,
    "text": "public or personal is relevant to transparency and explainability; bias; scale-up; economic, social and environmental impacts; research; data availability and computational capacity. AI actors in this dimension include data collectors and processors who collect and process data, by gathering and cleaning data, labelling, performing checks for completeness and quality, and documenting the characteristics of the dataset. AI Model An AI model is a computational representation of all or part of the external environment of an AI system encompassing, for example, processes, objects, ideas, people and/or interactions that take place in that environment. AI models use data and/or expert knowledge provided by humans and/or automated tools to represent, describe and interact with real or virtual environments. Core characteristics include technical type, how the model is built (using expert knowledge, machine learning or both) and how the model is used (for what objectives and using what performance measures). The AI Model dimension is important for public policy because key properties of AI models degree of transparency and/or explainability, robustness, and implications for human rights, privacy and fairness depend on the type of model as well as the model-building and inferencing processes. For example, systems using neural networks are often seen as having the potential to provide comparatively higher accuracy but less explainability than other types. Explainability is often tied to system complexity; the more OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 21 OECD 2022 complex a model is, the harder it is to explain. The degree to which a model evolves in response to data is relevant to public policy and consumer protection regimes, especially for AI systems that can learn from iterations and evolve over time. Understanding how a model was developed and/or maintained is another key consideration for assigning roles and responsibilities throughout risk management processes. AI actors in",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 9,
    "text": "this dimension include developers and modellers who build and use models and verify and validate them. Task & Output The Task & Output dimension refers to the tasks the system performs, e.g. personalisation, recognition, forecasting or goal-driven optimisation; its outputs; and the resulting action(s) that influence the overall context. Core characteristics of this dimension include system task(s); action autonomy; systems that combine tasks and actions like autonomous vehicles; core application areas like computer vision; and evaluation methods. This dimension is important for public policy because personalisation tasks, for example, generate outputs that could raise bias and fairness issues. Recognition tasks can raise concerns of human rights, robustness and security as well as bias. Moreover, the actions taken based on the outcomes of an AI system, e.g. by autonomous vehicles, generate issues of fairness, safety, security and accountability. More broadly, the level of autonomy in an AI system s actions and the role of humans also raise important questions for human rights and fundamental values, among other concerns. AI actors in this dimension include system integrators who deploy AI systems. Applying an AI system in the lab and/or in the field As already mentioned, some criteria of the framework are more applicable to AI in the field contexts than to AI in the lab contexts, and vice versa in the field : AI in the lab refers to the AI system s conception and development, before deployment. It is applicable to the Data & Input (e.g. qualifying the data), AI Model (e.g. training the initial model) and Task & Output dimensions (e.g. for a personalisation task) of the framework. It is particularly relevant to ex-ante risk-management approaches and requirements. AI in the field refers to the use and evolution of an AI system after deployment and is particularly relevant to ex-post",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      22
    ],
    "titles": [
      "22     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 10,
    "text": "risk-management approaches and requirements. It is applicable to all the dimensions, including the People & Planet and Economic Context dimensions. In addition, it is important to underscore that an AI system in the field can change in many significant ways over time, especially with regards to breadth of deployment, technological maturity, users and capabilities. For example, this can happen through improved or different datasets that become available for model- building. The framework s definitions, concepts and criteria are designed to be dynamic since the classification of an AI system may change. This may happen when a system evolves and incorporates new data and techniques, is deployed more widely, matures or grows in capacity. In addition, the framework may need to be reviewed at regular intervals for continued relevance in view of social, technical and legal developments that may affect AI systems as well as the contexts in which they evolve. 22 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Figure 3. Application of an AI system in the lab or in the field Link between the classification and actors in the AI system lifecycle The AI system lifecycle can serve as a complementary tool to understand a system s key technical characteristics and encompasses the following phases: planning and design; collecting and processing data; building and using the model; verifying and validating; deployment; and operating and monitoring (OECD, 2019d[1]). These phases often take place in an iterative manner and are not necessarily sequential. The decision to retire an AI system from operation may occur at any point during the operating and monitoring phase. The dimensions of the OECD Framework for the Classification of AI Systems can be associated with different stages of an AI system s lifecycle. This is useful for identifying which actors are relevant to each",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      23
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   23"
    ],
    "chunk_index": 11,
    "text": "dimension, which matters for accountability and risk management measures (OECD, 2019d[1]).4 AI actors are those who play an active role throughout the AI system lifecycle and can include organisations and individuals that deploy or operate AI (OECD, 2019d[1]). AI actors are a subset of stakeholders and may differ depending upon the dimension. As mentioned above, lifecycle actors and stakeholders in the different dimensions are: People & Planet: End-users and stakeholders who use or are impacted by AI systems. Stakeholders encompass all organisations and individuals involved in or affected by AI systems, directly or indirectly. Economic Context: System operators who plan and design and operate and monitor applied AI systems. Data & Input: Data collectors and processors who collect and process data, including gathering and cleaning data, labelling, performing checks for completeness and quality, and documenting the characteristics of the dataset. AI Model: Developers and modellers who build and use models and verify and validate them. Task & Output: System integrators who deploy AI systems. CONTEXT DATA & INPUT TASK & OUTPUT PEOPLE & PLANET AI System AI in the field AI MODEL AI in the lab OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 23 OECD 2022 Figure 4. The AI system lifecycle Note: Actors are illustrative and not exhaustive and based on previous OECD work on the AI system lifecycle. Source: Based on the AI system lifecycle work of AIGO (OECD, 2019f[2]). Box 1. Characterisation of an AI system based on the OECD AI Principles (2019) An AI system is a machine- based system that is capable of influencing the environment by producing recommendations, predictions or other outcomes5 for a given set of objectives. 6 It uses machine and/or human- based inputs/data to: 1) perceive environments; 2) abstract these perceptions into models; and 3) use the models to formulate",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      24
    ],
    "titles": [
      "24     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 12,
    "text": "options for outcomes.7 AI systems are designed to operate with varying levels of autonomy (OECD, 2019f[2]). Figure 5. Stylised conceptual view of an AI system (per OECD AI Principles) Source: (OECD, 2019f[2]) It should be noted that in the present classification framework, the Perceiving (data collection) and data/input elements that were separate elements in the original OECD work presented in Figure 5 have been combined in an effort to simplify the framework (see dotted lines in Figure 4). The Outcomes and Acting elements have also been combined. 24 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Other important scoping considerations The framework may need to be reviewed at regular intervals for its dynamic nature, as mentioned above but also for continued relevance in view of social, technical and legal developments that may affect AI systems as well as the contexts in which they evolve. The framework s dimensions are designed as independent, orthogonal units that affect one another. For example, tasks linked to the Task & Output dimension in the framework impact the Data & Input dimension and how the AI Model dimension is formulated. It also matters whether or not the data was collected for a specific purpose. Collecting data for one purpose and then using it for a purpose that was not intended during the collection stage can cause a misalignment between a task s ideal objective and the approximation of the objective by the data provided. The degree of generality of an AI system refers to its ability to perform several tasks including ones for which it was not initially trained. While there is no single indicator of generality, several criteria in this framework can indicate generality when combined and where objective and consistent information is available. These include: 1) Scale ; 2) Model development/maintenance",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      25
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   25"
    ],
    "chunk_index": 13,
    "text": "; and 3) Combining tasks and actions into multi-task, composite systems . The framework s dimensions are meant to be used when considering issues in any policy domain that may be affected by an AI system. For example, labour policy makers might take into account: 1) People & Planet: job creation and displacement, access to training; 2) Economic Context: dismissal policy and social dialogue/worker consultation in the industry of deployment; 3) Data & Input: job automation by sensors, new jobs in areas such as data science or data labelling; 4) AI Model: building AI skills and attracting talent; and 5) Task & Output: task automation and impacts on job quality and quantity. Health technology standards might take into account: 1) People & Planet: patients, stakeholders and planet; 2) Economic Context: health technology efficiency; 3) Data & Input: clinical records and trial data; 4) AI Model: transparency and explainability; 5) Task & Output: output description, post deployment change management plan, benchmarking, and oversight. OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 25 OECD 2022 This section describes the characteristics of each of the framework s dimensions, key actors in each dimension and their role in the AI system lifecycle. The roles that actors play are especially relevant to the principle of accountability. The framework provides a structured way to assess AI systems potential to promote the development of human-centric, trustworthy AI as set out in the OECD AI Principles, i.e. AI systems that benefit people and planet; uphold human rights, democratic values and fairness; are transparent and explainable; are robust, secure and safe; and whose operators are accountable and to implement policy recommendations in the areas of AI R&D investment; data, compute, technologies; enabling policies and regulation; labour and skills; and international cooperation (OECD, 2019d[1]). People & Planet People & Planet",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 14,
    "text": "are at the centre of the framework (see Figure 1). The framework focuses on human rights and well-being in considering how people as a whole interact with and are affected by an AI system throughout that system s lifecycle. People interact with AI systems in many ways from designing and deciding what data to collect and how to collect it, labelling data, choosing baselines and performance criteria, to putting in place explainability and evaluation mechanisms. In using the framework to review an AI system for acquisition and deployment, the following definitions apply: Users of an AI system or application are the individuals or groups that utilise the system for a specific purpose. Impacted stakeholders can be indirectly or directly affected by the deployment of an AI system or application but do not necessarily interact with the system. An AI system or application can impact several different stakeholder groups. For example, regarding a credit scoring system, the intended users are typically bank employees who use the system to assess customers creditworthiness. The impacted stakeholders are the consumers as well as the regulatory body overseeing financial stability. The following sections describe key criteria of AI users and impacted stakeholders, and how the AI system impacts them indirectly or directly. Users AI competency Often, the users and intended users of an AI system are not those who developed and implemented it; nor do they usually operate it. Users can range in competency from AI experts to amateur end-users. For example, AI systems deployed in sectors such as healthcare or agriculture are often used by practitioners or domain experts who are not typically AI experts. In light of the implications of the wide variety in levels of expertise among end-users, AI systems can be distinguished based on whether their typical users have any systems",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      26
    ],
    "titles": [
      "26     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 15,
    "text": "operation training, according to the following terminology: Amateur: User who has no training8 2 Classification framework 26 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Trained practitioner who is not an AI expert: User with some specific training on how to use the AI system in question AI expert: User with specific training and knowledge of how AI works in the application or system considered (an AI expert or system developer) Other Why does this matter? AI system users relate to accountability (Principle 1.5); transparency and explainability (Principle 1.3); and safety, security and robustness (Principle 1.4). Impacted stakeholders The following stakeholders may be impacted directly or indirectly, and consciously or unconsciously by the AI system: Workers/employees Consumers Business Government agencies/regulators Scientists/researchers Children or other vulnerable or marginalised groups Why does this matter? Stakeholders impacted by the system are most relevant to transparency and explainability (Principle 1.3) and to policy and regulatory frameworks (Principle 2.2). Stakeholder groups such as consumers, workers/employees or children are often covered by existing policy and regulatory regimes. In Europe, the General Data Protection Regulation (GDPR) gives data subjects (those whose data is collected, held or processed) the right, under some circumstances, to not be subject to automated decision-making. Optionality and redress Optionality or dependence refers to the degree of choice that users or impacted stakeholders have on whether or not they are subject to the effects of an AI system, whether their involvement is active or passive. Optionality can be understood as the extent to which users can opt out of the effects or the influence of the AI system, e.g. by switching to another AI system and the societal repercussions of doing so, e.g. for access to healthcare or financial services. This is also referred to as switchability (AI Ethics Impact Group, 2020[8]).",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      27
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   27"
    ],
    "chunk_index": 16,
    "text": "It is important to consider the human aspect or the degree to which they are involved in developing AI systems and models, of the operation and outputs of the system, and if humans are in , on and out-of-the-loop . The following are generally considered to be distinct modes of optionality in a given AI system: Users cannot opt out of the AI system s output. Users can opt out of the AI system s output. Users can challenge or correct the AI system s output. Users can reverse the AI system s output ex-post. Benefits and risks to human rights and democratic values Some AI systems generate outputs that can impact individuals human rights, either negatively or positively (see Table 3). Low-risk contexts for individuals, such as a restaurant recommendation may not determine OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 27 OECD 2022 an individuals actions in an area of life that is directly related to fundamental human rights, such as health or fair access to employment, for instance, even if they affect certain aspects of self-determination. As a result, systems operating in low-risk contexts may not need multi-layered and costly approaches to verification and can therefore rely mostly on machines, provided there are adequate protections regarding combining different datasets at aggregate levels. Having humans in the loop of certain AI system processes and/or a human appeal process are important to reducing risk. Table 3. Sample checklist for assessing the potential impact of an AI system on selected human rights and democratic values, direct or indirect Impact of AI on human rights or democratic values Outcome- dependent No impact Liberty, safety and security Physical, psychological and moral integrity Freedom of expression, assembly and association Freedom of thought, conscience and religion Rule of law, absence of arbitrary sentencing Equality",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 17,
    "text": "and non-discrimination Social and economic rights (e.g. health, education) Quality of democratic institutions (e.g. free elections) Right to property Aggregate society-level risk (please detail) Other (detail) Note: International human rights refer to a body of international laws, including the Universal Declaration of Human Rights, as well as regional human rights systems such as that of the Council of Europe. Human rights provide a set of universal minimum standards based on, among others, values of human dignity, autonomy and equality, in line with the rule of law. Human rights overlap with wider ethical concerns and with other areas of regulation relevant to AI, such as personal data protection or product safety law. Risk-assessment frameworks would usually also include the likelihood the risk will occur, its impact and mitigation measures. Source: Based on (UN General Assembly, 1948[3]) (CoE, 2020[4]), (CoE, 1998[5]) and (OHCHR, 2011[6]). Why does this matter? Transparency and explainability (Principle 1.3), as well as accountability (Principle 1.5), are widely viewed as having higher importance in contexts where the outcomes of an AI system can impact human rights. Examples include AI used to sentence criminals, recommend decisions about educational opportunities or conduct job screenings. Such high-stakes situations often require formal transparency and accountability mechanisms (Principles 1.3 and 1.5), including transparency about the role of AI and human involvement in the process (e.g. human-in-the-loop), the full consequences of the AI system s action on all stakeholders and the availability of appeals processes, particularly where life and liberty are at stake. Across the spectrum, people broadly agree that AI-based outcomes (e.g. a credit score) should not be the only decisive factor when applications or decisions have a significant impact on people s lives. Such applications may for example require that a human consider the social context, the precise decisions enabled by the AI",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      28
    ],
    "titles": [
      "28     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 18,
    "text": "system as well as its limitations and the variables it uses to help avoid unintended consequences. For example, the GDPR stipulates that a human must be in the loop if a decision has legal or similarly significant effects on people.9 Benefits and risks to the environment, well-being and society Many AI systems use human data as inputs and generate outputs that can impact individuals and societies 10 well-being, either positively or negatively.11 This impact pertains to different areas of life, such as work and job quality, environment quality, social connections and civic 28 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 engagement, among others (Table 4). In addition, AI systems can impact societal well-being in the aggregate. Table 4. Sample checklist for assessing potential impact of an AI system s outcomes on well-being Impact of AI on well-being Outcome- dependent No impact Physical and mental health Housing Income and wealth Quality of job Quality of environment Social connections Civic engagement Education, knowledge and skills Work-life balance Aggregate society-level impact (please detail) Duration of impact Note: Outcomes can be captured by measures of inequality, for example. Source: (OECD, 2020[7]). Why does this matter? In the current context of accelerating climate change and loss of biodiversity across the planet, AI brings significant opportunities to mitigate risks and to help adapt. These and other dimensions of universal well-being call for responsible AI that is based on algorithms and optimisation functions that are human-centric, user-defined, guarantee benefits for people and planet (Principle 1.1) and maintain accountability (Principle 1.5). International cooperation (Principle 2.5) is a must in the face of urgent global challenges. Work, human and employment displacement potential {where objective and consistent information is available} An AI system s ability to automate tasks previously or currently conducted by humans depends on a",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      29
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   29"
    ],
    "chunk_index": 19,
    "text": "variety of task-dependent factors such as perception and manipulation requirements, presence of uncertainty in a task, and creative and social intelligence factors.12 Historically, automation has been limited to tasks that: require perceiving and manipulating homogeneous objects with clearly defined processes and limited uncertainty; are conducted within controlled environments; and do not require creativity or social interaction. However, recent innovations in AI are changing the automation landscape and more tasks typically executed by higher-skilled workers are being automated. For simplicity, an AI system s potential to automate tasks can be split into three categories, listed below. High displacement potential: AI systems that perform tasks that use clearly defined processes and outputs (e.g. tasks performed by clinical lab technicians, optometrists, chemical engineers, actuaries, credit analysts, accountants, operations research analysts, concierges, mechanical drafters, brokerage clerks and quality control inspectors). This does not imply a high likelihood of being replaced by AI. That would require a more complex assessment of the technical feasibility and context of the task to be performed. OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 29 OECD 2022 Low displacement potential: AI systems perform tasks that require reasoning about novel situations (e.g. research), interpersonal skills (e.g. teachers and managers, some baristas) and physical occupations that require perception and manipulation of a plurality of irregular objects in uncontrolled environments with limited room for mobility (e.g. maids, cleaners, cafeteria attendants, hotel porters, roofers and painters, massage therapists, plasterers and stucco masons). This does not necessarily mean that the occupation will not see significant automation of key tasks. No displacement potential: Some AI systems execute tasks that could not be performed by humans with the same accuracy, specificity or scale (e.g. AI systems used in cybersecurity and threat detection). Why does this matter? An AI system s capacity to automate tasks and improve",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      30
    ],
    "titles": [
      "30     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 20,
    "text": "worker productivity can impact the world of work (Principle 2.4). This impact of AI will have implications for education strategies for affected groups as well as potential policies to share the benefits of increased worker productivity. Key actors in the People & Planet dimension are end-users and stakeholders The end-users (or intended users) of an AI system or application are the individuals or groups that use the system for a specific purpose. The impacted stakeholders encompass all organisations and individuals involved in or affected by AI systems, directly or indirectly. They do not necessarily interact with the system and can be indirectly or directly affected by the deployment of an AI system or application. 30 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Economic Context According to this framework, the Economic Context dimension of an AI system represents its socio- economic environment, including its broader natural and physical environment. This dimension is mostly relevant to a specific application of an AI system rather than to a generic AI system. The context is observable and can be influenced through actions resulting from an AI system s outputs (OECD, 2019f[2]). Core characteristics of the Economic Context, described in more detail in the sections that follow, include the sector in which an AI system is deployed, its business function, its critical (or non-critical) nature, and its deployment impact and scale. The key AI actor in this dimension is the system operator. Industrial sector AI is diffusing rapidly throughout industrial sectors and is increasingly applied in fields such as finance and insurance, advertising, transport, manufacturing and healthcare. Each industrial sector represents a different context that has different implications, in terms of industry structure, regulation and policy making, for AI systems. In April 2021, the European Commission (EC) proposed an AI-related package of",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      31
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   31"
    ],
    "chunk_index": 21,
    "text": "governance guidelines and regulations, including a proposal for AI regulation using a risk-based approach that incorporates several risks linked to other sectors, notably healthcare, transport, energy and parts of the public sector (e.g. law enforcement, migration, border control, judiciary, social security and employment). It should be noted that some applications of AI span multiple industries or even multiple functions within a single industry for example, recruitment. This classification framework uses the International Standard Industrial Classification of All Economic Activities (ISIC REV 4), which allows for comparability with other sources of cross-country data on employment, skills, demography of enterprises, value-added and more. The highest sectoral-level categories of economic activities, sections include (this document does not address AI systems used in military contexts): Section A Agriculture, forestry and fishing Section B Mining and quarrying Section C Manufacturing Section D Electricity, gas, steam and air conditioning supply Section E Water supply; sewerage, waste management and remediation activities Section F Construction Section G Wholesale and retail trade; repair of motor vehicles and motorcycles Section H Transportation and storage Section I Accommodation and food service activities Section J Information and communication Section K Financial and insurance activities Section L Real estate activities Section M Professional, scientific and technical activities Section N Administrative and support service activities Section O Public administration and defence; compulsory social security Section P Education Section Q Human health and social work activities Section R Arts, entertainment and recreation Section S Other service activities OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 31 OECD 2022 Section T Activities of households as employers; undifferentiated goods- and services-producing activities of households for own use Section U Activities of extraterritorial organizations and bodies Why does this matter? The policy implications of deploying AI systems vary significantly from one sector to the next. The industrial sector",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      32
    ],
    "titles": [
      "32     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 22,
    "text": "has a particularly high impact on economic and social benefits (Principle 1.1) and on jobs and skills (Principle 2.4). Business function and model For cross-functional use of AI in an organisation such as in recruitment, promotion, training or even dismissal the business function classification will be very important. Functional areas in organisations in which AI systems can be employed include but are not limited to: Human resource management Sales ICT management and information security Marketing and advertisement Logistics Citizen/customer service Procurement Maintenance Accounting Monitoring and quality control Production Planning and budgeting Research and development Compliance and justice Why does this matter? Different AI systems can perform the same task in different functional areas, with different implications for policy making. For instance, a forecasting algorithm used to improve (optimise) logistics may have different implications than a forecasting system designed to support hiring decisions. The business function for which the AI system is used will thus have a specific impact on economic and social benefits (Principle 1.1); fairness and absence of bias (Principle 1.2); security, safety and robustness (Principle 1.4); and jobs and skills (Principle 2.4) Business model: for-profit use, non-profit use or public service Operators may use an AI system (OECD, 2011[8]) in the following ways: For-profit use subscription fee model For-profit use advertising model For-profit use other model Non-profit use (outside public sector) voluntary donations and community models Public service Other 32 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Why does this matter? The use case and business model of the AI system operator can be relevant to determining the objectives that an AI system is optimising for, e.g. maximising engagement in advertising- based business models. Impacts on critical functions and activities Critical functions are economic and social activities for which the interruption or disruption would have",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 23,
    "text": "serious consequences. They include: 1) the health, safety, and security of citizens; 2) the effective functioning of services essential to the economy and society, and of the government; or 3) economic and social prosperity more broadly (OECD, 2019a[9]); (OECD, 2019e[10]). Public safety, health and consumer protection are vast domains many professions use AI in assessments (e.g. pilots, nuclear reactor operators, law enforcement). It is important to note that the risks associated with the use of AI in critical functions should be weighed against the benefits and requirements for security and the accuracy of outcomes. Further, not all systems in a critical sector are considered critical. For example, administrative time-tracking systems in hospitals and banks are not considered critical systems. Critical systems and activities are defined as follows: AI system deployed in a critical sector or infrastructure (e.g. energy, transport, water, health, digital infrastructure and finance). AI system performs or serves a critical function independent from its sector (e.g. conducting elections, maintaining supply chains, law enforcement, providing medical care, supporting the financial system). Why does this matter? In some sectors, critical functions are accompanied by heightened risk considerations with ex-ante regulations. The critical function will have a particular impact on security, safety and robustness (Principle 1.4). In the European Union, the Network and Information Security (NIS) Directive mandates the supervision of critical sectors. EU Member states must supervise the cybersecurity of critical market operators ex-ante in critical sectors like energy, transport, water, health, digital infrastructure and finance, and ex-post surveillance is required for critical digital service providers like online market places, cloud services and online search engines. In the United States, national critical functions include conducting elections, maintaining supply chains, law enforcement and providing medical care (CISA, 2019[11]). In the financial sector, banks operate some critical functions, like the SWIFT system",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      33
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   33"
    ],
    "chunk_index": 24,
    "text": "for sending payment orders between financial institutions. Scale of deployment and technology maturity AI systems economic and social impact varies depending on the following four factors: breadth of an AI system s deployment; its maturity, from a technology standpoint; stakeholder(s) impacted by the system; and the for-profit, non-profit or public service use of the system. Breadth of deployment The breadth of deployment of an AI system relates to the number of individuals affected by a system and is characterised by the following attributes: Pilot project Narrow deployment: Deployment is at the level, for example, of one company or of one small, targeted region Broad deployment: Deployment is at the level, for example, of one sector across different countries Widespread deployment: Deployment reaches across countries and sectors OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 33 OECD 2022 Technical maturity {where objective and consistent information is available} The maturity of deployed AI systems can vary widely. Technology readiness levels (TRLs) can help classify an AI system s technical maturity. The following nine TRL categories are based on Joint Research Centre (JRC) analysis (Martinez Plumed, 2020[12]) building on the NASA TRL framework (Mankins, 1995[13]). It should be noted that TRLs are viewed as a rough measure (Terrile et al., 2015[14]) Basic principles observed and reported TRL 1: Lowest level of technology readiness, where research begins to be translated into applied R&D. Sample output might be a scientific article on a new technology s principles. Technology concept and/or application formulated TRL 2: Speculative practical applications are invented based on assumptions not yet proven or analysed. Sample output might be a publication or reference highlighting the applications of the new technology. Analytical and experimental critical function and/or characteristic proof-of-concept TRL 3: Continued research and development efforts include analytical studies and lab studies to physically",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 25,
    "text": "validate analytical predictions of separate elements of the technology. Sample output might be measurement of parameters in the lab. Component and/or layout in controlled environment TRL 4: Basic technological components are integrated to verify they can work together but in a relatively superficial manner. Sample output might be integration of ad hoc software or hardware in the laboratory. Component and/or layout validated in relevant environment TRL 5: Reliability is significantly increased and basic technological components integrated with fairly realistic supporting elements that can be tested in a simulated environment. Sample output might be realistic laboratory integration of components. Representative model or prototype system demonstrated in relevant environment TRL 6: Sample output might be testing a prototype in a realistic laboratory environment or in a simulated operational environment. System prototype demonstration in operational environment TRL 7: Examples include testing the prototype in operational testing platforms (real-world clinical setting, vehicle, etc.). System or subsystem complete and qualified through test and demonstration TRL 8: Technology proved to work in its final form and under expected conditions. In most cases, this TRL represents the end of true system development. Examples include developmental testing and evaluation of the system to determine if the requirements and specifications are fulfilled. Actual system or subsystem in final form in operational environment TRL 9: Actual application of the technology in conditions such as those encountered in operational conditions Strong monitoring and improvement processes are critical to continue to improve the system. Why does this matter? AI system maturity is particularly relevant to safety, robustness and security (Principle 1.4); accountability (Principle 1.5); and R&D investment (Principle 2.1). Key actors in the Economic Context dimension include system operators AI system operators are key AI actors in the Economic Context dimension, which can be associated with the planning and design or specification",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      34
    ],
    "titles": [
      "34     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 26,
    "text": "stage of the AI system lifecycle as well as, following deployment, with the operating and monitoring phase. Planning and design of an AI system involves defining the system and its objectives, underlying assumptions, context and requirements (OECD, 2019f[2]). The planning and design or specification phase is critical for public policy, and any major public failures and issues related to other sections of the OECD framework can be avoided if first addressed at the specification phase. For example, in the case of a credit system, the operator of the system is likely to be the bank s information 34 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 technology department. Planning and design processes for this system, then, might require expertise from data scientists, domain experts and governance experts. Operating and monitoring an AI system involves continuously assessing its recommendations and impacts (both intended and unintended) in light of the system s objectives as well as the ethical considerations that go into its operation. In this phase, problems are identified and adjustments made by reverting to other phases or, if necessary, retiring an AI system from production. For monitoring purposes, AI system operators can establish: Transparent, accessible information about the AI system s objectives and assumptions: Provide interested stakeholders with access to useful information. Performance monitoring mechanisms: Such as metrics to assess the performance and accuracy of the AI system. Tools or processes for developing or maintaining trustworthy AI: Using tools like guidelines; governance frameworks; product development or lifecycle tools including for model robustness; risk management frameworks; sector-specific codes of conduct; process standards; technical validation approaches; technical documentation; technical standards; toolkits, toolboxes or software tools; educational material; change-management processes; certification (technical and/or process-related); or tools for protection against adverse attacks. Why does this matter? Key actors in the Economic Context",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      35
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   35"
    ],
    "chunk_index": 27,
    "text": "dimension are often AI system operators who plan and design the AI system and, following deployment, operate and monitor it. System operators relate to accountability (Principle 1.5); transparency and explainability (Principle 1.3); and safety, security and robustness (Principle 1.4). OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 35 OECD 2022 Data & Input An AI system can be based on expert input or on data, both of which can be generated by humans or automated tools such as machine-learning algorithms. Historically, AI systems were powered by expert input in the form of logical representations that formed the basis of early optimisation and planning tools such as those used in medical diagnosis, credit-card fraud detection or in chess playing (e.g. IBM s Deep Blue). They needed researchers to build detailed decision structures to translate real-world complexity into rules to help machines arrive at human-like decisions. Expert input also includes structures such as ontologies, knowledge graphs, decision rules and analytical functions (see section on Structure of data ).13 In recent years, AI systems have become more and more statistical and probabilistic and are increasingly powered by a growing variety of data types. Data can be collected and processed during development in the lab as well as during production or run time in the field. Most of the characteristics of AI system data are relevant to both training data and data used in the field, except for the dynamic nature (the degree to which it changes or updates) of the data, which is relevant primarily during production, or in the field. Core characteristics of the Data & Input dimension are data provenance (where the data comes from); data collection and origin (e.g. data collection, origin, dynamic nature and scale); domain (e.g. personal, proprietary or public); data quality and appropriateness; and their technical characteristics",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      36
    ],
    "titles": [
      "36     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 28,
    "text": "(e.g. structure and encoding). The next sections draw from OECD work (OECD, 2019[15]). Collection method, provenance and dynamic nature Detection and collection of data and input Humans or machines can detect and collect ( track ) data and input from the context or environment by: Collected by humans: This takes place when a human is needed to observe and collect information that requires subjective judgment, such as a person s mental state. Other examples of data collected by humans are crowd-sourcing data and human-based computation, where certain steps of the computation process are conducted by humans. Collected by automated sensors: Devices that automatically monitor and record data include cameras, microphones, thermometers, laboratory instruments and other sensors such as Internet of Things (IoT) devices, but also the automated recording of information from online log files, mobile phones, GPS watches and activity wristbands. Collected by humans and automated sensors: Some data are collected by humans together with automated tools. In healthcare applications, data from sensors such as heartbeat or blood pressure detectors will often be combined with a doctor s assessment. Why does this matter? Data and input collection through automated sensing can benefit society in fields such as healthcare and safety (e.g. activity trackers associated with health applications) or environmental applications. Data and input collection can also surface labour-market considerations (Principle 2.4), including the automation of tasks (e.g. security surveillance or maintenance assessments); improving worker safety and satisfaction; measuring worker productivity; and codifying expert knowledge. 36 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Provenance of data and input The following list draws on the data provenance categorisation made by Abrams (Abrams, 2014[16]) and the OECD (OECD, 2019[15]) of data collected with decreasing levels of awareness. It should be noted that these categories can overlap and most systems will",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 29,
    "text": "combine data from different sources. Here, we broaden the original categorisation that focused on personal data to also cover expert input and non-personal data, as well as data that are synthetically generated. Expert input: Human knowledge that is codified into rules and structures such as ontologies (concepts and properties), knowledge graphs and analytical functions (e.g. the objective function or rewards an AI model will optimise for). Provided data: Data that originate from actions by individuals or by organisations that are aware of the data being provided. They include initiated (e.g. a license application), transactional (e.g. bills paid) and posted (e.g. social networking posts) data. Observed data: Collected through observation of a behaviour or activity through human observation or the use of automated instruments or sensors. Examples include website visitor provenance and browsing patterns observed by a website administrator. Observed data also include sounds, scents, temperature, GPS position or soil acidity. Observed data about individuals can be engaged (e.g. voluntarily accepting cookie tracking on a website), unanticipated (e.g. the tracking of seconds spent looking at a specific image online) or passive (e.g. CCTV images of individuals). Synthetic data: Usually generated by computer simulations, including data collected through reinforcement learning. Synthetic data allow for simulation of scenarios that are difficult to observe or replicate in real life (e.g. a car accident) or are otherwise too expensive to collect at scale (e.g. millions of miles of driving time for self-driving cars). They include most applications of physical modelling, such as music synthesisers or flight simulators. AI system output of synthetic data approximates reality but is generated algorithmically. Derived data: Data taken from other data to become a new data element. Derived data include computational (e.g. a credit score) and categorical data (e.g. age group of a buyer). They can be inferred (e.g. the",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      37
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   37"
    ],
    "chunk_index": 30,
    "text": "product of a probability-based analytic process like a fraud score or risk of accident) or aggregated (e.g. abstracted from more fine-grained data). Proprietary data are often characterised as derived data. Why does this matter? Awareness and consent for the provision of personal data about individuals is a critical focus area for privacy and consumer protection (Principle 1.2). Synthetic data allow for simulation of scenarios that are difficult to observe or replicate in real life (e.g. a car accident) and are relevant to safety (Principle 1.4). Expert input is typically human knowledge that is codified into rules. Dynamic nature of data Data can be static or dynamic , to varying degrees: Static data: These data do not change after they are collected (e.g. a given publication, a product s batch number or the geographic latitudes and longitudes of a fixed element like a building or a mountain). Dynamic data updated from time-to-time: Dynamic data continually change after they are recorded in order to maintain their integrity. Models relying on dynamic data can leverage incremental algorithms that update the model frequently based on incoming data. Dynamic data can be updated from time-to- time without necessarily being real-time data. Examples include timetables of flights estimated time of arrival using batch processing. Dynamic real-time data: Dynamic real-time data are delivered immediately after collection with no delay. Examples of systems that use real-time data processing include an alarm system triggered by OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 37 OECD 2022 an entry signal, a recommender system that evolves in real-time as it is being used (e.g. with a streaming video service like YouTube) and an autonomous driving system that reacts to real-time environmental data. Why does this matter? The degree to which data are static or dynamic is particularly relevant to public policy",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 31,
    "text": "and accountability (Principle 1.5) for AI systems that can iterate and evolve over time and may change their behaviour in unforeseen ways. Scale {where objective and consistent information is available} The scale of a dataset is a continuous variable that has an ever-increasing upper limit. If real-time, scale can be roughly measured as the order of magnitude of bytes per time unit (e.g. tens of petabytes per second) or the number of requests to the AI system per second. If static, size is measured in bytes (e.g. hundreds of gigabytes). The scale of data continues to change as technology advances. The upper limit is generally reached by very few government and commercial enterprises that accommodate high-velocity, real-time data streams supporting extremely large data volumes. The scale of data can be: Very large: One exabyte (one billion gigabytes) or larger. Extremely large volumes of data take time to gather/accumulate and require complex systems to operate and process. Large: Tens of petabytes (per second if real-time). Medium: Hundreds of gigabytes. Small: Tens of gigabytes or smaller. There are no constraints to transferring and processing small amounts of data in current broadband networks and computing environments. Why does this matter? AI powered by machine-learning technology is known to rely on large volumes of data to function well, based on which patterns are inferred. There is active research on AI systems that use less data, such as one-shot learning (Principle 2.1). These AI systems are learning through self-play via reinforcement learning to drastically reduce the scale of the data needed to train a model. In terms of robustness of AI systems (Principle 1.4), researchers have found that there is a trade-off between the quantity of data and the number of variables in a model. A larger model one with more parameters consumes more input",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      38
    ],
    "titles": [
      "38     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 32,
    "text": "resources (e.g. compute capacity, data) than smaller models. However, at large scales, large models can learn to use data more efficiently than smaller models, leading to the counterintuitive result that larger models can match the performance of smaller models while using less data. This has implications for situations where training-data samples are expensive to generate, which likely confers an advantage to large companies entering new domains with models based on supervised learning.14 Data size also relates to the efforts to build the technology infrastructure to process, transfer and share large volumes of data for AI (Principle 2.2). Rights, or domain, and identifiability Rights associated with data and input This sub-dimension distinguishes the rights, or domain, associated with data and input used by an AI system, and the policy implications when used in training data and/or in a deployment context. Data domains include the following three categories, which can overlap in certain applications (see Figure 6): 38 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Proprietary data: Data that are privately held, often by corporations, and typically protected by intellectual property rights including copyright and trade secrets or by other access and control regimes such as contract and cyber-criminal law. There is typically an economic interest to restrict access to proprietary data. 15 Input into an AI system in the form of rules can be considered a form of proprietary data. Public data: Data not protected by intellectual property rights or any other rights with similar effects and that in many cases can be shared for access and re-used through open data regimes. Personal data: Data that relates to an identified or identifiable individual . Why does this matter? Proprietary data raise issues such as transparency and explainability (Principle 1.3); bias in AI systems (Principle 1.2); as well as",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      39
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   39"
    ],
    "chunk_index": 33,
    "text": "considerations of business scale-up (Principle 2.2). Public data is relevant to economic, social and environmental impacts (Principle 1.1); research (Principle 2.1); and data availability and compute capacity (Principle 2.2). Personal data is associated with privacy considerations and legislation and usually requires more restrictive access regimes. Personal data is relevant to issues related to human rights, fairness and privacy (Principle 1.2). Figure 6. Personal, private and public domains of data Source: (OECD, 2019[15]). Identifiability of personal data Personal data taxonomies differentiate between different categories of personal data. ISO/IEC 19441 (2017) distinguishes five categories, or states , of data identifiability: Identified data: Data that can be unambiguously associated with a specific person because they contain personal identifiable information. Pseudonymised data: Data for which all personal identifiers are substituted by aliases. The alias assignment is such that it cannot be reversed by reasonable efforts, except for the party that performed the assignment. Unlinked pseudonymised data: Data for which all personal identifiers are irreversibly erased or substituted by aliases. The linkage cannot be re-established by reasonable efforts, including by the party that performed the assignment. OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 39 OECD 2022 Anonymised data: Data that are not linked to attributes that can be altered (i.e. attributes values are randomised or generalised) in such a way that there is a reasonable level of confidence that a person cannot be identified, directly or indirectly, by the data alone or in combination with other data. Aggregated data: Statistical data that do not contain individual-level entries and are combined with information about enough different persons that individual-level attributes are not identifiable. Why does this matter? The type of personal data used by AI systems has implications for individuals human rights, fairness and privacy (Principle 1.2). Data identifiability can help assess the level of",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 34,
    "text": "risk to privacy and inform the need for legal and technical protection and access control. Concerns are raised that even absent personal data, AI systems are able to infer data and correlations from proxy variables that are not personally identified, such as purchasing history or location. In addition, some regimes such as the EU s GDPR, distinguish sensitive personal data that consist of racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, biometric data, health data or data concerning a person's sex life or sexual orientation. In the United States, personal data considered sensitive include data about children and financial and health information.16 Data quality and appropriateness {where objective and consistent information is available} Data appropriateness (or qualification ) is about defining criteria to ensure that the data are appropriate for use in a project, fit for purpose, and relevant to the system or process following standard practice in the industry sector. For example, in clinical trials to evaluate drug efficiency, criteria for using patient data must include patients clinical history (previous treatments, surgery, etc.). Data quality also plays a key role for AI systems, as do the standards and procedures to manage data quality and appropriateness. An AI application or system applies standard criteria or industry-defined criteria in order to assess: Data appropriateness: Data are appropriate for the purpose for which they are to be used, following standard practice in the industry sector. Sample representativeness: Selected variables and training or evaluation data accurately depict/reflect the population in the AI system environment. Adequate sample size: Sample size displays an appropriate level of granularity, coverage and sufficiency of data. Completeness and coherence of sample: Sample is complete, with minimal missing or partial values. Outliers must not affect the quality of data. Low data noise :",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      40
    ],
    "titles": [
      "40     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 35,
    "text": "Data is infrequently incorrect, corrupted or distorted (e.g. intentional or unintentional mistakes in survey data, data from defective sensors). Why does this matter? Data appropriateness impacts the accuracy and reliability of the outcome of AI systems and relates to their robustness, security and safety (Principle 1.4.) The use of inappropriate data/input in an AI system can lead to erroneous and possibly dangerous conclusions.17 Data quality has important policy implications for human rights and fairness (Principle 1.2), as well as to the robustness and safety of an AI system (Principle 1.4): from both fairness and robustness perspectives, datasets must be inclusive, diverse and representative so they do not misrepresent specific (sub) groups. 40 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Structure and format of data and input Structure of data and input This sub-dimension identifies common types of data structures: Unstructured data: Include data that either do not have a pre-defined data model or are not organised and labelled in a pre-defined manner (e.g. text, image, audio, video and other data types such as sensor data and interlinkages between graph networks, social media or website data). Unstructured data often include irregularities and ambiguities that are difficult for traditional programmes to analyse. Unstructured data are sometimes referred to as raw data. Semi-structured data: In practice, most data combine both unstructured and structured data. For example, a photo taken with a smartphone consists of the image itself (unstructured data), accompanied by structured metadata about the image (when and where it was taken, what device took the picture, the picture format, its resolution, etc.). Similarly, data on a social network such as Twitter include unstructured text alongside structured metadata about the author of the text and his or her networks. In addition to social media, examples of semi-structured data include device",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 36,
    "text": "or sensor data. Structured data: Data that are stored in a pre-defined format and are straightforward to analyse. Structured data have labels describing their attributes and relationships with other data. Vast amounts of user data from websites or e-commerce sites are structured, fuelling the development of a wide variety of marketing techniques (e.g. personalised advertisements), recommendation mechanisms (e.g. Amazon products, Netflix content, Spotify recommendations, YouTube s up next videos) and engagement systems (e.g. Facebook feeds). Examples of structured data include interlinked tables and databases. Complex structured data: Data often produced in the form of a model, which is both the output of an AI algorithm and can be used as input to another system. Examples of complex structured data include ontologies (e.g. partial models of the environment), knowledge graphs, rules (e.g. expert systems) and analytical functions (e.g. adversarial learning or reinforcement learning functions). Each of these structural alternatives can be encoded in various data formats (e.g. binary, numeric, text) and represent different forms of media (e.g. audio, image, video or their combinations). Data labelling is the process of tagging data samples, which generally require human knowledge to build training data. Why does this matter? Data and input structure relates to transparency and auditability (Principle 1.3) and directly impacts the AI model choice. Structured data are easier to document and audit (Principle 1.5) and also influences data-sharing policies (Principle 2.2) Format of data and metadata {where objective and consistent information is available} Data format (or encoding) refers to the format of the data themselves. Data format is closely related to data collection (e.g. a camera will produce a specific format of image data) and to data modelling, where different modelling techniques will require specific data formats (e.g. time-series modelling requires temporally sequenced data). Dataset metadata may include information on how a",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      41
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   41"
    ],
    "chunk_index": 37,
    "text": "dataset was created, its composition, its intended uses and how it has been maintained over time. Formats and standards for annotating datasets often need to be developed while taking into account industry sector and use case: Standardised data format: Standardised data have a format pre-agreed to by the providers of the data, which allows for easier comparability, i.e. for a dataset to be compared to other datasets. Non-standardised data format: Data can also be in ad hoc formats created for the purpose of particular applications (e.g. video). Standardised dataset metadata. OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 41 OECD 2022 Non-standardised dataset metadata. Why does this matter? Standardisation of data formats facilitates interoperability and data re-use across applications and for accessibility, and can help ensure that data are findable, catalogued, searchable and re-usable. The use of standardised formats may improve an AI system s robustness and security by making it easier to address security vulnerabilities (Principle 1.4). In some sectors, standardised templates for dataset metadata annotation are being developed. Standardised metadata facilitates the development and sharing of training datasets and, by extension, can help accelerate the development and use of an AI system (Principle 2.2). Key AI actors in the Data & Input dimension include data collectors and data processors The Data & Input dimension maps directly to the data collection and processing stage of the AI system lifecycle (Figure 3), where it involves gathering and cleaning data, labelling, performing checks for completeness and quality, and documenting the characteristics of the dataset. Dataset characteristics include information on how a dataset was created, composition, intended uses and how it was maintained over time (OECD, 2019[15]). Key actors also include data rights holders who are impacted by whether these rights are intellectual property (e.g. copyright) or personal data rights such as",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      42
    ],
    "titles": [
      "42     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 38,
    "text": "those recognised by the GDPR in the EU. Data collection and processing currently involves expertise from actors such as data scientists, domain experts, data engineers and data providers. Actions performed by data collectors and processors include: Performing checks: For data quality and appropriateness. Transparent information about the data and inputs used in the AI system: Providing interested stakeholders with access to meaningful information on the data and inputs used in the AI system. Labelling data: Such as tagging data with informative data. Protecting personal data. Documenting data and dataset characteristics. Using tools or processes for trustworthy AI: Such as guidelines, governance frameworks, product development/lifecycle tools, risk management, sector-specific codes of conduct, process standards, technical validation approaches, technical documentation, technical standards, toolkits/toolboxes/software tools, educational material, change-management processes, and certification (technical and/or process-related). 42 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 AI Model AI model-building is part of a system s development in the lab (especially for non-learning AI) whereas model inferencing, the process of using the model, takes place in production in the field. To accurately classify AI systems, it is helpful to identify the core AI model, or models, around which the system is built as they determine a wide array of characteristics. The AI Model dimension considers AI models as composites of multiple, core, technical components, and analyses the choice of AI models, how the models are built, how the models are interlinked with one another and other sub-systems, and how they are used, also known as model inferencing . This section discusses how the traits of an AI model relate to policy considerations. What is an AI model? AI models are actionable representations of all or part of the external context or environment of an AI system (encompassing, for example, processes, objects, ideas, people and/or",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 39,
    "text": "interactions taking place in context). AI models use data and/or expert knowledge provided by humans and/or automated tools to represent, describe and interact with real or virtual environments (Box 2). Box 2. OECD characterisation of an AI model, included in the OECD AI Principles (2019) To examine and classify different types of AI systems and different scenarios, it is helpful to identify the following elements of an AI model (Figure 7): - The model itself, an object that forms the core of an AI system and represents all or part of the system s external environment. - The model-building process, often called training or optimisation, that is part of the system development in the lab. - The process of using the model, in which model-inferencing algorithms generate outputs for information or action from the model, given specific objectives and performance measures. Model-inferencing tends to take place in production in the field. Figure 7. Detailed conceptual view of an AI model Source: (OECD, 2019f[2]) The purpose of an AI model within a deployed system: The model choice and model-building approach depend on the purpose of the AI system, i.e. the problem that the AI system is trying to solve. AI models can be used to make recommendations in response to an input, such as answering a question, what is the best next move in a chess game? , generating data in response to a prompt, what would a person look like if they were 20 years older? , making predictions about the future courses of action, \"will this road become congested?\" and a wide range of other processes (see Task of the system sub-section in the Task & Output section). AI models are not universal: It is important to highlight that AI models include assumptions and biases about the world around",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      43
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   43"
    ],
    "chunk_index": 40,
    "text": "them (OECD, 2019f[2]). Many different representations of the same environment can be developed to serve different purposes; there is no universal unique or correct model to represent a given reality. OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 43 OECD 2022 One model or many models? Most applied AI systems use composite models: In practice, the vast majority of AI systems in deployed, real-world contexts are composite systems. They are composed of a variety of interlinked AI sub-models derived from different sources, working together for a specific purpose. For instance, Google Photos a popular application for storing and searching photos consists of several systems and subsystems, including those for storing data (photo storage), searching data via words that the user specifies, and interpreting the search terms which leverages additional systems, some of which are also AI models. When a user searches for photos by date, a human-designed system looks up a photo s upload date or the date encoded in the photo metadata. When a user searches for photos by a concept such as \"flowers\", an AI model carries out a recognition function to match photos with the query. Why does this matter? Key properties of AI models, such as the degree of transparency and/or explainability (Principle 1.3); the level of robustness (Principle 1.4); and human rights, privacy and fairness implications (Principle 1.2), depend on the type of model as well as the model-building and inferencing processes (see questions in Box 3). It is important to note that AI models can be built to achieve a specific set of objectives but then used with different objectives, as in the case of transfer learning, for example (Principle 1.4). Some leading experts stress the importance of very carefully specifying AI objectives to be the fulfilment of human goals rather than intelligence and",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 41,
    "text": "efficiency (Russell, 2019[17]). Box 3. Transparency and explainability (Principle 1.3) and safety, security and robustness (Principle 1.4) throughout an AI system s lifecycle Risk assessment and management throughout the AI system lifecycle is the topic of a separate, follow- on facet of research (see section on Refining classification criteria ). Possible questions to help determine AI system transparency and explainability (Principle 1.3) might include: Is it clear what the objectives of the AI system are, i.e. is it possible to formalise the problem that the system is being asked to solve? Does the AI system provide useful and meaningful information for understanding its performance and outputs/decisions? Can all of the AI system s outputs both intermediary and final for achieving a given goal be explained? Can the determinant data or knowledge that an AI system uses to make decisions be identified? Do two similar-looking cases verifiably result in similar outcomes, i.e. can the consistency and integrity of AI system outcomes be verified?18 Possible questions for policy makers to help determine the safety, security and robustness of AI systems might include: Do safety metrics exist that can evaluate the safety of an AI system for a given use case? How does the entity deploying the AI system test for safety during development? What measures has the entity deploying the AI system taken to do an adversarial evaluation that is, explore the AI system through the lens of being a bad actor and trying to break it? Does the AI system change significantly if it is trained with variations of the data available? Are there measures in place to validate and verify the AI system s outcomes? What measures are in place to facilitate traceability in the AI system, including in relation to datasets, processes and decisions made during the AI system",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      44
    ],
    "titles": [
      "44     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 42,
    "text": "lifecycle? 44 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 AI model characteristics Information availability The amount of information, organised into the following basic categories available about the model(s) used in an AI system, can provide a first indication of its degree of transparency: Detailed information about the model(s) used in the system is available. Some information about the model(s) used in the system is available. No information about the model(s) used in the system is available. Licensing rights associated with the model {where objective and consistent information is available} A model can be available under open-source or proprietary licensing regimes. Open-source software (OSS) is software for which source code is public and can be freely copied, shared and modified (OECD, 2019c[18]). Models based on proprietary software aim to protect an organisation s source code partially or fully, including through intellectual property regimes. The following are common types of licensing rights: Self-managed OSS Third-party managed OSS Self-managed proprietary Third-party managed proprietary AI model types19 Symbolic AI models: Symbolic or knowledge-based AI uses human-generated logical representations to infer a conclusion from a set of constraints (variables). These constraints include rules, ontologies and search algorithms and rely on explicit descriptions of variables agents like humans, entities like factories, objects like machines, variables that can be stock conditions and descriptions of the inter- relations between these variables. Symbolic models are expressed in languages such as mathematical logic (if/then statements or more abstract ways of representing knowledge via mathematical formulae), agent-based models, event-driven models, etc. (see example in Box 4). Symbolic AI is still in widespread use for optimisation and planning tools. Statistical AI models: Statistical AI models (e.g. genetic algorithms, neural networks and deep learning) identify patterns based on data rather than expert, human knowledge. They have seen increasing uptake recently.",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      45
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   45"
    ],
    "chunk_index": 43,
    "text": "Statistical AI models were previously used primarily for recognition purposes (for instance, translating writing on cheques into machine-readable code). More recently, they are also being used for tasks like generation, such as synthesising and generating images or audio. Models that rely on data are designed to effectively extract and represent knowledge from data rather than to contain explicit knowledge knowledge that is sharable and easily comprehensible. Hybrid AI models: Many applied AI systems combine symbolic and statistical models into hybrid models. For example, NLP algorithms often combine statistical approaches building on large amounts of data and symbolic approaches that consider issues such as grammar rules. OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 45 OECD 2022 Box 4. Car engine production systems: Example of a symbolic AI model One real-world example of a symbolic AI model is a car engine production system that involves different factories. In this example, each factory may have different machines assembling parts, operated by teams with different skills who are working on specific shifts. Parts can be sent from one factory to another using different logistics systems. The specificities and processing rules of this heterogeneous system (which comprises humans, factories, machines, stock, finances, etc.) are codified based on expert knowledge that describes each specific part of the system and its interactions with the rest. The AI model in this case is built, validated and calibrated based on this knowledge and can be used to simulate possible demand in order to optimise production mechanisms in response to demand volatility. Why does this matter? An AI model s degree of explainability is determined primarily by the design of the AI model and is linked to the complexity of the system. The more complex the model, the harder it is to explain. Explainability means enabling people affected by",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 44,
    "text": "the outcome of an AI system to understand how the outcome was arrived at. Sub-models of rules-based, symbolic models are often easily understood, making it comparatively straightforward to find certain types of errors. By contrast, some machine-learning systems, notably, neural networks, rely on abstract mathematical relationships between factors that can be challenging or even impossible for humans to understand. Hybrid AI systems that combine models built on both data and human expertise are viewed as a promising alternative that addresses the limitations of both system and machine-learning approaches. They can provide visibility on complex situations or environments with many interactions, help to predict what may happen in the future, and foster inclusive and sustainable growth and well-being (Principle 1.1). Discriminative or generative models {where objective and consistent information is available} Discriminative model: Focuses on predicting data labels by learning to distinguish between dataset classes. They are more robust and capable of addressing outlier issues but cannot generate new data and can misclassify data points. Examples of discriminative models include regression analyses, support-vector machines (SVM), traditional neural networks, decision trees and random forests. Generative model: Involves discovering and learning the patterns and distribution of the input data, enabling the generation of new plausible examples that could be part of the original distribution. Examples of generative models include na ve Bayes models, hidden Markov models, linear discriminant analysis (LDA) and generative adversarial networks (GANs). Models combining both discriminative and generative properties: Designed to achieve a given goal or generate an output. Why does this matter? For policy purposes, whether a model is discriminative or generative determines the type of output that it generates: Outputs from discriminative models are predictions whereas outputs from generative models are artefacts {where objective and consistent information is available}. Model ensembles: In some cases, the system is underpinned",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      46
    ],
    "titles": [
      "46     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 45,
    "text": "by an AI model interacting independently with other AI models. Model ensembles are collections of models that act together in parallel to cooperate on a single task or decision, which increases complexity but often improves accuracy. 46 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 System is composed of a single AI model. Why does this matter? A system involving a high degree of distinct, interacting systems may be more complex than a system composed of a single model, which often increases the probability of failures (Principle 1.4). It should be noted that systems are more and more often multi-tasking systems.20 It is important to note that as models are combined, accuracy often improves but errors can propagate and multiply more easily, especially if uncertainty characterisations are not properly taken into account by downstream models. Model-building Model-building from machine-learned or human-encoded knowledge in the lab The model-building process is often called training or optimisation . Objectives (e.g. output variables) and performance measures (e.g. accuracy, resources for training and representativeness of the dataset) guide the model-building process. AI systems using machine learning for model-building have seen tremendous uptake over the past few years. Machine learning, as explained earlier in this report, is a set of techniques that allows machines to learn in an automated manner through patterns and inferences rather than through explicit instructions from a human. Machine-learning approaches often teach machines to reach an outcome by showing them many examples of correct outcomes. However, they can also define a set of rules and let the machine learn by trial and error. Machine learning contains numerous techniques that have been used for decades and range from linear and logistic regressions, decision trees and principle component analysis to deep neural networks. A machine-learning AI model can be built from data",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      47
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   47"
    ],
    "chunk_index": 46,
    "text": "that is labelled or unlabelled, resulting in different machine-learning paradigms; whether data is labelled or not may already be inferred by the section on Format of data and metadata. When analysing AI systems, they can be roughly grouped into how much emphasis they place on human-encoded knowledge acquisition versus machine-learned knowledge acquisition: Acquisition from human-encoded knowledge (for example, writing rules): Human-written rules that capture relationships between elements of the environment by logical rules enable an AI model to deduce a conclusion from a set of constraints and data. They require that researchers build detailed and human-understandable decision structures to translate real-world complexity and help machines make decisions. Acquisition from data through supervised learning: AI models identify a relationship between input dimensions and labelled target dimensions. Acquisition from data through unsupervised learning: AI models identify a relationship between input data points based on their similarity. Acquisition from data through semi-supervised learning: AI models use both labelled and unlabelled data to identify a relationship between input dimensions and labelled target dimensions. Acquisition from data through reinforcement-learning: Does not require labelled input or data, nor suboptimal output to be corrected. Reinforcement learning leverages both systems ability to explore current knowledge. Acquisition from data, augmented by human-encoded knowledge: Hybrid AI model systems combining human-encoded knowledge with knowledge acquired from data are common. For example, self-driving cars are frequently built using complex human-encoded rule sets that encode laws about how to drive acceptable turns, speed limits, aspects related to braking speeds and tolerances, and so on. These rule sets are then combined with vision systems typically based on neural networks, which have acquired their capabilities via supervised learning on datasets annotated by the self-driving car companies. OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 47 OECD 2022 Why does this matter? AI systems are only",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 47,
    "text": "as good as the data they are trained on or the expert input they are built on. Machine-learning systems can make predictions about data similar to that on which they were trained, from which they derive associations and patterns that can fail in settings that are meaningfully different from those encountered in training. Data-labelling, as explained earlier in this report, is the process of tagging data samples, which generally requires human knowledge to build training data. Data-labelling is critical and can itself require some explainability in contexts such as content moderation, where assigning a label such as \"misinformation\" or violent is important. In supervised learning, the label itself represents extra knowledge that is most often provided by a human in the loop , while in unsupervised learning, a human did not label such content. Expert systems have their own limitations as they require humans to build detailed decision structures to translate real-world complexity and help machines produce outputs. Model evolution in the field (applicable only to machine learning systems) Some machine-learning models can continue to evolve, acquiring abilities from interacting directly with data during the model-building process: No evolution during operation (no interaction): Dataset is static and does not change over time. An AI model is given a dataset and learns patterns or associations from it. Evolution during operation through active interaction (including uncontrolled learning): AI model actively interacts with the environment and receives data based on these interactions. An example of such a setting is a robot arm, which learns to perform a task, such as picking up a cup, by repeatedly attempting to perform the task and receiving feedback on which movements were successful and which movements were not. Evolution during operation through passive interaction: AI model receives a continuous stream of data (for example, stock prices), which",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      48
    ],
    "titles": [
      "48     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 48,
    "text": "the system is unable to affect but to which it needs to adapt. Why does this matter? The degree to which a model evolves in response to data and input from its environment in the field is particularly relevant to public policy for AI systems that can iterate and evolve over time and may change their behaviour in unforeseen ways. Model evolution and model drift (where a model degrades because of changes in data, input or output) are directly relevant to safety, security and robustness (Principle 1.4) as well as accountability and liability (Principle 1.5). AI models using static data are comparatively more stable. There may be a trade-off between the adaptive nature of an AI system (i.e. whether the model evolves in the field based on input from its environment) and the quality of its outcomes. This trade-off may be more acute with real-time data, as more conflicting data may arrive faster, creating a further risk of compromising the quality of the outcomes (Principle 1.4). 48 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Central or federated learning (applicable only to machine-learning systems) Models of machine-learning systems can be trained centrally or via multiple local servers or edge devices such as smartphones: Centralised learning: Uploads all datasets to a central processing environment to train an algorithm. All datasets are considered local to the training environment. Most current machine learning is centralised. Federated (collaborative) learning: Trains an algorithm across multiple processing environments, which can include edge devices or different data centres. Data samples are kept locally within each environment and are not copied across environments. There is no centralised, complete dataset with which the algorithm can train.21 Why does this matter? Federated learning helps to address critical issues like privacy (Principle 1.2), data security and data access rights",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 49,
    "text": "(Principle 1.4) by building models without sharing data. It also distributes the computing requirements to train an AI system, although this may actually increase latency (processing delays). Model development and maintenance {where objective and consistent information is available} There are multiple approaches to AI model development and maintenance (BSA, 2021[19]), including: Universal model: Developer provides multiple AI actors or stakeholders (e.g. deployers, operators, users) with access to a single pre-trained model. Customisable model: Developer provides a model that can be customised and/or re-trained by other AI actors, by, for example, using different data. Tailored model: Developer trains a model on behalf of an AI actor or stakeholder using the AI actor or stakeholder s data. Why does this matter? Understanding how an AI system s model was developed and/or maintained is a key consideration for assigning roles and responsibilities throughout a risk-management process. It is also relevant to assessing the system s robustness, security and safety (Principle 1.4) as well as accountability (Principle 1.5). For instance, the developer that trained and maintained the universal model on behalf of others would generally be best positioned to address most aspects of model risk management throughout the system s lifecycle. For customisable models, many key risk-management responsibilities would likely shift to the organisation that re-trained and/or customised the model. The bulk of risk-management responsibilities for tailored models fall on the entity that developed the model. Model inference, or using a model An AI model can be used in many different ways, and inference is the process of using an AI model trained from data or manually encoded to derive a prediction, recommendation or other outcome based on new data that the model was not trained on (see Box 2, Figure 7). Different inference strategies can be used to derive varying results from the",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      49
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   49"
    ],
    "chunk_index": 50,
    "text": "same model. These strategies are usually designed to optimise specific objectives and performance measures like robustness, accuracy, speed, business metrics or other criteria. Deterministic inference is when the model s outcomes can be fully determined by the parameter values and random variation is not possible. Inference strategies include reasoning techniques used in expert systems. If random variation is present in the context in which the model operates, then probabilistic OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 49 OECD 2022 inference may be more appropriate. There are a variety of methods for comparing and selecting different inference strategies. Deterministic and probabilistic models {where objective and consistent information is available} Deterministic models: Follow precise rules ( if this, then that ) and generate a single outcome. Probabilistic models: Infer several possible models to explain data and deciding which model to use is uncertain. Outcomes made using these different models are also uncertain. Probabilistic models quantify these uncertainties.22 The different outcomes are associated with different levels of, for instance, performance measures like level of confidence, robustness or risk that can be optimised with different inferencing techniques.23 AI systems can combine both deterministic and probabilistic models. Why does this matter? For policy purposes, whether a model is probabilistic is relevant to testing and testability (Principle 1.4) as well as to explainability (Principle 1.3). Probabilistic models can generate multiple outcomes with information about their uncertainty. Given the randomness element in probabilistic models, a specific outcome may not easily be reproducible (Principle 1.3 and 1.4). Model transparency and explainability Different AI models can exhibit different degrees of transparency and explainability. Among other things, this entails determining whether meaningful and easy-to-understand information is made available to: Make stakeholders aware of their interactions with AI systems, including in the workplace Enable those affected by an AI system",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      50
    ],
    "titles": [
      "50     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 51,
    "text": "to understand and challenge the outcome and how it was produced by the AI model (e.g. by setting the weights of the AI model s components) Why does this matter? An AI system s transparency and explainability (Principle 1.3) relates to how the model is used and how easily and thoroughly the structure and outputs of the system i.e. understanding the link between input and output can be understood, and by whom. Key actors in the AI Model dimension include developers and modellers Model-building and interpretation involve the creation or selection of models/algorithms, their calibration and/or training and inferencing (use). It also involves verification and validation, whereby models are executed and tuned (maximising performance) with tests to assess performance across various dimensions and considerations. Model-building and inferencing involve human experts such as modellers, model engineers, data scientists, developers and domain experts. Currently, model verification and validation involves data scientists, data/model/systems engineers and governance experts. Actions performed by developers and modellers include: Model-building by selecting and training a model Verification and validation to execute and tune models, including metrics to authorise the system for broader deployment When it comes to testing to assess performance across various dimensions and considerations, characteristics of the team of AI system developers such as gender, country, cultural background have been shown to impact the way AI systems are built, as developers can incorporate unconscious biases (Freire, 2021[20]). This may result in advocacy for diversity in teams that create AI systems. Actors in the AI Model dimension may also include auditors. 50 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Task & Output The Task & Output dimension describes what the AI system does the task it performs and the action that derives from it. The model produces recommendations, predictions or other outcomes for a",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 52,
    "text": "given set of objectives, and a human or a machine (an actuator ) acts upon those recommendations, predictions or outcomes to influence the environment in which the AI system operates, at varying levels of autonomy. Task(s) of the system The task of an AI system drives the choice of AI model and refers to what the system does, i.e. the function that it performs.24 The following seven categories cover most tasks performed by AI systems (Table 5): Recognition: Identifying and categorising data (e.g. image, video, audio and text) into specific classifications as well as image segmentation and object detection. Event detection: Connecting data points to detect patterns, as well as outliers or anomalies. Forecasting: Using past and existing behaviours to predict future outcomes. Personalisation: Developing a profile of an individual and learning and adapting its output to that individual over time. Interaction support: Interpreting and creating content to power conversational and other interactions between machines and humans (possibly involving multiple media such as voice, text and images). Goal-driven optimisation: Finding the optimal solution to a problem for a cost function or predefined goal. Reasoning with knowledge structures: Inferring new outcomes that are possible even if they are not present in existing data, through modelling and simulation. The above categories are subject to change as AI technologies evolve and should be regularly reviewed and broadened to include new types of applications. Table 5. AI system tasks What it does Type of learning / reasoning Examples Recognition Identifies and categorises data (e.g. image, video, audio and text) into specific classifications. Output is often one label, e.g. this is a cat . Supervised classification. Image & object detection; facial recognition; audio, sound, handwriting and text recognition; gesture detection. Event detection Connects data points to detect patterns as well as outliers or anomalies. Uses",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      51
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   51"
    ],
    "chunk_index": 53,
    "text": "non-machine learning cognitive approaches as well as machine learning. Event detection increasingly uses unsupervised and reinforcement learning techniques in which the AI system does not know what it is looking for. Fraud and risk detection, flagging human mistakes, intelligent monitoring. Forecasting Uses past and existing behaviours to predict future outcomes, generally to help make decisions. Contains a clear temporal dimension. Tends to use machine-learning techniques such as supervised learning and is adaptive and helps improve forecasting over time. Some knowledge-based symbolic systems also perform forecasting, mostly based on uncertainty representation. Forecasting is generally used for decision support. It may include descriptive analytics, predictive analytics and projective analytics. Assisted search, predicting future values for data, predicting failure, predicting population behaviour, identifying and selecting best fit, identifying matches in data, optimising activities, intelligent navigation. Personalisation Develops a profile of an individual and then learns and adapts to that individual over time. The output is usually a ranking, e.g. a search engine ranking. Most personalisation algorithms are based on supervised or reinforcement learning models. Recommender systems based on search and browsing (Netflix, Amazon), personalised fitness, wellness, finance. OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 51 OECD 2022 Interaction support Interprets and creates content to power conversational and other interactions between machines and humans (e.g. involving voice, text, images). Can be real- time or not. Interaction tends to use semi-supervised or reinforcement learning, enabling models to evolve. Chatbots, voice assistants, sentiments model and intent analysis. Goal-driven optimisation Gives systems a goal and the ability to find the optimal solution to a problem, which can be by learning through trial and error. It assumes a cost function is given. Goal-driven optimisation is not necessarily a number. It can be called prescription when based on an optimisation. Game playing, resource/logistics optimisation, iterative problem-solving, bidding and",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 54,
    "text": "advertising, real-time auctions, scenario simulation. Reasoning with knowledge structures Infers new outcomes that are possible, even if they are not present in existing data, through modelling and simulation. This task involves causal reasoning rather than correlation and uses AI techniques beyond machine learning. Expert systems, legal argumentation, recruitment systems, diagnosis, planning. Other Please describe Source: OECD, 2022. Why does this matter? A few policy considerations associated with the tasks performed by AI systems include: Recognition systems require data that is representative and unbiased to function appropriately. Recognition of people and biometrics, such as facial recognition or voice recognition systems, can raise concerns in relation to human rights (Principle 1.2) and robustness and security in case of adversarial attacks (Principle 1.4). Event detection can benefit people and planet (Principle 1.1), safety and security (Principle 1.4), yet in some contexts raises human rights concerns (Principle 1.2) when used to monitor individuals activity. In forecasting, depending on the application, keeping a human in the loop may be important for accountability (Principle 1.5). Personalisation can impact social structures and well-being positively (Principle 1.1 benefits), but can also conflict with human values and individuals right to self-determination (Principle 1.2.) as it tends to provide people with content that they have liked before or that similar people have liked; contributing to disinformation and echo-chamber effects. Interaction support tasks in which an AI system interacts with people may implicate data usage and data privacy (Principle 1.2) and may require higher transparency and disclosure of the fact that one is interacting with a chatbot (Principle 1.3). It may also impact labour markets (Principle 2.4). Goal-driven optimisation and other similar tasks using machine-learning algorithms that can learn from themselves through trial and error and may require humans in or on the loop (Principle 1.5). Limits on the power of",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      52
    ],
    "titles": [
      "52     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 55,
    "text": "this type of system may be needed if exponential growth occurs (e.g. artificial general intelligence). In addition, when goal specification is imperfectly defined, these systems may drift away from intended behaviour, thus potentially raising issues for robustness (Principle 1.5.). Reasoning with knowledge structures is promising to help inclusive and sustainable growth and well- being (Principle 1.1), by allowing for the simulation of different scenarios considering causal and counterfactual relationships and situations that change with time, such as improving legacy power generation systems.25 52 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Combining tasks and actions into multi-task, composite systems {where objective and consistent information is available} AI systems frequently perform several tasks, such as event detection, forecasting and personalisation (see Table 5 for the full list), before producing an outcome that influences the environment. A composite AI is essentially an interlinked network of agents. Several composite systems that combine different tasks are common and well-known. They may generate specific policy considerations that differ from those produced by single-task systems. However, it may be difficult to anticipate and test composite AIs, which, like any complex system, will have unpredictable boundaries and impacts and will be hard or impossible to fit into a formula. For example, a hybrid AI can be planned and tested by the same developer, but the network of AIs cannot. The following are examples of composite AI systems or application areas that combine several tasks and, in some cases, actions. The list is not exhaustive. When selecting the right system, users may add other relevant types of composite systems, as appropriate. Content generation (also referred to as synthesis): Includes generating new images, video, text, assessment and audio. This task combines forecasting and recognition tasks. However, the output often combines several existing elements such as images, text",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 56,
    "text": "and audio to produce an object that was never seen before. This task tends to use structured learning. Examples of content generation include machine translation, generative art, news stories including fake news, spam emails and deep fake videos. Autonomous systems: Robots and robotic systems increasingly embed different tasks such as recognition and goal-driven optimisation to perform an action in the real world. Similarly, autonomous systems perform a recognition task, which they use to try to find an optimal path to arrive at the best solution, and then act accordingly. In an autonomous vehicle, this could be a recommendation to turn left or right to minimise travel time, followed by the execution of the action. This system is autonomous in the sense that it does not require human supervision to act on its environment, but the way in which it performs its task (recognition) is not autonomous, as it relies on supervised learning. Monitoring and control systems: These systems manage, command, direct or regulate the behaviour of other devices or systems using control loops. Control systems generally assess environments through recognition, event detection or forecasting and propose a goal-driven action. They range from domestic heating controllers to large industrial control systems that use reinforcement learning to manage processes or machines. Control systems are common in the context of robotics or factories. One example of a control system is a fraud detection system with an event detection task combined with an action (e.g. freezing a bank account). IoAI , combining AI and Internet of Things (IoT): AI is allowing insights to be extracted from data. Internet of Things (IoT) refers to the connection of an increasing number of devices and objects over time to the Internet. Following the convergence of fixed and mobile networks, and between telecommunications and broadcasting, the IoT represents",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      53
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   53"
    ],
    "chunk_index": 57,
    "text": "the next step in the convergence between Information and Communications Technologies (ICTs) and economies and societies. IoT is becoming common in daily life, with many billions of interconnected smart devices, equipment, machines and infrastructure creating opportunities for automation and for interaction in real time. Why does this matter? From a policy perspective, content generation has relevant implications for human rights and democratic values (Principle 1.2), particularly when the content generated is realistic enough to be confused with real content. AI content generation amplifies the need to provide meaningful information to make stakeholders aware of their interactions with AI systems so that those affected understand and challenge the outcome (Principle 1.3; Principle 1.5). AI content generation also raises intellectual property rights questions (e.g. on the patentability of AI-assisted inventions and copyrighting of AI-generated creative work). OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 53 OECD 2022 Autonomous systems and control systems have received increased attention and have direct implications for human safety (Principle 1.4) and accountability (Principle 1.5) as well as transparency and explainability. Action autonomy level A human or machine actuator (see Box 1) uses the outcome from the AI system (more specifically, the outcome from the inferencing process) to perform an action prescribed by humans that influences the environment in which the system operates. The way in which this action is performed determines the autonomy level of an AI system; that is, the degree to which a system can act without human involvement. Below are four common variations in the degree of AI system autonomy, using a typology that originated in the field of aviation (Endsley, 1987[21]): No-action autonomy (also referred to as human support ): System cannot act on its recommendations or output. The human uses or disregards the AI system s recommendations or output at will. Low-action",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 58,
    "text": "autonomy (also referred to as human-in-the-loop ): System evaluates input and acts upon its recommendations or output if the human agrees. Medium-action autonomy (also referred to as human-on-the-loop ): System evaluates input and acts upon its recommendations or output unless the human vetoes. High-action autonomy (also referred to as human-out-of-the-loop ): System evaluates input and acts upon its recommendations or output without human involvement. It should be noted that for certain AI models with no, low or medium autonomy such as active learning algorithms the user or operator of the AI system may be contributing to its training and/or to validating its outputs. Why does this matter? High-action autonomy systems pose important policy considerations, in particular when deployed in critical functions and activities or in contexts that may put human rights or fundamental values (Principle 1.2) at risk. AI systems in which the user or operator may be contributing to its training and/or to the validation of its outputs raise transparency (Principles 1.3) and accountability (Principles 1.5) considerations. Core application areas Below are four core application areas but the list is not exhaustive. When selecting an AI system, users may consider other relevant applications areas, as appropriate. Human language technologies: Analyse, modify, produce or respond to human text and speech. Human language technologies may combine tasks like recognition, personalisation and interaction support. Computer vision: Concerned with training computers to interpret and understand the visual world. Feeding digital images and videos into deep learning models, machines can identify and classify objects and react to what they see. Computer vision may include tasks like object recognition and event detection. Robotics: A system that contributes to the movement of robots. This involves the mechanical aspects and programme systems that make it possible to control robots, such as recognition and goal-driven optimisation to perform",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      54,
      55
    ],
    "titles": [
      "54     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS",
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   55"
    ],
    "chunk_index": 59,
    "text": "an action in the real world. Autonomous vehicles are supported by robotic systems. Automation and/or optimisation: Process automation and/or simulation using structured data such as data mining, pattern recognition, a recommendation system or forecasting/prediction. Numerical 54 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 optimisation maximises or minimises a real function to optimise a business process such as scheduling, process controlling or operational research. Evaluation methods {where objective and consistent information is available} In some cases, there are agreed standards or general methods to assess an AI system or application within a given industry context and for a type of task(s): There are industry standards for evaluating AI systems as applied to this specific task and context. There are other methods for evaluating this AI system as applied to this specific task and context. There are no methods or industry standards available for evaluating this system. Why does this matter? The availability of agreed standards or general methods to assess an AI system or application within a given industry context and for a type of task(s) relates to accountability (Principle 1.5) and robustness and safety concerns (Principle 1.4). The ability of operators and in some cases users to evaluate output can help identify incidents or instances of malfunctioning and assess the degree of reliability of the system, with a view to regulate or improve it. Key actors in the Task & Output dimension include system integrators The Task & Output dimension is often associated with the deployment stage of the AI system lifecycle (OECD, 2019f[2]). Deployment into live production involves piloting, checking compatibility with legacy systems, ensuring regulatory compliance, managing organisational change and evaluating user experience. Deployment currently involves experts such as system integrators, developers, systems/software engineers, testers and domain experts. OECD FRAMEWORK FOR THE CLASSIFICATION OF AI",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 60,
    "text": "SYSTEMS 55 OECD 2022 Applying the framework to real-world systems with expert and survey input The OECD AI Framework is designed to classify the application of AI systems in specific, real-world contexts. The OECD.AI Network Experts has classified four systems systems 1, 2, 8 and 9 out of the nine in the list below. The group invited a broad range of stakeholders from government, business, civil society and academia to test the framework s usability and robustness through an online survey and public consultation, whereby respondents used the framework to classify a selection of applied AI systems and AI systems (systems 1 to 7) or create an AI system classification of their choice (systems 8 and 9): System 1 Credit-scoring system: Recommendation engine to help gauge a loan applicant s credit- worthiness. It does so by using human-based inputs (e.g. a set of rules) and data inputs (e.g. loan payments histories) to assess whether applicants are repaying loans on a regular basis. System 2 AlphaGo Zero: Plays the board game Go better than professional human players. The board game s environment is virtual and player positions are constrained by the rules of the game. AlphaGo Zero uses both human-based inputs, including the rules of Go, and machine-based inputs, primarily data learned through repeated play against itself. System 3 Recommendation engine: Assists consumers shopping online by generating personalised suggestions based on users' browsing history and data. For instance, Amazon currently uses item-to-item collaborative filtering, which scales to massive datasets and produces high-quality recommendations in real time. This type of filtering matches each of the user's purchased and rated items to similar items, then combines those similar items into tailored recommendations. System 4 Automated voice assistant: Uses Natural Language Processing (NLP) to match user text or voice input to executable commands. Many",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      56
    ],
    "titles": [
      "56     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 61,
    "text": "continually learn using AI techniques including machine learning. Some of these assistants, like Google Assistant (which contains Google Lens) and Samsung Bixby, also have the added ability to do image processing to recognise objects in the image to help the users get better results from the clicked images. System 5 C-CORE scans and processes satellite imagery over ocean areas to locate marine environmental structures or objects such as icebergs. The system determines object type, position and size of identified structures and automatically enters that information into a marine safety database. System 6 CASTER reviews inputted molecular information of drugs for medical research purposes. The system is trained to recognize possible drug interactions and then models organic chemical reactions to predict drug-to-drug interactions, including potential harmful interactions. System 7 Face Image Quality: FIQ is a tool for determining the quality of a digital face image. The system reviews a digital face image and produces a face image quality score, which is used by developers of facial recognition technologies to help determine the reliability of a face image collected through their technology. System 8 Manufacturing plant hybrid management system, such as a Qlector.com LEAP system. System 9 Generic AI system Generative Pre-trained Transformer 3 (GPT-3), taking into account that most of the functionality will depend on the final application context. 3 Applying the framework 56 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 151 respondents provided a complete classification of one of the first seven systems on this list and another 18 respondents provided a complete classification of another AI application or system of their choice. They also provided feedback on the framework throughout the survey. Several questions were rephrased following respondents feedback. Where the responses to criteria lacked consistency, the assumption was that responding required in-depth knowledge of a",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 62,
    "text": "particular application; these criteria were annotated with {where objective and consistent information is available} . See Table 6 for a breakdown of the survey responses. Table 6. Analysis of AI system classification survey results, system examples 1-7 Framework dimension Criterion # possible answers Average consistency C-CORE SCORE CASTER AlphaGo Zero Voice assistant FIQ Recommendation engine % uncertain or blank low medium high People & Planet Users' AI competency 63% Low High High Low High Med High N/A Impacted stakeholders 74% High High High High Low Med Med N/A Optionality 78% High High High Low Low High Med 25% Risks to human rights and democratic values 70% High High High High Med Med Med N/A Potential effects on people s well- being 68% Med Med High High High High Med 8% Potential for human labour displacement 64% Med High High High Med High Med 21% Economic Context Industrial sector N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Business function N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Business model 76% High High Low Low High Low High 22% Impact on critical activities 70% Med Low Low High Med High Med 4% Technical maturity (TRL) 68% High High Low High High High High 7% Data & Input Detection and collection 63% High Low High Low Low Med Low 8% Provenance of the data 75% High Low High Med Med High Med N/A Dynamic nature of the data 60% High High Med Low Med Low Low 3% Rights 72% High High Low Med Low Med Med N/A Identifiability of personal data 85% High High High High High High Low N/A Structure of the data 60% Low High Low High Low Med High 7% Format of the data 66% Low High Low Med Low Low Med",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 63,
    "text": "N/A Scale of the data 59% High Low Med Low Med Med Med 25% Appropriateness and quality of the data 52% Med Low Low High Med Med Low 17% Model Information availability N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Type of AI model 59% High Med Med Med High Low Low 15% Rights associated with model N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Single or multiple models N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Generative or discriminative 80% Med High Med Low High High Med 31% Model building 56% High Low High Low Med High Low 27% Model evolutionML N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Federated or central learningML N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Development and maintenance N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Deterministic or probabilistic N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Model transparency N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Task & Output Task(s) performed by system 78% High High Med Med Med High Med 8% Combining tasks and actions N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A System's level of autonomy 56% Low Low Med High Med Med Low 10% Degree of human involvement 58% Low Low High High Med Med Low 15% Core application N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Evaluation N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Number of fully completed surveys Note: When there are two possible survey responses (e.g. Yes or No), \"High\" consistency means that over 75% of responses are the same, while",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      57
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   57"
    ],
    "chunk_index": 64,
    "text": "\"Medium\" consistency means that over 66% of responses are the same. When there are three possible responses (e.g. Increase/Same/Decrease), \"High\" consistency means that over 65% of responses are the same while \"Medium\" consistency means that over 50% of responses are the same. When there are multiple questions for one criterion (e.g. impacted stakeholders), consistency refers to the average consistency per answer. ML = for machine-learning models. Source: Based on the 151 surveys that were fully completed, out of a total of over 850 surveys that were received in June 2021 (700 of which were partially completed). Builds on (Aiken, 2019[22]). OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 57 OECD 2022 These key conclusions from the survey responses are reflected throughout the report: The framework is best suited to specific applications of AI systems rather than to generic AI systems; thus, classifying an AI system using the framework requires more information on the specific application, context of use and more. The more specific the applications (e.g. credit-scoring system, AlphaGo Zero), the more consistent the survey responses, as many of the criteria pertain to specific application areas. The more general the systems (e.g. voice assistant, recommendation engine) the less consistent the responses. Respondents were significantly better at classifying criteria in the People & Planet and Economic Context dimensions consistently than the criteria in the other, more technical dimensions. Classifying technical characteristics requires more information than is typically available about an AI system; they tended to generate many uncertain or blank responses. The following sections delve deeper into how the four examples Systems 1, 2, 8 and 9 align with each of the dimensions of the OECD Framework for the Classification of AI Systems. System 1: Credit-scoring system A credit-scoring system is representative of a machine-based system that influences its environment (whether",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      58
    ],
    "titles": [
      "58     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 65,
    "text": "people are granted a loan). It makes recommendations (a credit score) for a given set of objectives (that, together, determine credit-worthiness). It does so by using both machine-based inputs (historical data on people s profiles and on whether they repaid loans) and human-based inputs (a set of rules). With these two sets of inputs, the system perceives real environments (whether people have replayed loans in the past or whether they are repaying loans on an ongoing basis). It transforms such perceptions into models automatically. A credit-scoring algorithm could use a statistical model, for example. Finally, it uses model inferencing (the credit-scoring algorithm) to formulate a recommendation (a credit score) of options for outcomes (providing or denying a loan). People & Planet Core characteristic Survey question Response Users of AI system What is the level of competency of users who interact with the system? Amateur (bank employee) Impacted stakeholders Who is impacted by the system (e.g. consumers, workers, government agencies)? Consumers, the bank Optionality and redress Can users opt out, e.g. switch systems? Can users challenge or correct the output? Not optional / cannot opt out Human rights and democratic values Can the system s outputs impact fundamental human rights? Possible impact on: - rule of law; absence of arbitrary sentencing - equality and non-discrimination - right to property - economic and social rights Well-being, society and the environment Can the system s outputs impact areas of life related to well-being (e.g. job quality, the environment, health, social interactions, civic engagement, education)? Possible impact on: - physical and mental health - work and job quality - quality of environment - social connections - civic engagement - education - work-life balance {Displacement potential} Could the system automate tasks that are or were being executed by humans? Depends on deployment context 58 OECD",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 66,
    "text": "FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Economic Context Core characteristic Survey question Response Industrial sector Which industrial sector is the system deployed in (e.g. finance, agriculture)? Section K: Financial and insurance activities (per ISIC REV 4) Business function What business function(s) or functional areas is the AI system employed in (e.g. sales, customer service, human resources)? Sales, customer service Business model Is the system a for-profit use, non-profit use or public service system? For-profit use other model Impacts critical functions / activities Would the disruption of the system s function or activity affect essential services? Yes Breadth of deployment Is the AI system deployment a pilot, narrow, broad or widespread? Broad {Technical maturity} How technically mature is the system (Technology Readiness Level TRL)? Actual system or subsystem in final form in operational environment TRL 9 Data & Input Core characteristic Survey question Response Detection and collection Are the data and input collected by humans, automated sensors, both? Humans (set of rules) and automated sensing devices (e.g. loan payments) Provenance of data and input Are the data and input from experts; provided, observed, synthetic or derived? - provided by experts (rules) - provided by loan candidate (e.g. personal information) - observed by the algorithm (e.g. history of payments) - derived data (e.g. credit rating and other scores) Dynamic nature Are the data dynamic, static, dynamic updated from time to time or real-time? - static (e.g. gender); - dynamic data updated from time to time (e.g. salary) - dynamic real-time data (e.g. day-to-day payments Rights associated with data and input Are the data proprietary (privately held), public (no intellectual property rights) or personal data (related to identifiable individual)? Personal and proprietary Identifiability of personal data If personal data, are they anonymised or pseudonymised? Identified data {Data quality and appropriateness}",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      59
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   59"
    ],
    "chunk_index": 67,
    "text": "Is the dataset fit for purpose? Is the sample size adequate? Is it representative and complete enough? How noisy are the data? - quality unknown - appropriate data {Structure of the data and input} Are the data structured, semi-structured, complex structured or unstructured? Structured data {Format of data and metadata} Is the format of the data and metadata standardised or non- standardised? Standardised {Scale} What is the dataset s scale? Small or medium AI Model Core characteristic Survey question Response Model information availability Is any information available about the system s model? Yes AI model type Is the model symbolic (human-generated rules), statistical (uses data) or hybrid? Hybrid {Rights associated with model} Is the model open-source or proprietary, self or third-party managed? Proprietary {Discriminative or generative} Is the model generative, discriminative or both? Discriminative (score as a probability) {Single or multiple model(s)} Is the system composed of one model or several interlinked models? Yes Model-building from machine or human knowledge Does the system learn based on human-written rules, from data, through supervised learning or through reinforcement learning? Acquisition from data, augmented by human-encoded knowledge Model evolution in the field (applicable only to machine- learning systems) Does the model evolve and / or acquire abilities from interacting with data in the field? Evolution during operation through passive interaction Central or federated learning (applicable only to machine- learning systems) Is the model trained centrally or in a number of local servers or edge devices? Central {Model development and maintenance} Is the model universal, customisable or tailored to the AI actor s data? Context-dependent {Deterministic and probabilistic} Is the model used in a deterministic or probabilistic manner? Deterministic Transparency and explainability Is information available to users to allow them to understand model outputs? Context-dependent OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 59",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 68,
    "text": "OECD 2022 Task & Output Core characteristic Survey question Response Task(s) of the system What tasks does the system perform (e.g. recognition, event detection, forecasting)? Forecasting, reasoning with knowledge structures {Combining tasks and actions into composite systems} Does the system combine several tasks and actions (e.g. content generation systems, autonomous systems, control systems)? Yes Action autonomy How autonomous are the system s actions and what role do humans play? Low autonomy Core application area(s) Does the system belong to a core application area such as human language technologies, computer vision, automation and / or optimisation or robotics? Human language technologies {Evaluation methods} Are there standards or methods available for evaluating system output? Yes System 2: AlphaGo Zero AlphaGo Zero is an AI system that plays the board game Go better than any professional, human Go players. The board game s environment is virtual and fully observable. Game positions are constrained by the objectives and the rules of the game. AlphaGo Zero is a system that uses both human-based inputs (the rules of the game) and machine-based inputs (learning based on playing iteratively against itself, starting from completely random play). It abstracts the data into a stochastic (randomly determined) model of actions ( moves in the game) and is trained via so-called reinforcement learning. Finally, it uses the model to propose a new move based on the state of play. People & Planet Core characteristic Survey question Response Users of AI system What is the level of competency of users who interact with the system? Expert practitioner (e.g. DeepMind engineers) Impacted stakeholders Who is impacted by the system (e.g. consumers, workers, government agencies)? None for now; if deployed in production, specific communities (e.g. Go players) Optionality and redress Can users opt out, e.g. switch systems? Can users challenge or correct the",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      60
    ],
    "titles": [
      "60     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 69,
    "text": "output? If deployed in production, some optionality Human rights and democratic values Can the system s outputs impact fundamental human rights? No Well-being, society and the environment Can the system s outputs impact areas of life related to well-being (e.g. job quality, the environment, health, social interactions, civic engagement, education)? No {Displacement potential} Could the system automate tasks that are or were being executed by humans? Low displacement potential (TBD) Economic Context Core characteristic Survey question Response Industrial sector Which industrial sector is the system deployed in (e.g. finance, agriculture)? Section R: Arts, Entertainment, and Recreation (per ISIC REV 4) Business function What business function(s) or functional areas is the AI system employed in (e.g. sales, customer service, human resources)? N/A Business model Is the system a for-profit use, non-profit use or public service system? Non-profit (outside public sector) or for profit Impacts critical functions / activities Would the disruption of the system s function or activity affect essential services? No Breadth of deployment Is the AI system deployment a pilot, narrow, broad or widespread? Narrow {Technical maturity} How technically mature is the system (Technology Readiness Level TRL)? System or subsystem complete and qualified through test and demonstration TRL 8 60 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Data & Input Core characteristic Survey response Response Detection and collection Are the data and input collected by humans, automated sensors, both? Humans (the rules of the game of Go) and automated sensing devices Provenance of data and input Are the data and input from experts; provided, observed, synthetic or derived? Provided by experts (the rules of the game of Go), observed by the algorithm, and synthetic data Dynamic nature Are the data dynamic, static, dynamic updated from time to time or real-time? Static (human knowledge) and dynamic, real-time",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 70,
    "text": "data (each move in the game) Rights associated with data and input Are the data proprietary (privately held), public (no intellectual property rights) or personal data (related to identifiable individual)? Public and proprietary Identifiability of personal data If personal data, are they anonymised; pseudonymised? N/A {Data quality and appropriateness} Is the dataset fit for purpose? Is the sample size adequate? Is it representative and complete enough? How noisy are the data? Representative and appropriate, low noise/missing values/outliers {Structure of the data and input} Are the data structured, semi-structured, complex structured or unstructured? Complex structured {Format of data and metadata} Is the format of the data and metadata standardised or non- standardised? Standardised and non-standardised {Scale} What is the dataset s scale? TBD (large or very large) AI Model Core characteristic Survey question Response Model information availability Is any information available about the system s model? Yes AI model type Is the model symbolic (human-generated rules), statistical (uses data) or hybrid? Hybrid {Rights associated with model} Is the model open-source or proprietary, self or third-party managed? Proprietary {Discriminative or generative} Is the model generative, discriminative or both? Generative {Single or multiple model(s)} Is the system composed of one model or several interlinked models? One model Model-building from machine or human knowledge Does the system learn based on human-written rules, from data, through supervised learning or through reinforcement learning? Acquisition from data, augmented by human-encoded knowledge Model evolution in the field ML Does the model evolve and / or acquire abilities from interacting with data in the field? Evolution during operation through passive interaction Central or federated learning ML Is the model trained centrally or in a number of local servers or edge devices? Central {Model development and maintenance} Is the model universal, customisable or tailored to the AI actor s data?",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      61
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   61"
    ],
    "chunk_index": 71,
    "text": "Context-dependent {Deterministic and probabilistic} Is the model used in a deterministic or probabilistic manner? Both Transparency and explainability Is information available to users to allow them to understand model outputs? Context-dependent Task & Output Core characteristic Survey question Response Task(s) of the system What tasks does the system perform (e.g. recognition, event detection, forecasting)? Forecasting, goal-driven optimization, reasoning with knowledge structures {Combining tasks and actions into composite systems} Does the system combine several tasks and actions (e.g. content generation systems, autonomous systems, control systems)? Yes Action autonomy How autonomous are the system s actions and what role do humans play? High Core application area(s) Does the system belong to a core application area such as human language technologies, computer vision, automation and / or optimisation or robotics? Human language technologies and/or computer vision (TBC) {Evaluation methods} Are there standards or methods available for evaluating system output? Yes OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 61 OECD 2022 System 3: Qlector.com LEAP system to manage a manufacturing plant An AI system controlling and running a manufacturing plant is a representative example of a complex hybrid AI system, with the specific context of the physical manufacturing plant factory floor. Different AI models, associated with different data sources perform particular activities for the factory based on different types of data and input. These modelling activities include: event detection (anomaly detection based on data from machines in production lines); goal-driven optimisation (based on orders and schedules, logistics and supply chain data); reasoning with knowledge structures (simulations); interaction support (customer relationship management); demand forecasting (based on sales); and strategic market forecasting (based on market research). For illustrative purposes, all of these models can be combined into a large, evolving knowledge graph with a symbolic AI type of data structure that interconnects the different tasks",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      62
    ],
    "titles": [
      "62     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 72,
    "text": "performed at the factory (Figure 8). The resulting AI model is a hybrid analytical model of a manufacturing plant that some could refer to as a digital twin of the factory. The outputs of the model include: alerts, information on dashboards, scheduling, communications with customers, and simulations of possible futures to inform decisions. While humans are often involved in the actions resulting from the system outputs, factory processes are increasingly autonomous. The output/decision feeds back into the context/physical environment. Figure 8. AI system to help manage a manufacturing plant 62 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 People & Planet Core characteristic Survey question Response Users of AI system What is the level of competency of users who interact with the system? Amateurs, non-expert and expert practitioners Impacted stakeholders Who is impacted by the system (e.g. consumers, workers, government agencies)? Consumers, workers / employees, business Optionality and redress Can users opt out, e.g. switch systems? Can users challenge or correct the output? Varies Human rights and democratic values Can the system s outputs impact fundamental human rights? Possible impact on: - physical, psychological and moral integrity - equality and non-discrimination Well-being, society and the environment Can the system s outputs impact areas of life related to well-being (e.g. job quality, the environment, health, social interactions, civic engagement, education)? Possible impact on: - health - income and wealth - environmental quality - social connections - work and job quality - work-life balance {Displacement potential} Could the system automate tasks that are or were being executed by humans? High displacement potential Economic Context Core characteristic Survey question Response Industrial sector Which industrial sector is the system deployed in (e.g. finance, agriculture)? Section C: Manufacturing (per ISIC REV 4) Business function What business function(s) or functional areas is the AI",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 73,
    "text": "system employed in, (e.g. sales, customer service, human resources)? Many, including sales, customer service, planning and budgeting, procurement, logistics, human resource management, monitoring and quality control, production, maintenance Business model Is the system a for-profit use, non-profit use or public service system? For-profit use Impacts critical functions / activities Would the disruption of the system s function or activity affect essential services? Depends on the type of goods manufactured Breadth of deployment Is the AI system deployment a pilot, narrow, broad or widespread? Narrow {Technical maturity} How technically mature is the system (Technology Readiness Level TRL)? Actual system or subsystem in final form in operational environment TRL 9 Data & Input Core characteristic Survey question Response Detection and collection Are the data and input collected by humans, automated sensors, both? Humans and automated sensing devices Provenance of data and input Are the data and input from experts; provided, observed, synthetic or derived? All (expert input, provided data, observed data, synthetic data and derived data) Dynamic nature Are the data dynamic, static, dynamic, updated from time to time or real-time? Static (human knowledge) and dynamic real-time data (data from machines in production lines) Rights associated with data and input Are the data proprietary (privately held), public (no intellectual property rights) or personal data (related to identifiable individual)? Proprietary Identifiability of personal data If personal data, are they anonymised, pseudonymised? N/A {Data quality and appropriateness} Is the dataset fit for purpose? Is the sample size adequate? Is it representative and complete enough? How noisy are the data? Representative and appropriate, noise /missing values/outliers {Structure of the data and input} Are the data structured, semi-structured, complex structured or unstructured? All (unstructured, semi-structured, unstructured, complex structured) {Format of data and metadata} Is the format of the data and metadata standardised or non- standardised? Standardised",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      63
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   63"
    ],
    "chunk_index": 74,
    "text": "and non-standardised {Scale} What is the dataset s scale? Medium OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 63 OECD 2022 AI Model Core characteristic Survey question Response Model information availability Is any information available about the system s model? Yes AI model type Is the model symbolic (human-generated rules), statistical (uses data) or hybrid? Hybrid {Rights associated with model} Is the model open-source or proprietary, self or third-party managed? Proprietary {Discriminative or generative} Is the model generative, discriminative or both? Discriminative and generative {Single or multiple model(s)} Is the system composed of one model or several interlinked models? Yes Model building from machine or human knowledge Does the system learn based on human-written rules, from data, through supervised learning or through reinforcement learning? Acquisition from data, augmented by human-encoded knowledge Model evolution in the field ML Does the model evolve and / or acquire abilities from interacting with data in the field? Evolution during operation through active and passive interaction Central or federated learning ML Is the model trained centrally or in a number of local servers or edge devices? Federated {Model development and maintenance} Is the model universal, customisable or tailored to the AI actor s data? Context-dependent {Deterministic and probabilistic} Is the model used in a deterministic or probabilistic manner? Both Transparency and explainability Is information available to users to allow them to understand model outputs? Context-dependent Task & Output Core characteristic Survey question Response Task(s) of the system What tasks does the system perform (e.g. recognition, event detection, forecasting)? All (recognition, event detection, forecasting, interaction support, goal- driven optimization, reasoning with knowledge structures) {Combining tasks and actions into composite systems} Does the system combine several tasks and actions (e.g. content generation systems, autonomous systems, control systems)? Yes Action autonomy How autonomous are the system s actions",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      64
    ],
    "titles": [
      "64     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 75,
    "text": "and what role do humans play? Medium autonomy Core application area(s) Does the system belong to a core application area such as human language technologies, computer vision, automation and / or optimisation or robotics? Human language technologies, robotics, computer vision, optimisation {Evaluation methods} Are there standards or methods available for evaluating system output? Yes System 4: GPT-3 GPT-3 is a large, pre-trained language model that has the capacity to search across, generate and manipulate strings of text. The model can take in arbitrary inputs, in the form of text strings, which lead to it generating an output. GPT-3 can be conditioned with up to 2048 distinct characters, which enable it to learn from the examples it is primed with. GPT-3 is a general purpose AI system, meaning it can theoretically be used to deploy applications in any sector of the economy. Such applications would need to be considered within their specific socio-economic context; for example, a creative-writing application built with GPT3 should be treated differently from one that seeks to give a user medical advice in response to a query. Examples of GPT3 use cases include text classification activities to search across news articles and generation of emails from a summary sentence. The example of creative writing is applied to the classification framework below. 64 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 People & Planet Core characteristic Survey question Response Users of AI system What is the level of competency of users who interact with the system? Amateur Impacted stakeholders Who is impacted by the system (e.g. consumers, workers, government agencies)? Workers (e.g. could lead to automation of some tasks), consumers Optionality and redress Can users opt out, e.g. switch systems? Can users challenge or correct the output? Optional / can opt out Human rights and democratic",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 76,
    "text": "values Can the system s outputs impact fundamental human rights? Possible impact on: - rule of law, absence of arbitrary sentencing - freedom of thought, conscience and religion - equality and non-discrimination - quality of democratic institutions (e.g. free elections) Well-being, society and the environment Can the system s outputs impact areas of life related to well-being (e.g. job quality, the environment, health, social interactions, civic engagement, education)? Possible impact on: - work and job quality - education {Displacement potential} Could the system automate tasks that are or were being executed by humans? TBD Economic Context Core characteristic Survey question Response Industrial sector Which industrial sector is the system deployed in (e.g. finance, agriculture)? Section J: Information and Communication (per ISIC REV 4) Business function What business function(s) or functional areas is the AI system employed in (e.g. sales, customer service, human resources)? Any Business model Is the system a for-profit use, non-profit use or public service system? For-profit use other model (e.g. business intelligence) or non-profit use (e.g. research, journalism) Impacts critical functions / activities Would the disruption of the system s function or activity affect essential services? No Breadth of deployment Is the AI system deployment a pilot, narrow, broad or widespread? Narrow deployment {Technical maturity} How technically mature is the system (Technology Readiness Level TRL)? System prototype demonstration in operational environment TRL 7 Data & Input Core characteristic Survey question Response Detection and collection Are the data and input collected by humans, automated sensors, both? Collected by humans and automated sensing devices (e.g. collected by humans with subsequent filtering by machines and humans) Provenance of data and input Are the data and input from experts; provided, observed, synthetic or derived? Observed and derived Dynamic nature Are the data dynamic, static, dynamic updated from time to time or",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      65
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   65"
    ],
    "chunk_index": 77,
    "text": "real- time? Dynamic data updated from time to time Rights associated with data and input Are the data proprietary (privately held), public (no intellectual property rights) or personal data (related to identifiable individual)? Public and proprietary Identifiability of personal data If personal data, are they anonymised, pseudonymised? N/A {Data quality and appropriateness} Is the dataset fit for purpose? Is the sample size adequate? Is it representative and complete enough? How noisy are the data? Noisy data, that is, by design, highly representative and diverse with regard to a large part of (predominantly English) text and code found on the Internet; appropriate data {Structure of the data and input} Are the data structured, semi-structured, complex structured or unstructured? Unstructured data {Format of data and metadata} Is the format of the data and metadata standardised or non- standardised? Non-standardised {Scale} What is the dataset s scale? Very large OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 65 OECD 2022 AI Model Core characteristic Survey question Response Model information availability Is any information available about the system s model? Yes AI model type Is the model symbolic (human-generated rules), statistical (uses data) or hybrid? Statistical (data-driven) {Rights associated with model} Is the model open-source or proprietary, self or third-party managed? Proprietary {Discriminative or generative} Is the model generative, discriminative or both? Generative {Single or multiple model(s)} Is the system composed of one model or several interlinked models? One Model-building from machine or human knowledge Does the system learn based on human-written rules, from data, through supervised learning or through reinforcement learning? Acquisition from data, augmented by human-encoded knowledge Model evolution in the field ML Does the model evolve and / or acquire abilities from interacting with data in the field? Evolution during operation through passive interaction Central or federated learning ML Is the",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      66
    ],
    "titles": [
      "66     OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS"
    ],
    "chunk_index": 78,
    "text": "model trained centrally or in a number of local servers or edge devices? Central {Model development and maintenance} Is the model universal, customisable or tailored to the AI actor s data? Context-dependent {Deterministic and probabilistic} Is the model used in a deterministic or probabilistic manner? Deterministic Transparency and explainability Is information available to users to allow them to understand model outputs? Context-dependent Task & Output Core characteristic Survey question Response Task(s) of the system What tasks does the system perform (e.g. recognition, event detection, forecasting)? Reasoning with knowledge structures, interaction support, recognition, personalisation {Combining tasks and actions into composite systems} Does the system combine several tasks and actions (e.g. content generation systems, autonomous systems, control systems)? Yes Action autonomy How autonomous are the system s actions and what role do humans play? Low autonomy Core application area(s) Does the system belong to a core application area such as human language technologies, computer vision, automation and / or optimisation or robotics? Human language technologies {Evaluation methods} Are there standards or methods available for evaluating system output? TBD 66 OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS OECD 2022 Refining classification criteria based on real-world evidence In an effort to add value to the framework and make it more actionable and meaningful to stakeholders looking to incorporate an AI system into their activities, next steps for the OECD Experts Working Group include populating the current classification system with actual AI systems and refining the specific classification criteria based on this evidence. The refinement process includes identifying or developing metrics or proxies to help assess subjective criteria such as impact on human rights and well-being. It is important to point out that there may be a trade-off between developing a simple, user-friendly assessment (which was the goal of the initial classification framework exercise)",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [
      67
    ],
    "titles": [
      "OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS   67"
    ],
    "chunk_index": 79,
    "text": "and a very accurate assessment, as the latter may require significant in-depth information on an AI system that may be unknown to the average user. Some contexts may require more detailed follow-up questions to assess AI systems that are not relevant in others. If the potential impacts are large, the focus may need to be on potential biases in the data or modelling process and focus, or on data representativeness, that is, whether such data can impact the decisioning system. For example, whether the ethnicity of a car owner might impact an insurance claim evaluation, whether the model should move to an unexamined state, etc. Tracking AI incidents A related next step is to develop a common framework for reporting AI incidents, especially those that are negative or harmful, or controversies. The incidents framework would leverage the classification framework and help to ensure global consistency and interoperability in incident reporting. It would be part of a Global AI Incidents Tracker at the OECD, with the contributions of partner institutions, to build the evidence base about risks that have materialised into incidents or near incidents. Developing a risk assessment framework The classification framework presented in this report describes key aspects of an applied AI system, including the various contexts in which it impacts the real world, the nature and type of data and input, the various AI models, and the types of tasks it executes and output it produces. Information derived from this framework for a specific use case, augmented by information on governance and risk mitigation processes, could be useful in assessing the associated ethical and societal risks of an applied AI system, which may have considerable practical significance for stakeholders in numerous contexts policy making, business, etc. 4 Next steps OECD FRAMEWORK FOR THE CLASSIFICATION OF AI SYSTEMS 67",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 80,
    "text": "OECD 2022 Box 5. AI risk-based approaches to AI system application Policy makers favour a risk-based approach to regulating AI in order to focus oversight and intervention where it is most needed, while avoiding unnecessary hurdles to innovation. The OECD AI Principles state that AI actors should, based on their roles, the context, and their ability to act, apply a systematic risk management approach to each phase of the AI system lifecycle on a continuous basis to address risks related to AI systems, including privacy, digital security, safety and bias . The risks in using any AI system strongly depend on the application. Since it is difficult to anticipate and assess every possible use case, applied AI systems should be grouped into a small collection of risk levels. As part of the EC AI Act, the European Commission put forward four levels of risk: unacceptable, high, limited and minimal. Various academic groups as well as expert panels (e.g. the German Data Ethics Commission and IEC SEG10) have proposed four to five risk levels. And the ISO, National Institute of Standards and Technology (NIST), Institute of Electrical and Electronics Engineers (IEEE) and others are working on risk-assessment and risk-management frameworks from different angles and objectives. Regardless of the number of risk levels or which organisation proposes them, the following are typical criteria for determining the risk level of an AI application or system: Scale, i.e. seriousness of adverse impacts (and probability) Scope, i.e. breadth of application, such as the number of individuals that are or will be affected Optionality, i.e. degree of choice as to whether to be subject to the effects of an AI system Part of the OECD s value add is its capability to involve and coordinate with several groups working on AI risk assessment and management so",
    "n_words": 300
  },
  {
    "pdf": "oecd_ai_classification_framework.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 81,
    "text": "as to promote international interoperability in designing technical, policy and governance AI risk frameworks. The OECD Experts Working Group, with members from across sectors and professions, plans to conduct further analysis of the criteria to include in a risk assessment and how best to aggregate these criteria, taking into account that different criteria may be interdependent. The group will use examples of AI systems in clearly different risk categories to assess the usefulness of different criteria and to try to calibrate these criteria in an empirical way where possible, leveraging evidence from its Global AI Incidents Tracker. The next phase of work is expected to produce an actionable AI system risk methodology, which will build on the current AI system classification framework and on other work streams taking place in partner organisations, as well as, for example, the OECD AI Network of Experts Expert Group on Trustworthy AI. It should be emphasised that discussions about handling AI risks rely on and are complementary to existing, well-established risk assessment frameworks, e.g. for functional and product safety (OECD, 2016[23]) or digital security (OECD, 2015[24]), as well as existing frameworks for quality management systems; hence, these discussions tend to focus on ethical and societal risks. Existing human rights and responsible business-impact assessment guidelines are also directly relevant (OECD, 2018[25]). As of the development of this report, coordination with these groups and with partner organisations has begun.",
    "n_words": 233
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      4
    ],
    "titles": [
      "EXECUTIVE SUMMARY"
    ],
    "chunk_index": 1,
    "text": "EXECUTIVE SUMMARY The aim of the Guidelines is to promote Trustworthy AI. Trustworthy AI has three components, which should be met throughout the system's entire life cycle: (1) it should be lawful, complying with all applicable laws and regulations (2) it should be ethical, ensuring adherence to ethical principles and values and (3) it should be robust, both from a technical and social perspective since, even with good intentions, AI systems can cause unintentional harm. Each component in itself is necessary but not sufficient for the achievement of Trustworthy AI. Ideally, all three components work in harmony and overlap in their operation. If, in practice, tensions arise between these components, society should endeavour to align them. These Guidelines set out a framework for achieving Trustworthy AI. The framework does not explicitly deal with Trustworthy AI s first component (lawful AI).1 Instead, it aims to offer guidance on the second and third components: fostering and securing ethical and robust AI. Addressed to all stakeholders, these Guidelines seek to go beyond a list of ethical principles, by providing guidance on how such principles can be operationalised in socio- technical systems. Guidance is provided in three layers of abstraction, from the most abstract in Chapter I to the most concrete in Chapter III, closing with examples of opportunities and critical concerns raised by AI systems. I. Based on an approach founded on fundamental rights, Chapter I identifies the ethical principles and their correlated values that must be respected in the development, deployment and use of AI systems. Key guidance derived from Chapter I: Develop, deploy and use AI systems in a way that adheres to the ethical principles of: respect for human autonomy, prevention of harm, fairness and explicability. Acknowledge and address the potential tensions between these principles. Pay particular attention to situations",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 2,
    "text": "involving more vulnerable groups such as children, persons with disabilities and others that have historically been disadvantaged or are at risk of exclusion, and to situations which are characterised by asymmetries of power or information, such as between employers and workers, or between businesses and consumers.2 Acknowledge that, while bringing substantial benefits to individuals and society, AI systems also pose certain risks and may have a negative impact, including impacts which may be difficult to anticipate, identify or measure (e.g. on democracy, the rule of law and distributive justice, or on the human mind itself.) Adopt adequate measures to mitigate these risks when appropriate, and proportionately to the magnitude of the risk. II. Drawing upon Chapter I, Chapter II provides guidance on how Trustworthy AI can be realised, by listing seven requirements that AI systems should meet. Both technical and non-technical methods can be used for their implementation. Key guidance derived from Chapter II: Ensure that the development, deployment and use of AI systems meets the seven key requirements for Trustworthy AI: (1) human agency and oversight, (2) technical robustness and safety, (3) privacy and data governance, (4) transparency, (5) diversity, non-discrimination and fairness, (6) environmental and societal well-being and (7) accountability. Consider technical and non-technical methods to ensure the implementation of those requirements. All normative statements in this document aim to reflect guidance towards achieving the second and third component of trustworthy AI (ethical and robust AI). These statements are hence not meant to provide legal advice or to offer guidance on compliance with applicable laws, though it is acknowledged that many of these statements are to some extent already reflected in existing laws. In this regard, see 21 and following. See articles 24 to 27 of the Charter of Fundamental Rights of the EU (EU Charter), dealing with",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      5
    ],
    "titles": [],
    "chunk_index": 3,
    "text": "the rights of the child and the elderly, the integration of persons with disabilities and workers rights. See also article 38 dealing with consumer protection. Foster research and innovation to help assess AI systems and to further the achievement of the requirements; disseminate results and open questions to the wider public, and systematically train a new generation of experts in AI ethics. Communicate, in a clear and proactive manner, information to stakeholders about the AI system s capabilities and limitations, enabling realistic expectation setting, and about the manner in which the requirements are implemented. Be transparent about the fact that they are dealing with an AI system. Facilitate the traceability and auditability of AI systems, particularly in critical contexts or situations. Involve stakeholders throughout the AI system s life cycle. Foster training and education so that all stakeholders are aware of and trained in Trustworthy AI. Be mindful that there might be fundamental tensions between different principles and requirements. Continuously identify, evaluate, document and communicate these trade-offs and their solutions. III. Chapter III provides a concrete and non-exhaustive Trustworthy AI assessment list aimed at operationalising the key requirements set out in Chapter II. This assessment list will need to be tailored to the specific use case of the AI system.3 Key guidance derived from Chapter III: Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems, and adapt it to the specific use case in which the system is being applied. Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy AI is not about ticking boxes, but about continuously identifying and implementing requirements, evaluating solutions, ensuring improved outcomes throughout the AI system s lifecycle, and involving stakeholders in this. A final section of the document aims to concretise some of the issues",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 4,
    "text": "touched upon throughout the framework, by offering examples of beneficial opportunities that should be pursued, and critical concerns raised by AI systems that should be carefully considered. While these Guidelines aim to offer guidance for AI applications in general by building a horizontal foundation to achieve Trustworthy AI, different situations raise different challenges. It should therefore be explored whether, in addition to this horizontal framework, a sectorial approach is needed, given the context-specificity of AI systems. These Guidelines do not intend to substitute any form of current or future policymaking or regulation, nor do they aim to deter the introduction thereof. They should be seen as a living document to be reviewed and updated over time to ensure their continuous relevance as the technology, our social environments, and our knowledge evolve. This document is a starting point for the discussion about Trustworthy AI for Europe .4 Beyond Europe, the Guidelines also aim to foster research, reflection and discussion on an ethical framework for AI systems at a global level. In line with the scope of the framework, this assessment list does not provide any advice on ensuring legal compliance (lawful AI), but limits itself to offering guidance on meeting the second and third components of trustworthy AI (ethical and robust AI). This ideal is intended to apply to AI systems developed, deployed and used in the Member States of the European Union (EU), as well as to systems developed or produced elsewhere but deployed and used in the EU. When referring to \"Europe\" in this document, we mean this to encompass the EU Member States. However, these Guidelines also aspire to be relevant outside the EU. In this regard, it can also be noted that both Norway and Switzerland are part of the Coordinated Plan on AI agreed and published",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      6
    ],
    "titles": [
      "A. INTRODUCTION"
    ],
    "chunk_index": 5,
    "text": "in December 2018 by the Commission and Member States. A. INTRODUCTION In its Communication of 25 April 2018 and 7 December 2018, the European Commission set out its vision for artificial intelligence (AI), which supports ethical, secure and cutting-edge AI made in Europe .5 Three pillars underpin the Commission s vision: (i) increasing public and private investments in AI to boost its uptake, (ii) preparing for socio-economic changes, and (iii) ensuring an appropriate ethical and legal framework to strengthen European values. To support the implementation of this vision, the Commission established the High-Level Expert Group on Artificial Intelligence (AI HLEG), an independent group mandated with the drafting of two deliverables: (1) AI Ethics Guidelines and (2) Policy and Investment Recommendations. This document contains the AI Ethics Guidelines, which have been revised following further deliberation by our Group in light of feedback received from the public consultation on the draft published on 18 December 2018. It builds on the work of the European Group on Ethics in Science and New Technologies6 and takes inspiration from other similar efforts.7 Over the past months, the 52 of us met, discussed and interacted, committed to the European motto: united in diversity. We believe that AI has the potential to significantly transform society. AI is not an end in itself, but rather a promising means to increase human flourishing, thereby enhancing individual and societal well-being and the common good, as well as bringing progress and innovation. In particular, AI systems can help to facilitate the achievement of the UN s Sustainable Development Goals, such as promoting gender balance and tackling climate change, rationalising our use of natural resources, enhancing our health, mobility and production processes, and supporting how we monitor progress against sustainability and social cohesion indicators. To do this, AI systems8 need to be",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 6,
    "text": "human-centric, resting on a commitment to their use in the service of humanity and the common good, with the goal of improving human welfare and freedom. While offering great opportunities, AI systems also give rise to certain risks that must be handled appropriately and proportionately. We now have an important window of opportunity to shape their development. We want to ensure that we can trust the socio- technical environments in which they are embedded. We also want producers of AI systems to get a competitive advantage by embedding Trustworthy AI in their products and services. This entails seeking to maximise the benefits of AI systems while at the same time preventing and minimising their risks. In a context of rapid technological change, we believe it is essential that trust remains the bedrock of societies, communities, economies and sustainable development. We therefore identify Trustworthy AI as our foundational ambition, since human beings and communities will only be able to have confidence in the technology s development and its applications when a clear and comprehensive framework for achieving its trustworthiness is in place. This is the path that we believe Europe should follow to become the home and leader of cutting-edge and ethical technology. It is through Trustworthy AI that we, as European citizens, will seek to reap its benefits in a way that is aligned with our foundational values of respect for human rights, democracy and the rule of law. Trustworthy AI Trustworthiness is a prerequisite for people and societies to develop, deploy and use AI systems. Without AI systems and the human beings behind them being demonstrably worthy of trust, unwanted consequences may ensue and their uptake might be hindered, preventing the realisation of the potentially vast social and economic COM(2018)237 and COM(2018)795. Note that the term made in Europe",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      7
    ],
    "titles": [
      "1. it should be lawful, complying with all applicable laws and regulations;"
    ],
    "chunk_index": 7,
    "text": "is used throughout the Commission s communication. The scope of these Guidelines however aims to encompass not only those AI systems made in Europe, but also those developed elsewhere and deployed or used in Europe. Throughout this document, we hence aim to promote trustworthy AI for Europe. The European Group on Ethics in Science and New Technologies (EGE) is an advisory group of the Commission. See Section 3.3 of COM(2018)237. The Glossary at the end of this document provides a definition of AI systems for the purpose of this document. This definition is further elaborated on in a dedicated document prepared by the AI HLEG that accompanies these Guidelines, titled \"A definition of AI: Main capabilities and scientific disciplines\". benefits that they can bring. To help Europe realise those benefits, our vision is to ensure and scale Trustworthy AI. Trust in the development, deployment and use of AI systems concerns not only the technology s inherent properties, but also the qualities of the socio-technical systems involving AI applications.9 Analogous to questions of (loss of) trust in aviation, nuclear power or food safety, it is not simply components of the AI system but the system in its overall context that may or may not engender trust. Striving towards Trustworthy AI hence concerns not only the trustworthiness of the AI system itself, but requires a holistic and systemic approach, encompassing the trustworthiness of all actors and processes that are part of the system s socio-technical context throughout its entire life cycle. Trustworthy AI has three components, which should be met throughout the system's entire life cycle: 1. it should be lawful, complying with all applicable laws and regulations; 2. it should be ethical, ensuring adherence to ethical principles and values; and 3. it should be robust, both from a technical and social",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 8,
    "text": "perspective, since, even with good intentions, AI systems can cause unintentional harm. Each of these three components is necessary but not sufficient in itself to achieve Trustworthy AI.10 Ideally, all three work in harmony and overlap in their operation. In practice, however, there may be tensions between these elements (e.g. at times the scope and content of existing law might be out of step with ethical norms). It is our individual and collective responsibility as a society to work towards ensuring that all three components help to secure Trustworthy AI.11 A trustworthy approach is key to enabling responsible competitiveness , by providing the foundation upon which all those affected by AI systems can trust that their design, development and use are lawful, ethical and robust. These Guidelines are intended to foster responsible and sustainable AI innovation in Europe. They seek to make ethics a core pillar for developing a unique approach to AI, one that aims to benefit, empower and protect both individual human flourishing and the common good of society. We believe that this will enable Europe to position itself as a global leader in cutting-edge AI worthy of our individual and collective trust. Only by ensuring trustworthiness will European individuals fully reap AI systems benefits, secure in the knowledge that measures are in place to safeguard against their potential risks. Just as the use of AI systems does not stop at national borders, neither does their impact. Global solutions are therefore required for the global opportunities and challenges that AI systems bring forth. We therefore encourage all stakeholders to work towards a global framework for Trustworthy AI, building international consensus while promoting and upholding our fundamental rights-based approach. Audience and Scope These guidelines are addressed to all AI stakeholders designing, developing, deploying, implementing, using or being affected by",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      8
    ],
    "titles": [
      "B. A FRAMEWORK FOR TRUSTWORTHY AI"
    ],
    "chunk_index": 9,
    "text": "AI, including but not limited to companies, organisations, researchers, public services, government agencies, institutions, civil society organisations, individuals, workers and consumers. Stakeholders committed towards achieving Trustworthy AI can voluntarily opt to use these Guidelines as a method to operationalise their commitment, in particular by using the practical assessment list of Chapter III when developing, deploying or using AI systems. This assessment list can also complement and hence be incorporated in existing assessment processes. The Guidelines aim to provide guidance for AI applications in general, building a horizontal foundation to achieve Trustworthy AI. However, different situations raise different challenges. AI music recommendation systems do not These systems comprise humans, state actors, corporations, infrastructure, software, protocols, standards, governance, existing laws, oversight mechanisms, incentive structures, auditing procedures, best practices reporting and others. This does not exclude the fact that additional conditions may be(come) necessary. This also means that the legislature or policy-makers may need to review the adequacy of existing law where these might be out of step with ethical principles. raise the same ethical concerns as AI systems proposing critical medical treatments. Likewise, different opportunities and challenges arise from AI systems used in the context of business-to-consumer, business-to- business, employer-to-employee and public-to-citizen relationships, or more generally, in different sectors or use cases. Given the context-specificity of AI systems, the implementation of these Guidelines needs to be adapted to the particular AI-application. Moreover, the necessity of an additional sectorial approach, to complement the more general horizontal framework proposed in this document, should be explored. To gain a better understanding of how this guidance can be implemented at a horizontal level, and of those matters that require a sectorial approach, we invite all stakeholders to pilot the Trustworthy AI assessment list (Chapter III) that operationalises this framework and to provide us feedback. Based on",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 10,
    "text": "the feedback gathered through this piloting phase, we will revise the assessment list of these Guidelines by early 2020. The piloting phase will be launched by the summer of 2019 and last until the end of the year. All interested stakeholders will be able to participate by indicating their interest through the European AI Alliance. B. A FRAMEWORK FOR TRUSTWORTHY AI These Guidelines articulate a framework for achieving Trustworthy AI based on fundamental rights as enshrined in the Charter of Fundamental Rights of the European Union (EU Charter), and in relevant international human rights law. Below, we briefly touch upon Trustworthy AI s three components. Lawful AI AI systems do not operate in a lawless world. A number of legally binding rules at European, national and international level already apply or are relevant to the development, deployment and use of AI systems today. Legal sources include, but are not limited to: EU primary law (the Treaties of the European Union and its Charter of Fundamental Rights), EU secondary law (such as the General Data Protection Regulation, the Product Liability Directive, the Regulation on the Free Flow of Non-Personal Data, anti-discrimination Directives, consumer law and Safety and Health at Work Directives), the UN Human Rights treaties and the Council of Europe conventions (such as the European Convention on Human Rights), and numerous EU Member State laws. Besides horizontally applicable rules, various domain-specific rules exist that apply to particular AI applications (such as for instance the Medical Device Regulation in the healthcare sector). The law provides both positive and negative obligations, which means that it should not only be interpreted with reference to what cannot be done, but also with reference to what should be done and what may be done. The law not only prohibits certain actions but also enables others.",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      9
    ],
    "titles": [],
    "chunk_index": 11,
    "text": "In this regard, it can be noted that the EU Charter contains articles on the freedom to conduct a business and the freedom of the arts and sciences , alongside articles addressing areas that we are more familiar with when looking to ensure AI s trustworthiness, such as for instance data protection and non-discrimination. The Guidelines do not explicitly deal with the first component of Trustworthy AI (lawful AI), but instead aim to offer guidance on fostering and securing the second and third components (ethical and robust AI). While the two latter are to a certain extent often already reflected in existing laws, their full realisation may go beyond existing legal obligations. Nothing in this document shall be construed or interpreted as providing legal advice or guidance concerning how compliance with any applicable existing legal norms and requirements can be achieved. Nothing in this document shall create legal rights nor impose legal obligations towards third parties. We however recall that it is the duty of any natural or legal person to comply with laws whether applicable today or adopted in the future according to the development of AI. These Guidelines proceed on the assumption that all legal rights and obligations that apply to the processes and activities involved in developing, deploying and using AI systems remain mandatory and must be duly observed. Ethical AI Achieving Trustworthy AI requires not only compliance with the law, which is but one of its three components. Laws are not always up to speed with technological developments, can at times be out of step with ethical norms or may simply not be well suited to addressing certain issues. For AI systems to be trustworthy, they should hence also be ethical, ensuring alignment with ethical norms. Robust AI Even if an ethical purpose is ensured, individuals",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 12,
    "text": "and society must also be confident that AI systems will not cause any unintentional harm. Such systems should perform in a safe, secure and reliable manner, and safeguards should be foreseen to prevent any unintended adverse impacts. It is therefore important to ensure that AI systems are robust. This is needed both from a technical perspective (ensuring the system s technical robustness as appropriate in a given context, such as the application domain or life cycle phase), and from a social perspective (in due consideration of the context and environment in which the system operates). Ethical and robust AI are hence closely intertwined and complement each other. The principles put forward in Chapter I, and the requirements derived from these principles in Chapter II, address both components. The framework The Guidance in this document is provided in three chapters, from most abstract in Chapter I to most concrete in Chapter III: Chapter I Foundations of Trustworthy AI: sets out the foundations of Trustworthy AI by laying out its fundamental-rights12 based approach. It identifies and describes the ethical principles that must be adhered to in order to ensure ethical and robust AI. Chapter II Realising Trustworthy AI: translates these ethical principles into seven key requirements that AI systems should implement and meet throughout their entire life cycle. In addition, it offers both technical and non-technical methods that can be used for their implementation. Chapter III Assessing Trustworthy AI: sets out a concrete and non-exhaustive Trustworthy AI assessment list to operationalise the requirements of Chapter II, offering AI practitioners practical guidance. This assessment should be tailored to the particular system's application. The document s final section lists examples of beneficial opportunities and critical concerns raised by AI systems, which should serve to stimulate further debate. The Guidelines structure is illustrated in Figure",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      11
    ],
    "titles": [
      "I."
    ],
    "chunk_index": 13,
    "text": "1 below Fundamental rights lie at the foundation of both international and EU human rights law and underpin the legally enforceable rights guaranteed by the EU Treaties and the EU Charter. Being legally binding, compliance with fundamental rights hence falls under trustworthy AI's first component (lawful AI). Fundamental rights can however also be understood as reflecting special moral entitlements of all individuals arising by virtue of their humanity, regardless of their legally binding status. In that sense, they hence also form part of the second component of trustworthy AI (ethical AI). I. Chapter I: Foundations of Trustworthy AI This Chapter sets out the foundations of Trustworthy AI, grounded in fundamental rights and reflected by four ethical principles that should be adhered to in order to ensure ethical and robust AI. It draws heavily on the field of ethics. AI ethics is a sub-field of applied ethics, focusing on the ethical issues raised by the development, deployment and use of AI. Its central concern is to identify how AI can advance or raise concerns to the good life of individuals, whether in terms of quality of life, or human autonomy and freedom necessary for a democratic society. Ethical reflection on AI technology can serve multiple purposes. First, it can stimulate reflection on the need to protect individuals and groups at the most basic level. Second, it can stimulate new kinds of innovations that seek to foster ethical values, such as those helping to achieve the UN Sustainable Development Goals13, which are firmly embedded in the forthcoming EU Agenda 2030.14 While this document mostly concerns itself with the first purpose mentioned, the importance that ethics could have in the second should not be underestimated. Trustworthy AI can improve individual flourishing and collective wellbeing by generating prosperity, value creation and wealth maximization. It",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 14,
    "text": "can contribute to achieving a fair society, by helping to increase citizens health and well-being in ways that foster equality in the distribution of economic, social and political opportunity. It is therefore imperative that we understand how to best support AI development, deployment and use to ensure that everyone can thrive in an AI-based world, and to build a better future while at the same time being globally competitive. As with any powerful technology, the use of AI systems in our society raises several ethical challenges, for instance relating to their impact on people and society, decision-making capabilities and safety. If we are increasingly going to use the assistance of or delegate decisions to AI systems, we need to make sure these systems are fair in their impact on people s lives, that they are in line with values that should not be compromised and able to act accordingly, and that suitable accountability processes can ensure this. Europe needs to define what normative vision of an AI-immersed future it wants to realise, and understand which notion of AI should be studied, developed, deployed and used in Europe to achieve this vision. With this document, we intend to contribute to this effort by introducing the notion of Trustworthy AI, which we believe is the right way to build a future with AI. A future where democracy, the rule of law and fundamental rights underpin AI systems and where such systems continuously improve and defend democratic culture will also enable an environment where innovation and responsible competitiveness can thrive. A domain-specific ethics code however consistent, developed and fine-grained future versions of it may be can never function as a substitute for ethical reasoning itself, which must always remain sensitive to contextual details that cannot be captured in general Guidelines. Beyond developing a",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      12
    ],
    "titles": [
      "2. From fundamental rights to ethical principles"
    ],
    "chunk_index": 15,
    "text": "set of rules, ensuring Trustworthy AI requires us to build and maintain an ethical culture and mind-set through public debate, education and practical learning. 1. Fundamental rights as moral and legal entitlements We believe in an approach to AI ethics based on the fundamental rights enshrined in the EU Treaties,15 the EU Charter and international human rights law.16 Respect for fundamental rights, within a framework of democracy and the rule of law, provides the most promising foundations for identifying abstract ethical principles and values, which can be operationalised in the context of AI. The EU Treaties and the EU Charter prescribe a series of fundamental rights that EU member states and EU institutions are legally obliged to respect when implementing EU law. These rights are described in the EU Charter The EU is based on a constitutional commitment to protect the fundamental and indivisible rights of human beings, to ensure respect for the rule of law, to foster democratic freedom and promote the common good. These rights are reflected in Articles 2 and 3 of the Treaty on European Union, and in the Charter of Fundamental Rights of the EU. Other legal instruments reflect and provide further specification of these commitments, such as for instance the Council of Europe s European Social Charter or specific legislation such as the EU s General Data Protection Regulation. by reference to dignity, freedoms, equality and solidarity, citizens rights and justice. The common foundation that unites these rights can be understood as rooted in respect for human dignity thereby reflecting what we describe as a human-centric approach in which the human being enjoys a unique and inalienable moral status of primacy in the civil, political, economic and social fields.17 While the rights set out in the EU Charter are legally binding,18 it is important",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 16,
    "text": "to recognise that fundamental rights do not provide comprehensive legal protection in every case. For the EU Charter, for instance, it is important to underline that its field of application is limited to areas of EU law. International human rights law and in particular the European Convention on Human Rights are legally binding on EU Member States, including in areas that fall outside the scope of EU law. At the same time, fundamental rights are also bestowed on individuals and (to a certain degree) groups by virtue of their moral status as human beings, independently of their legal force. Understood as legally enforceable rights, fundamental rights therefore fall under the first component of Trustworthy AI (lawful AI), which safeguards compliance with the law. Understood as the rights of everyone, rooted in the inherent moral status of human beings, they also underpin the second component of Trustworthy AI (ethical AI), dealing with ethical norms that are not necessarily legally binding yet crucial to ensure trustworthiness. Since this document does not aim to offer guidance on the former component, for the purpose of these non-binding guidelines, references to fundamental rights reflect the latter component. 2. From fundamental rights to ethical principles 2.1 Fundamental rights as a basis for Trustworthy AI Among the comprehensive set of indivisible rights set out in international human rights law, the EU Treaties and the EU Charter, the below families of fundamental rights are particularly apt to cover AI systems. Many of these rights are, in specified circumstances, legally enforceable in the EU so that compliance with their terms is legally obligatory. But even after compliance with legally enforceable fundamental rights has been achieved, ethical reflection can help us understand how the development, deployment and use of AI systems may implicate fundamental rights and their underlying values, and",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 17,
    "text": "can help provide more fine-grained guidance when seeking to identify what we should do rather than what we (currently) can do with technology. Respect for human dignity. Human dignity encompasses the idea that every human being possesses an intrinsic worth , which should never be diminished, compromised or repressed by others nor by new technologies like AI systems.19 In this context, respect for human dignity entails that all people are treated with respect due to them as moral subjects, rather than merely as objects to be sifted, sorted, scored, herded, conditioned or manipulated. AI systems should hence be developed in a manner that respects, serves and protects humans physical and mental integrity, personal and cultural sense of identity, and satisfaction of their essential needs.20 Freedom of the individual. Human beings should remain free to make life decisions for themselves. This entails freedom from sovereign intrusion, but also requires intervention from government and non-governmental organisations to ensure that individuals or people at risk of exclusion have equal access to AI s benefits and opportunities. In an AI context, freedom of the individual for instance requires mitigation of (in)direct illegitimate coercion, threats to mental autonomy and mental health, unjustified surveillance, deception and unfair manipulation. In fact, freedom of the individual means a commitment to enabling individuals to wield even higher control over their lives, including (among other rights) protection of the freedom to conduct a business, the freedom of the arts and science, freedom of expression, the right to private life and privacy, and freedom of It should be noted that a commitment to human-centric AI and its anchoring in fundamental rights requires collective societal and constitutional foundations in which individual freedom and respect for human dignity is both practically possible and meaningful, rather than implying an unduly individualistic account of the",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      13
    ],
    "titles": [
      "2.2 Ethical Principles in the Context of AI Systems22"
    ],
    "chunk_index": 18,
    "text": "human. Pursuant to Article 51 of the Charter, it applies to EU Institutions and to EU member states when implementing EU law. C. McCrudden, Human Dignity and Judicial Interpretation of Human Rights, EJIL, 19(4), 2008. For an understanding of human dignity along these lines see E. Hilgendorf, Problem Areas in the Dignity Debate and the Ensemble Theory of Human Dignity, in: D. Grimm, A. Kemmerer, C. M llers (eds.), Human Dignity in Context. Explorations of a Contested Concept, 2018, pp. 325 ff. assembly and association. Respect for democracy, justice and the rule of law. All governmental power in constitutional democracies must be legally authorised and limited by law. AI systems should serve to maintain and foster democratic processes and respect the plurality of values and life choices of individuals. AI systems must not undermine democratic processes, human deliberation or democratic voting systems. AI systems must also embed a commitment to ensure that they do not operate in ways that undermine the foundational commitments upon which the rule of law is founded, mandatory laws and regulation, and to ensure due process and equality before the law. Equality, non-discrimination and solidarity - including the rights of persons at risk of exclusion. Equal respect for the moral worth and dignity of all human beings must be ensured. This goes beyond non-discrimination, which tolerates the drawing of distinctions between dissimilar situations based on objective justifications. In an AI context, equality entails that the system s operations cannot generate unfairly biased outputs (e.g. the data used to train AI systems should be as inclusive as possible, representing different population groups). This also requires adequate respect for potentially vulnerable persons and groups,21 such as workers, women, persons with disabilities, ethnic minorities, children, consumers or others at risk of exclusion. Citizens rights. Citizens benefit from a wide",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 19,
    "text": "array of rights, including the right to vote, the right to good administration or access to public documents, and the right to petition the administration. AI systems offer substantial potential to improve the scale and efficiency of government in the provision of public goods and services to society. At the same time, citizens rights could also be negatively impacted by AI systems and should be safeguarded. When the term citizens rights is used here, this is not to deny or neglect the rights of third-country nationals and irregular (or illegal) persons in the EU who also have rights under international law, and therefore in the area of AI systems. 2.2 Ethical Principles in the Context of AI Systems22 Many public, private, and civil organizations have drawn inspiration from fundamental rights to produce ethical frameworks for AI systems.23 In the EU, the European Group on Ethics in Science and New Technologies ( EGE ) proposed a set of 9 basic principles, based on the fundamental values laid down in the EU Treaties and Charter.24 We build further on this work, recognising most of the principles hitherto propounded by various groups, while clarifying the ends that all principles seek to nurture and support. These ethical principles can inspire new and specific regulatory instruments, can help interpreting fundamental rights as our socio-technical environment evolves over time, and can guide the rationale for AI systems development, deployment and use adapting dynamically as society itself evolves. AI systems should improve individual and collective wellbeing. This section lists four ethical principles, rooted in fundamental rights, which must be respected in order to ensure that AI systems are developed, deployed and used in a trustworthy manner. They are specified as ethical imperatives, such that AI practitioners should always strive to adhere to them. Without imposing a hierarchy,",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      14
    ],
    "titles": [],
    "chunk_index": 20,
    "text": "we list the principles here below in manner that mirrors the order of appearance of the fundamental rights upon which they are based in the EU Charter.25 For a description of the term as used throughout this document, see the Glossary. These principles also apply to the development, deployment and use of other technologies, and hence are not specific to AI systems. In what follows, we have aimed to set out their relevance specifically in an AI-related context. Reliance on fundamental rights also helps to limit regulatory uncertainty as it can build on the basis of decades of practice of fundamental rights protection in the EU, thereby offering clarity, readability and foreseeability. More recently, the AI4People s taskforce has surveyed the aforementioned EGE principles as well as 36 other ethical principles put forward to date and subsumed them under four overarching principles: L. Floridi, J. Cowls, M. Beltrametti, R. Chatila, P. Chazerand, V. Dignum, C. Luetge, R. Madelin, U. Pagallo, F. Rossi, B. Schafer, P. Valcke, E. J. M. Vayena (2018), \"AI4People An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations , Minds and Machines 28(4): 689-707. Respect for human autonomy is strongly associated with the right to human dignity and liberty (reflected in Articles 1 and 6 of the Charter). The prevention of harm is strongly linked to the protection of physical or mental integrity (reflected in Article 3). These are the principles of: (i) Respect for human autonomy (ii) Prevention of harm (iii) Fairness (iv) Explicability Many of these are to a large extent already reflected in existing legal requirements for which mandatory compliance is required and hence also fall within the scope of lawful AI, which is Trustworthy AI s first component.26 Yet, as set out above, while many legal obligations reflect ethical",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 21,
    "text": "principles, adherence to ethical principles goes beyond formal compliance with existing laws.27 The principle of respect for human autonomy The fundamental rights upon which the EU is founded are directed towards ensuring respect for the freedom and autonomy of human beings. Humans interacting with AI systems must be able to keep full and effective self- determination over themselves, and be able to partake in the democratic process. AI systems should not unjustifiably subordinate, coerce, deceive, manipulate, condition or herd humans. Instead, they should be designed to augment, complement and empower human cognitive, social and cultural skills. The allocation of functions between humans and AI systems should follow human-centric design principles and leave meaningful opportunity for human choice. This means securing human oversight28 over work processes in AI systems. AI systems may also fundamentally change the work sphere. It should support humans in the working environment, and aim for the creation of meaningful work. The principle of prevention of harm AI systems should neither cause nor exacerbate harm29 or otherwise adversely affect human beings.30 This entails the protection of human dignity as well as mental and physical integrity. AI systems and the environments in which they operate must be safe and secure. They must be technically robust and it should be ensured that they are not open to malicious use. Vulnerable persons should receive greater attention and be included in the development, deployment and use of AI systems. Particular attention must also be paid to situations where AI systems can cause or exacerbate adverse impacts due to asymmetries of power or information, such as between employers and employees, businesses and consumers or governments and citizens. Preventing harm also entails consideration of the natural environment and all living beings. The principle of fairness The development, deployment and use of AI systems must",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      15
    ],
    "titles": [
      "2.3 Tensions between the principles"
    ],
    "chunk_index": 22,
    "text": "be fair. While we acknowledge that there are many different interpretations of fairness, we believe that fairness has both a substantive and a procedural dimension. The substantive dimension implies a commitment to: ensuring equal and just distribution of both benefits and costs, and ensuring that individuals and groups are free from unfair bias, discrimination and stigmatisation. If unfair biases can be avoided, AI systems could even increase societal fairness. Equal opportunity in terms of access to education, goods, services and technology should also be fostered. Moreover, the use of AI systems should never lead to people being deceived or unjustifiably impaired in their freedom of choice. Additionally, fairness implies that AI practitioners should respect the principle of proportionality between means and ends, and consider carefully how to Fairness is closely linked to the rights to Non-discrimination, Solidarity and Justice (reflected in Articles 21 and following). Explicability and Responsibility are closely linked to the rights relating to Justice (as reflected in Article 47). Think for instance of the GDPR or EU consumer protection regulations. For further reading on this subject, see for instance L. Floridi, Soft Ethics and the Governance of the Digital, Philosophy & Technology, March 2018, Volume 31, Issue 1, pp 1 8. The concept of human oversight is further developed as one of the key requirements set out in Chapter II here below. Harms can be individual or collective, and can include intangible harm to social, cultural and political environments. This also encompasses the way of living of individuals and social groups, avoiding for instance cultural harm. balance competing interests and objectives.31 The procedural dimension of fairness entails the ability to contest and seek effective redress against decisions made by AI systems and by the humans operating them.32 In order to do so, the entity accountable for the",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 23,
    "text": "decision must be identifiable, and the decision-making processes should be explicable. The principle of explicability Explicability is crucial for building and maintaining users trust in AI systems. This means that processes need to be transparent, the capabilities and purpose of AI systems openly communicated, and decisions to the extent possible explainable to those directly and indirectly affected. Without such information, a decision cannot be duly contested. An explanation as to why a model has generated a particular output or decision (and what combination of input factors contributed to that) is not always possible. These cases are referred to as black box algorithms and require special attention. In those circumstances, other explicability measures (e.g. traceability, auditability and transparent communication on system capabilities) may be required, provided that the system as a whole respects fundamental rights. The degree to which explicability is needed is highly dependent on the context and the severity of the consequences if that output is erroneous or otherwise inaccurate.33 2.3 Tensions between the principles Tensions may arise between the above principles, for which there is no fixed solution. In line with the EU fundamental commitment to democratic engagement, due process and open political participation, methods of accountable deliberation to deal with such tensions should be established. For instance, in various application domains, the principle of prevention of harm and the principle of human autonomy may be in conflict. Consider as an example the use of AI systems for predictive policing , which may help to reduce crime, but in ways that entail surveillance activities that impinge on individual liberty and privacy. Furthermore, AI systems overall benefits should substantially exceed the foreseeable individual risks. While the above principles certainly offer guidance towards solutions, they remain abstract ethical prescriptions. AI practitioners can hence not be expected to find the right",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 24,
    "text": "solution based on the principles above, yet they should approach ethical dilemmas and trade-offs via reasoned, evidence-based reflection rather than intuition or random discretion. There may be situations, however, where no ethically acceptable trade-offs can be identified. Certain fundamental rights and correlated principles are absolute and cannot be subject to a balancing exercise (e.g. human dignity). Key guidance derived from Chapter I: Develop, deploy and use AI systems in a way that adheres to the ethical principles of: respect for human autonomy, prevention of harm, fairness and explicability. Acknowledge and address the potential tensions between these principles. Pay particular attention to situations involving more vulnerable groups such as children, persons with disabilities and others that have historically been disadvantaged or are at risk of exclusion, and to situations which are characterised by asymmetries of power or information, such as between employers and workers, or between businesses and consumers.34 This is relates to the principle of proportionality (reflected in the maxim that one should not use a sledge hammer to crack a nut ). Measures taken to achieve an end (e.g. the data extraction measures implemented to realise the AI optimisation function) should be limited to what is strictly necessary. It also entails that when several measures compete for the satisfaction of an end, preference should be given to the one that is least adverse to fundamental rights and ethical norms (e.g. AI developers should always prefer public sector data to personal data). Reference can also be made to the proportionality between user and deployer, considering the rights of companies (including intellectual property and confidentiality) on the one hand, and the rights of the user on the other. Including by using their right of association and to join a trade union in a working environment, as provided for by Article 12",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      16
    ],
    "titles": [
      "II."
    ],
    "chunk_index": 25,
    "text": "of the EU Charter of fundamental rights. For example, little ethical concern may flow from inaccurate shopping recommendations generated by an AI system, in contrast to AI systems that evaluate whether an individual convicted of a criminal offence should be released on parole. See articles 24 to 27 of the EU Charter, dealing with the rights of the child and the elderly, the integration of persons with disabilities and workers rights. See also article 38 dealing with consumer protection. Acknowledge that, while bringing substantial benefits to individuals and society, AI systems also pose certain risks and may have a negative impact, including impacts which may be difficult to anticipate, identify or measure (e.g. on democracy, the rule of law and distributive justice, or on the human mind itself.) Adopt adequate measures to mitigate these risks when appropriate, and proportionately to the magnitude of the risk. II. Chapter II: Realising Trustworthy AI This Chapter offers guidance on the implementation and realisation of Trustworthy AI, via a list of seven requirements that should be met, building on the principles outlined in Chapter I. In addition, available technical and non-technical methods are introduced for the implementation of these requirements throughout the AI system s life cycle. 1. Requirements of Trustworthy AI The principles outlined in Chapter I must be translated into concrete requirements to achieve Trustworthy AI. These requirements are applicable to different stakeholders partaking in AI systems life cycle: developers, deployers and end-users, as well as the broader society. By developers, we refer to those who research, design and/or develop AI systems. By deployers, we refer to public or private organisations that use AI systems within their business processes and to offer products and services to others. End-users are those engaging with the AI system, directly or indirectly. Finally, the broader society",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      17
    ],
    "titles": [
      "1.1 Human agency and oversight"
    ],
    "chunk_index": 26,
    "text": "encompasses all others that are directly or indirectly affected by AI systems. Different groups of stakeholders have different roles to play in ensuring that the requirements are met: a. Developers should implement and apply the requirements to design and development processes; b. Deployers should ensure that the systems they use and the products and services they offer meet the requirements; c. End-users and the broader society should be informed about these requirements and able to request that they are upheld. The below list of requirements is non-exhaustive.35 It includes systemic, individual and societal aspects: Human agency and oversight Including fundamental rights, human agency and human oversight Technical robustness and safety Including resilience to attack and security, fall back plan and general safety, accuracy, reliability and reproducibility Privacy and data governance Including respect for privacy, quality and integrity of data, and access to data Transparency Including traceability, explainability and communication Diversity, non-discrimination and fairness Including the avoidance of unfair bias, accessibility and universal design, and stakeholder participation Societal and environmental wellbeing Including sustainability and environmental friendliness, social impact, society and democracy Accountability Including auditability, minimisation and reporting of negative impact, trade-offs and redress. Without imposing a hierarchy, we list the principles here below in manner that mirrors the order of appearance of the principles and rights to which they relate in the EU Charter. Figure 2: Interrelationship of the seven requirements: all are of equal importance, support each other, and should be implemented and evaluated throughout the AI system s lifecycle While all requirements are of equal importance, context and potential tensions between them will need to be taken into account when applying them across different domains and industries. Implementation of these requirements should occur throughout an AI system s entire life cycle and depends on the specific application. While most",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      18
    ],
    "titles": [
      "1.2 Technical robustness and safety"
    ],
    "chunk_index": 27,
    "text": "requirements apply to all AI systems, special attention is given to those directly or indirectly affecting individuals. Therefore, for some applications (for instance in industrial settings), they may be of lesser relevance. The above requirements include elements that are in some cases already reflected in existing laws. We reiterate that in line with Trustworthy AI s first component it is the responsibility of AI practitioners to ensure that they comply with their legal obligations, both as regards horizontally applicable rules as well as domain-specific regulation. In the following paragraphs, each requirement is explained in more detail. 1.1 Human agency and oversight AI systems should support human autonomy and decision-making, as prescribed by the principle of respect for human autonomy. This requires that AI systems should both act as enablers to a democratic, flourishing and equitable society by supporting the user s agency and foster fundamental rights, and allow for human oversight. Fundamental rights. Like many technologies, AI systems can equally enable and hamper fundamental rights. They can benefit people for instance by helping them track their personal data, or by increasing the accessibility of education, hence supporting their right to education. However, given the reach and capacity of AI systems, they can also negatively affect fundamental rights. In situations where such risks exist, a fundamental rights impact assessment should be undertaken. This should be done prior to the system s development and include an evaluation of whether those risks can be reduced or justified as necessary in a democratic society in order to respect the rights and freedoms of others. Moreover, mechanisms should be put into place to receive external feedback regarding AI systems that potentially infringe on fundamental rights. Human agency. Users should be able to make informed autonomous decisions regarding AI systems. They should be given the knowledge",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 28,
    "text": "and tools to comprehend and interact with AI systems to a satisfactory degree and, where possible, be enabled to reasonably self-assess or challenge the system. AI systems should support individuals in making better, more informed choices in accordance with their goals. AI systems can sometimes be deployed to shape and influence human behaviour through mechanisms that may be difficult to detect, since they may harness sub-conscious processes, including various forms of unfair manipulation, deception, herding and conditioning, all of which may threaten individual autonomy. The overall principle of user autonomy must be central to the system s functionality. Key to this is the right not to be subject to a decision based solely on automated processing when this produces legal effects on users or similarly significantly affects them.36 Human oversight. Human oversight helps ensuring that an AI system does not undermine human autonomy or causes other adverse effects. Oversight may be achieved through governance mechanisms such as a human-in-the- loop (HITL), human-on-the-loop (HOTL), or human-in-command (HIC) approach. HITL refers to the capability for human intervention in every decision cycle of the system, which in many cases is neither possible nor desirable. HOTL refers to the capability for human intervention during the design cycle of the system and monitoring the system s operation. HIC refers to the capability to oversee the overall activity of the AI system (including its broader economic, societal, legal and ethical impact) and the ability to decide when and how to use the system in any particular situation. This can include the decision not to use an AI system in a particular situation, to establish levels of human discretion during the use of the system, or to ensure the ability to override a decision made by a system. Moreover, it must be ensured that public enforcers have",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 29,
    "text": "the ability to exercise oversight in line with their mandate. Oversight mechanisms can be required in varying degrees to support other safety and control measures, depending on the AI system s application area and potential risk. All other things being equal, the less oversight a human can exercise over an AI system, the more extensive testing and stricter governance is required. 1.2 Technical robustness and safety A crucial component of achieving Trustworthy AI is technical robustness, which is closely linked to the principle of prevention of harm. Technical robustness requires that AI systems be developed with a preventative approach to risks and in a manner such that they reliably behave as intended while minimising unintentional and unexpected harm, and preventing unacceptable harm. This should also apply to potential changes in their operating environment or the presence of other agents (human and artificial) that may interact with the system in an adversarial manner. In addition, the physical and mental integrity of humans should be ensured. Resilience to attack and security. AI systems, like all software systems, should be protected against vulnerabilities that can allow them to be exploited by adversaries, e.g. hacking. Attacks may target the data (data poisoning), the model (model leakage) or the underlying infrastructure, both software and hardware. If an AI system is attacked, e.g. in adversarial attacks, the data as well as system behaviour can be changed, leading the system to make different decisions, or causing it to shut down altogether. Systems and data can also become corrupted by malicious intention or by exposure to unexpected situations. Insufficient security processes can also result in erroneous decisions or even physical harm. For AI systems to be considered secure,37 possible unintended applications of the AI system (e.g. dual-use applications) and potential abuse of the system by malicious actors should",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      19
    ],
    "titles": [
      "1.3 Privacy and data governance"
    ],
    "chunk_index": 30,
    "text": "be taken into account, and steps should be taken to prevent and mitigate these.38 Fallback plan and general safety. AI systems should have safeguards that enable a fallback plan in case of problems. Reference can be made to Article 22 of the GDPR where this right is already enshrined. See e.g. considerations under 2.7 of the European Union s Coordinated Plan on Artificial Intelligence. There may be a strong imperative to develop a virtuous circle in research and development between understanding of attacks, development of adequate protection, and improvement of evaluation methodologies. To achieve this, convergence between the AI community and the security community should be promoted. In addition, it is the responsibility of all relevant actors to create common cross-border safety and security norms and to establish an environment of mutual trust, fostering international collaboration. For possible measures, see Malicious Use of AI, Avin S., Brundage M. et. al., 2018. This can mean that AI systems switch from a statistical to rule-based procedure, or that they ask for a human operator before continuing their action.39 It must be ensured that the system will do what it is supposed to do without harming living beings or the environment. This includes the minimisation of unintended consequences and errors. In addition, processes to clarify and assess potential risks associated with the use of AI systems, across various application areas, should be established. The level of safety measures required depends on the magnitude of the risk posed by an AI system, which in turn depends on the system s capabilities. Where it can be foreseen that the development process or the system itself will pose particularly high risks, it is crucial for safety measures to be developed and tested proactively. Accuracy. Accuracy pertains to an AI system s ability to make correct judgements,",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 31,
    "text": "for example to correctly classify information into the proper categories, or its ability to make correct predictions, recommendations, or decisions based on data or models. An explicit and well-formed development and evaluation process can support, mitigate and correct unintended risks from inaccurate predictions. When occasional inaccurate predictions cannot be avoided, it is important that the system can indicate how likely these errors are. A high level of accuracy is especially crucial in situations where the AI system directly affects human lives. Reliability and Reproducibility. It is critical that the results of AI systems are reproducible, as well as reliable. A reliable AI system is one that works properly with a range of inputs and in a range of situations. This is needed to scrutinise an AI system and to prevent unintended harms. Reproducibility describes whether an AI experiment exhibits the same behaviour when repeated under the same conditions. This enables scientists and policy makers to accurately describe what AI systems do. Replication files40 can facilitate the process of testing and reproducing behaviours. 1.3 Privacy and data governance Closely linked to the principle of prevention of harm is privacy, a fundamental right particularly affected by AI systems. Prevention of harm to privacy also necessitates adequate data governance that covers the quality and integrity of the data used, its relevance in light of the domain in which the AI systems will be deployed, its access protocols and the capability to process data in a manner that protects privacy. Privacy and data protection. AI systems must guarantee privacy and data protection throughout a system s entire lifecycle.41 This includes the information initially provided by the user, as well as the information generated about the user over the course of their interaction with the system (e.g. outputs that the AI system generated for specific",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      20
    ],
    "titles": [
      "1.4 Transparency"
    ],
    "chunk_index": 32,
    "text": "users or how users responded to particular recommendations). Digital records of human behaviour may allow AI systems to infer not only individuals preferences, but also their sexual orientation, age, gender, religious or political views. To allow individuals to trust the data gathering process, it must be ensured that data collected about them will not be used to unlawfully or unfairly discriminate against them. Quality and integrity of data. The quality of the data sets used is paramount to the performance of AI systems. When data is gathered, it may contain socially constructed biases, inaccuracies, errors and mistakes. This needs to be addressed prior to training with any given data set. In addition, the integrity of the data must be ensured. Feeding malicious data into an AI system may change its behaviour, particularly with self-learning systems. Processes and data sets used must be tested and documented at each step such as planning, training, testing and deployment. This should also apply to AI systems that were not developed in-house but acquired elsewhere. Access to data. In any given organisation that handles individuals data (whether someone is a user of the system or not), data protocols governing data access should be put in place. These protocols should outline who can access data and under which circumstances. Only duly qualified personnel with the competence and need to access individual s data should be allowed to do so. Scenarios where human intervention would not immediately be possible should also be considered. This concerns files that will replicate each step of the AI system s development process, from research and initial data collection to the results. Reference can be made to existing privacy laws, such as the GDPR or the forthcoming ePrivacy Regulation. 1.4 Transparency This requirement is closely linked with the principle of explicability and",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 33,
    "text": "encompasses transparency of elements relevant to an AI system: the data, the system and the business models. Traceability. The data sets and the processes that yield the AI system s decision, including those of data gathering and data labelling as well as the algorithms used, should be documented to the best possible standard to allow for traceability and an increase in transparency. This also applies to the decisions made by the AI system. This enables identification of the reasons why an AI-decision was erroneous which, in turn, could help prevent future mistakes. Traceability facilitates auditability as well as explainability. Explainability. Explainability concerns the ability to explain both the technical processes of an AI system and the related human decisions (e.g. application areas of a system). Technical explainability requires that the decisions made by an AI system can be understood and traced by human beings. Moreover, trade-offs might have to be made between enhancing a system's explainability (which may reduce its accuracy) or increasing its accuracy (at the cost of explainability). Whenever an AI system has a significant impact on people s lives, it should be possible to demand a suitable explanation of the AI system s decision-making process. Such explanation should be timely and adapted to the expertise of the stakeholder concerned (e.g. layperson, regulator or researcher). In addition, explanations of the degree to which an AI system influences and shapes the organisational decision-making process, design choices of the system, and the rationale for deploying it, should be available (hence ensuring business model transparency). Communication. AI systems should not represent themselves as humans to users; humans have the right to be informed that they are interacting with an AI system. This entails that AI systems must be identifiable as such. In addition, the option to decide against this interaction in",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 34,
    "text": "favour of human interaction should be provided where needed to ensure compliance with fundamental rights. Beyond this, the AI system s capabilities and limitations should be communicated to AI practitioners or end-users in a manner appropriate to the use case at hand. This could encompass communication of the AI system's level of accuracy, as well as its limitations. 1.5 Diversity, non-discrimination and fairness In order to achieve Trustworthy AI, we must enable inclusion and diversity throughout the entire AI system s life cycle. Besides the consideration and involvement of all affected stakeholders throughout the process, this also entails ensuring equal access through inclusive design processes as well as equal treatment. This requirement is closely linked with the principle of fairness. Avoidance of unfair bias. Data sets used by AI systems (both for training and operation) may suffer from the inclusion of inadvertent historic bias, incompleteness and bad governance models. The continuation of such biases could lead to unintended (in)direct prejudice and discrimination42 against certain groups or people, potentially exacerbating prejudice and marginalisation. Harm can also result from the intentional exploitation of (consumer) biases or by engaging in unfair competition, such as the homogenisation of prices by means of collusion or a non- transparent market.43 Identifiable and discriminatory bias should be removed in the collection phase where possible. The way in which AI systems are developed (e.g. algorithms programming) may also suffer from unfair bias. This could be counteracted by putting in place oversight processes to analyse and address the system s purpose, constraints, requirements and decisions in a clear and transparent manner. Moreover, hiring from diverse backgrounds, cultures and disciplines can ensure diversity of opinions and should be encouraged. Accessibility and universal design. Particularly in business-to-consumer domains, systems should be user-centric and designed in a way that allows all people",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      21
    ],
    "titles": [
      "1.6 Societal and environmental well-being"
    ],
    "chunk_index": 35,
    "text": "to use AI products or services, regardless of their age, gender, abilities or characteristics. Accessibility to this technology for persons with disabilities, which are present in all societal For a definition of direct and indirect discrimination, see for instance Article 2 of Council Directive 2000/78/EC of 27 November 2000 establishing a general framework for equal treatment in employment and occupation. See also Article 21 of the Charter of Fundamental Rights of the EU. See the EU Agency for Fundamental Rights paper: BigData: Discrimination in data-supported decision making , 2018, groups, is of particular importance. AI systems should not have a one-size-fits-all approach and should consider Universal Design44 principles addressing the widest possible range of users, following relevant accessibility standards.45 This will enable equitable access and active participation of all people in existing and emerging computer-mediated human activities and with regard to assistive technologies.46 Stakeholder Participation. In order to develop AI systems that are trustworthy, it is advisable to consult stakeholders who may directly or indirectly be affected by the system throughout its life cycle. It is beneficial to solicit regular feedback even after deployment and set up longer term mechanisms for stakeholder participation, for example by ensuring workers information, consultation and participation throughout the whole process of implementing AI systems at organisations. 1.6 Societal and environmental well-being In line with the principles of fairness and prevention of harm, the broader society, other sentient beings and the environment should be also considered as stakeholders throughout the AI system s life cycle. Sustainability and ecological responsibility of AI systems should be encouraged, and research should be fostered into AI solutions addressing areas of global concern, such as for instance the Sustainable Development Goals. Ideally, AI systems should be used to benefit all human beings, including future generations. Sustainable and environmentally friendly AI.",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 36,
    "text": "AI systems promise to help tackling some of the most pressing societal concerns, yet it must be ensured that this occurs in the most environmentally friendly way possible. The system s development, deployment and use process, as well as its entire supply chain, should be assessed in this regard, e.g. via a critical examination of the resource usage and energy consumption during training, opting for less harmful choices. Measures securing the environmental friendliness of AI systems entire supply chain should be encouraged. Social impact. Ubiquitous exposure to social AI systems47 in all areas of our lives (be it in education, work, care or entertainment) may alter our conception of social agency, or impact our social relationships and attachment. While AI systems can be used to enhance social skills,48 they can equally contribute to their deterioration. This could also affect people s physical and mental wellbeing. The effects of these systems must therefore be carefully monitored and considered. Society and Democracy. Beyond assessing the impact of an AI system s development, deployment and use on individuals, this impact should also be assessed from a societal perspective, taking into account its effect on institutions, democracy and society at large. The use of AI systems should be given careful consideration particularly in situations relating to the democratic process, including not only political decision-making but also electoral contexts. 1.7 Accountability The requirement of accountability complements the above requirements, and is closely linked to the principle of fairness. It necessitates that mechanisms be put in place to ensure responsibility and accountability for AI systems and their outcomes, both before and after their development, deployment and use. Auditability. Auditability entails the enablement of the assessment of algorithms, data and design processes. This does not necessarily imply that information about business models and intellectual property related to",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      22
    ],
    "titles": [
      "2. Technical and non-technical methods to realise Trustworthy AI"
    ],
    "chunk_index": 37,
    "text": "the AI Article 42 of the Public Procurement Directive requires technical specifications to consider accessibility and design for all . For instance EN 301 549. This requirement links to the United Nations Convention on the Rights of Persons with Disabilities. This denotes AI systems communicating and interacting with humans by simulating sociality in human robot interaction (embodied AI) or as avatars in virtual reality. By doing so, those systems have the potential to change our socio-cultural practices and the fabric of our social life. See for instance the EU-funded project developing AI-based software that enables robots to interact more effectively with autistic children in human-led therapy sessions, helping to improve their social and communication skills: em=Infocentre&artid=49968 system must always be openly available. Evaluation by internal and external auditors, and the availability of such evaluation reports, can contribute to the trustworthiness of the technology. In applications affecting fundamental rights, including safety-critical applications, AI systems should be able to be independently audited. Minimisation and reporting of negative impacts. Both the ability to report on actions or decisions that contribute to a certain system outcome, and to respond to the consequences of such an outcome, must be ensured. Identifying, assessing, documenting and minimising the potential negative impacts of AI systems is especially crucial for those (in)directly affected. Due protection must be available for whistle-blowers, NGOs, trade unions or other entities when reporting legitimate concerns about an AI system. The use of impact assessments (e.g. red teaming or forms of Algorithmic Impact Assessment) both prior to and during the development, deployment and use of AI systems can be helpful to minimise negative impact. These assessments must be proportionate to the risk that the AI systems pose. Trade-offs. When implementing the above requirements, tensions may arise between them, which may lead to inevitable trade-offs. Such",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 38,
    "text": "trade-offs should be addressed in a rational and methodological manner within the state of the art. This entails that relevant interests and values implicated by the AI system should be identified and that, if conflict arises, trade-offs should be explicitly acknowledged and evaluated in terms of their risk to ethical principles, including fundamental rights. In situations in which no ethically acceptable trade-offs can be identified, the development, deployment and use of the AI system should not proceed in that form. Any decision about which trade-off to make should be reasoned and properly documented. The decision-maker must be accountable for the manner in which the appropriate trade-off is being made, and should continually review the appropriateness of the resulting decision to ensure that necessary changes can be made to the system where needed.49 Redress. When unjust adverse impact occurs, accessible mechanisms should be foreseen that ensure adequate redress.50 Knowing that redress is possible when things go wrong is key to ensure trust. Particular attention should be paid to vulnerable persons or groups. 2. Technical and non-technical methods to realise Trustworthy AI To implement the above requirements, both technical and non-technical methods can be employed. These encompass all stages of an AI system s life cycle. An evaluation of the methods employed to implement the requirements, as well as reporting and justifying51 changes to the implementation processes, should occur on an ongoing basis. AI systems are continuously evolving and acting in a dynamic environment. The realisation of Trustworthy AI is therefore a continuous process, as depicted in Figure 3 here below. Figure 3: Realising Trustworthy AI throughout the system s entire life cycle Different governance models can help achieving this. E.g. the presence of an internal and/or external ethical (and sector specific) expert or board might be useful to highlight areas of",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      23
    ],
    "titles": [
      "2.1. Technical methods"
    ],
    "chunk_index": 39,
    "text": "potential conflict and suggest ways in which that conflict might best be resolved. Meaningful consultation and discussion with stakeholders, including those at risk of being adversely affected by an AI system is useful too. European universities should take a leading role in training the ethics experts needed. See also the European Union Agency for Fundamental Rights' Opinion on Improving access to remedy in the area of business and This entails e.g. justification of the choices in the system s design, development and deployment to implement the requirements. The following methods can be either complementary or alternative to each other, since different requirements and different sensitivities may raise the need for different methods of implementation. This overview is neither meant to be comprehensive or exhaustive, nor mandatory. Rather, its aim is to offer a list of suggested methods that may help to implement Trustworthy AI. 2.1. Technical methods This section describes technical methods to ensure Trustworthy AI that can be incorporated in the design, development and use phases of an AI system. The methods listed below vary in level of maturity.52 Architectures for Trustworthy AI Requirements for Trustworthy AI should be translated into procedures and/or constraints on procedures, which should be anchored in the AI system s architecture. This could be accomplished through a set of white list rules (behaviours or states) that the system should always follow, black list restrictions on behaviours or states that the system should never transgress, and mixtures of those or more complex provable guarantees regarding the system s behaviour. Monitoring of the system s compliance with these restrictions during operations may be achieved by a separate process. AI systems with learning capabilities that can dynamically adapt their behaviour can be understood as non- deterministic systems possibly exhibiting unexpected behaviour. These are often considered through the",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 40,
    "text": "theoretical lens of a sense-plan-act cycle. Adapting this architecture to ensure Trustworthy AI requires the requirements integration at all three steps of the cycle: (i) at the sense -step, the system should be developed such that it recognises all environmental elements necessary to ensure adherence to the requirements; (ii) at the plan -step, the system should only consider plans that adhere to the requirements; (iii) at the act -step, the system s actions should be restricted to behaviours that realise the requirements. The architecture as sketched above is generic and only provides an imperfect description for most AI systems. Nevertheless, it gives anchor points for constraints and policies that should be reflected in specific modules to result in an overall system that is trustworthy and perceived as such. Ethics and rule of law by design (X-by-design) Methods to ensure values-by-design provide precise and explicit links between the abstract principles which the system is required to respect and the specific implementation decisions. The idea that compliance with norms can be implemented into the design of the AI system is key to this method. Companies are responsible for identifying the impact of their AI systems from the very start, as well as the norms their AI system ought to comply with to avert negative impacts. Different by-design concepts are already widely used, e.g. privacy-by-design and security- by-design. As indicated above, to earn trust AI needs to be secure in its processes, data and outcomes, and should be designed to be robust to adversarial data and attacks. It should implement a mechanism for fail-safe shutdown and enable resumed operation after a forced shut-down (such as an attack). Explanation methods For a system to be trustworthy, we must be able to understand why it behaved a certain way and why it provided a given",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      24
    ],
    "titles": [
      "2.2. Non-technical methods"
    ],
    "chunk_index": 41,
    "text": "interpretation. A whole field of research, Explainable AI (XAI) tries to address this issue to better understand the system s underlying mechanisms and find solutions. Today, this is still an open challenge for AI systems based on neural networks. Training processes with neural nets can result in network parameters set to numerical values that are difficult to correlate with results. Moreover, sometimes small changes in data values might result in dramatic changes in interpretation, leading the system to e.g. confuse a school bus with an ostrich. This vulnerability can also be exploited during attacks on the system. Methods involving XAI research are vital not only to explain the system s While some of these methods are already available today, others still require more research. Those areas where further research is needed will also inform the AI HLEG's second deliverable, i.e. the Policy and Investment Recommendations. behaviour to users, but also to deploy reliable technology. Testing and validating Due to the non-deterministic and context-specific nature of AI systems, traditional testing is not enough. Failures of the concepts and representations used by the system may only manifest when a programme is applied to sufficiently realistic data. Consequently, to verify and validate processing of data, the underlying model must be carefully monitored during both training and deployment for its stability, robustness and operation within well- understood and predictable bounds. It must be ensured that the outcome of the planning process is consistent with the input, and that the decisions are made in a way allowing validation of the underlying process. Testing and validation of the system should occur as early as possible, ensuring that the system behaves as intended throughout its entire life cycle and especially after deployment. It should include all components of an AI system, including data, pre-trained models, environments and",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 42,
    "text": "the behaviour of the system as a whole. The testing processes should be designed and performed by an as diverse group of people as possible. Multiple metrics should be developed to cover the categories that are being tested for different perspectives. Adversarial testing by trusted and diverse red teams deliberately attempting to break the system to find vulnerabilities, and bug bounties that incentivise outsiders to detect and responsibly report system errors and weaknesses, can be considered. Finally, it must be ensured that the outputs or actions are consistent with the results of the preceding processes, comparing them to the previously defined policies to ensure that they are not violated. Quality of Service Indicators Appropriate quality of service indicators can be defined for AI systems to ensure that there is a baseline understanding as to whether they have been tested and developed with security and safety considerations in mind. These indicators could include measures to evaluate the testing and training of algorithms as well as traditional software metrics of functionality, performance, usability, reliability, security and maintainability. 2.2. Non-technical methods This section describes a variety of non-technical methods that can serve a valuable role in securing and maintaining Trustworthy AI. These too should be evaluated on an ongoing basis. Regulation As mentioned above, regulation to support AI s trustworthiness already exists today think of product safety legislation and liability frameworks. To the extent we consider that regulation may need to be revised, adapted or introduced, both as a safeguard and as an enabler, this will be raised in our second deliverable, consisting of AI Policy and Investment Recommendations. Codes of conduct Organisations and stakeholders can sign up to the Guidelines and adapt their charter of corporate responsibility, Key Performance Indicators ( KPIs ), their codes of conduct or internal policy documents to",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      25
    ],
    "titles": [
      "IEEE"
    ],
    "chunk_index": 43,
    "text": "add the striving towards Trustworthy AI. An organisation working on or with AI systems can, more generally, document its intentions, as well as underwrite them with standards of certain desirable values such as fundamental rights, transparency and the avoidance of harm. Standardisation Standards, for example for design, manufacturing and business practices, can function as a quality management system for AI users, consumers, organisations, research institutions and governments by offering the ability to recognise and encourage ethical conduct through their purchasing decisions. Beyond conventional standards, co- regulatory approaches exist: accreditation systems, professional codes of ethics or standards for fundamental rights compliant design. Current examples are e.g. ISO Standards or the IEEE P7000 standards series, but in the future a possible Trustworthy AI' label might be suitable, confirming by reference to specific technical standards that the system, for instance, adheres to safety, technical robustness and transparency. Certification As it cannot be expected that everyone is able to fully understand the workings and effects of AI systems, consideration can be given to organisations that can attest to the broader public that an AI system is transparent, accountable and fair.53 These certifications would apply standards developed for different application domains and AI techniques, appropriately aligned with the industrial and societal standards of different contexts. Certification can however never replace responsibility. It should hence be complemented by accountability frameworks, including disclaimers as well as review and redress mechanisms.54 Accountability via governance frameworks Organisations should set up governance frameworks, both internal and external, ensuring accountability for the ethical dimensions of decisions associated with the development, deployment and use of AI systems. This can, for instance, include the appointment of a person in charge of ethics issues relating to AI systems, or an internal/external ethics panel or board. Amongst the possible roles of such a person, panel",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 44,
    "text": "or board, is to provide oversight and advice. As set out above, certification specifications and bodies can also play a role to this end. Communication channels should be ensured with industry and/or public oversight groups, sharing best practices, discussing dilemmas or reporting emerging issues of ethical concerns. Such mechanisms can complement but cannot replace legal oversight (e.g. in the form of the appointment of a data protection officer or equivalent measures, legally required under data protection law). Education and awareness to foster an ethical mind-set Trustworthy AI encourages the informed participation of all stakeholders. Communication, education and training play an important role, both to ensure that knowledge of the potential impact of AI systems is widespread, and to make people aware that they can participate in shaping the societal development. This includes all stakeholders, e.g. those involved in making the products (the designers and developers), the users (companies or individuals) and other impacted groups (those who may not purchase or use an AI system but for whom decisions are made by an AI system, and society at large). Basic AI literacy should be fostered across society. A prerequisite for educating the public is to ensure the proper skills and training of ethicists in this space. Stakeholder participation and social dialogue The benefits of AI systems are many, and Europe needs to ensure that they are available to all. This requires an open discussion and the involvement of social partners and stakeholders, including the general public. Many organisations already rely on stakeholder panels to discuss the use of AI systems and data analytics. These panels include various members, such as legal experts, technical experts, ethicists, consumer representatives and workers. Actively seeking participation and dialogue on the use and impact of AI systems supports the evaluation of results and approaches, and can",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      26
    ],
    "titles": [],
    "chunk_index": 45,
    "text": "particularly be helpful in complex cases. Diversity and inclusive design teams Diversity and inclusion play an essential role when developing AI systems that will be employed in the real world. It is critical that, as AI systems perform more tasks on their own, the teams that design, develop, test and maintain, deploy and procure these systems reflect the diversity of users and of society in general. This contributes to objectivity and consideration of different perspectives, needs and objectives. Ideally, teams are not only diverse in terms of gender, culture, age, but also in terms of professional backgrounds and skill sets. As advocated by e.g. the IEEE Ethically Aligned Design Initiative: connections/ec/autonomous-systems.html. Key guidance derived from Chapter II: Ensure that the AI system s entire life cycle meets the seven key requirements for Trustworthy AI: (1) human agency and oversight, (2) technical robustness and safety, (3) privacy and data governance, (4) transparency, (5) diversity, non-discrimination and fairness, (6) environmental and societal well-being and (7) accountability. Consider technical and non-technical methods to ensure the implementation of those requirements. Foster research and innovation to help assessing AI systems and to further the achievement of the requirements; disseminate results and open questions to the wider public, and systematically train a new generation of experts in AI ethics. Communicate, in a clear and proactive manner, information to stakeholders about the AI system s capabilities and limitations, enabling realistic expectation setting, and about the manner in which the requirements are implemented. Be transparent about the fact that they are dealing with an AI system. Facilitate the traceability and auditability of AI systems, particularly in critical contexts and situations. Involve stakeholders throughout the AI system s life cycle. Foster training and education so that all stakeholders are aware of and trained in Trustworthy AI. Be mindful that",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 46,
    "text": "there might be fundamental tensions between different principles and requirements. Continuously identify, evaluate, document and communicate these trade-offs and their solutions. III. Chapter III: Assessing Trustworthy AI Based on the key requirements of Chapter II, this Chapter sets out a non-exhaustive Trustworthy AI assessment list (pilot version) to operationalise Trustworthy AI. It particularly applies to AI systems that directly interact with users, and is primarily addressed to developers and deployers of AI systems (whether self-developed or acquired from third parties). This assessment list does not address the operationalisation of the first component of Trustworthy AI (lawful AI). Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level. The assessment list and governance structure will be developed in close collaboration with stakeholders across the public and private sector. The process will be driven as a piloting process, allowing for extensive feedback from two parallel processes: a) a qualitative process, ensuring representability, where a small selection of companies, organisations and institutions (from different sectors and of different sizes) will sign up to pilot the assessment list and the governance structure in practice and to provide in-depth feedback; b) a quantitative process where all interested stakeholders can sign up to pilot the assessment list and provide feedback through an open consultation. After the piloting phase, we will integrate the results from the feedback process into the assessment list and prepare a revised version in early 2020.",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      27
    ],
    "titles": [
      "HR"
    ],
    "chunk_index": 47,
    "text": "The aim is to achieve a framework that can be horizontally used across all applications and hence offer a foundation for ensuring Trustworthy AI in all domains. Once such foundation has been established, a sectorial or application-specific framework could be developed. Governance Stakeholders may wish to consider how the Trustworthy AI assessment list can be implemented in their organisation. This can be done by incorporating the assessment process into existing governance mechanisms, or by implementing new processes. This choice will depend on the internal structure of the organisation as well as its size and available resources. Research demonstrates that management attention at the highest level is essential to achieve change.55 It also demonstrates that involving all stakeholders in a company, organisation or institution fosters the acceptance and the relevance of the introduction of any new process (whether or not technological).56 Therefore, we recommend implementing a process that embraces both the involvement of operational level as well as top management level. Level Relevant roles (depending on the organisation) Management and Board Top management discusses and evaluates the AI systems development, deployment or procurement and serves as an escalation board for evaluating all AI innovations and uses, when critical concerns are detected. It involves those impacted by the possible introduction of AI systems (e.g. workers) and their representatives throughout the process via information, consultation and participation procedures. Compliance/Legal department/Corporate responsibility department The responsibility department monitors the use of the assessment list and its necessary evolution to meet the technological or regulatory changes. It updates the standards or internal policies on AI systems and ensures that the use of such systems complies with the current legal and regulatory framework and to the values of the organisation. Product and Service Development or equivalent The Product and Service Development department uses the assessment list to",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 48,
    "text": "evaluate AI-based products and services and logs all the results. These results are discussed at management level, which ultimately approves the new or revised AI-based applications. Quality Assurance The Quality Assurance department (or equivalent) ensures and checks the results of the assessment list and takes action to escalate an issue higher up if the result is not satisfactory or if unforeseen results are detected. HR The HR department ensures the right mix of competences and diversity of profiles for developers of AI systems. It ensures that the appropriate level of training is delivered on Trustworthy AI inside the organisation. Procurement The procurement department ensures that the process to procure AI-based products or services includes a check of Trustworthy AI. Day-to-day Operations Developers and project managers include the assessment list in their daily work and document the results and outcomes of the assessment. Using the Trustworthy AI assessment list When using the assessment list in practice, we recommend paying attention not only to the areas of concern but also to the questions that cannot be (easily) answered. One potential problem might be the lack of diversity of skills and competences in the team developing and testing the AI system, and therefore it might be necessary to involve other stakeholders inside or outside the organisation. It is strongly recommended to log all results both in technical terms and in management terms, ensuring that the problem solving can be understood at all levels in the governance structure. This assessment list is meant to guide AI practitioners to achieve Trustworthy AI. The assessment should be tailored to the specific use case in a proportionate way. During the piloting phase, specific sensitive areas might be revealed and the need for further specifications in such cases will be evaluated in the next steps. While this See",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      28
    ],
    "titles": [
      "TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION)"
    ],
    "chunk_index": 49,
    "text": "for instance A. Bryson, E. Barth and H. Dale-Olsen, The Effects of Organisational change on worker well-being and the moderating role of trade unions, ILRReview, 66(4), July 2013; Jirjahn, U. and Smith, S.C. (2006). What Factors Lead Management to Support or Oppose Employee Participation With and Without Works Councils? Hypotheses and Evidence from Germany s Industrial Relations, 45(4), 650 680; Michie, J. and Sheehan, M. (2003). Labour market deregulation, flexibility and innovation , Cambridge Journal of Economics, 27(1), 123 143. assessment list does not provide concrete answers to address the raised questions, it encourages reflection on how Trustworthy AI can be operationalised, and on the potential steps that should be taken in this regard. Relation to existing law and processes It is also important for AI practitioners to recognise that there are various existing laws mandating particular processes or prohibiting particular outcomes, which may overlap and coincide with some of the measures listed in the assessment list. For example, data protection law sets out a series of legal requirements that must be met by those engaged in the collection and processing of personal data. Yet, because Trustworthy AI also requires the ethical handling of data, internal procedures and policies aimed at securing compliance with data protection laws might also help to facilitate ethical data handling and can hence complement existing legal processes. Compliance with this assessment list is not, however, evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable laws. Moreover, many AI practitioners already have existing assessment tools and software development processes in place to ensure compliance also with non-legal standards. The below assessment should not necessarily be carried out as a stand-alone exercise, but can be incorporated into such existing practices. TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) 1. Human agency and oversight",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 50,
    "text": "Fundamental rights: Did you carry out a fundamental rights impact assessment where there could be a negative impact on fundamental rights? Did you identify and document potential trade-offs made between the different principles and rights? Does the AI system interact with decisions by human (end) users (e.g. recommended actions or decisions to take, presenting of options)? Could the AI system affect human autonomy by interfering with the (end) user s decision-making process in an unintended way? Did you consider whether the AI system should communicate to (end) users that a decision, content, advice or outcome is the result of an algorithmic decision? In case of a chat bot or other conversational system, are the human end users made aware that they are interacting with a non-human agent? Human agency: Is the AI system implemented in work and labour process? If so, did you consider the task allocation between the AI system and humans for meaningful interactions and appropriate human oversight and control? Does the AI system enhance or augment human capabilities? Did you take safeguards to prevent overconfidence in or overreliance on the AI system for work processes? Human oversight: Did you consider the appropriate level of human control for the particular AI system and use case? Can you describe the level of human control or involvement? Who is the human in control and what are the moments or tools for human intervention? Did you put in place mechanisms and measures to ensure human control or oversight? Did you take any measures to enable audit and to remedy issues related to governing AI autonomy? Is there is a self-learning or autonomous AI system or use case? If so, did you put in place more specific mechanisms of control and oversight? Which detection and response mechanisms did you establish to assess",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      29
    ],
    "titles": [
      "2. Technical robustness and safety"
    ],
    "chunk_index": 51,
    "text": "whether something could go wrong? Did you ensure a stop button or procedure to safely abort an operation where needed? Does this procedure abort the process entirely, in part, or delegate control to a human? 2. Technical robustness and safety Resilience to attack and security: Did you assess potential forms of attacks to which the AI system could be vulnerable? Did you consider different types and natures of vulnerabilities, such as data pollution, physical infrastructure, cyber-attacks? Did you put measures or systems in place to ensure the integrity and resilience of the AI system against potential attacks? Did you verify how your system behaves in unexpected situations and environments? Did you consider to what degree your system could be dual-use? If so, did you take suitable preventative measures against this case (including for instance not publishing the research or deploying the system)? Fallback plan and general safety: Did you ensure that your system has a sufficient fallback plan if it encounters adversarial attacks or other unexpected situations (for example technical switching procedures or asking for a human operator before proceeding)? Did you consider the level of risk raised by the AI system in this specific use case? Did you put any process in place to measure and assess risks and safety? Did you provide the necessary information in case of a risk for human physical integrity? Did you consider an insurance policy to deal with potential damage from the AI system? Did you identify potential safety risks of (other) foreseeable uses of the technology, including accidental or malicious misuse? Is there a plan to mitigate or manage these risks? Did you assess whether there is a probable chance that the AI system may cause damage or harm to users or third parties? Did you assess the likelihood, potential damage, impacted",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      30
    ],
    "titles": [
      "3. Privacy and data governance"
    ],
    "chunk_index": 52,
    "text": "audience and severity? Did you consider the liability and consumer protection rules, and take them into account? Did you consider the potential impact or safety risk to the environment or to animals? Did your risk analysis include whether security or network problems such as cybersecurity hazards could pose safety risks or damage due to unintentional behaviour of the AI system? Did you estimate the likely impact of a failure of your AI system when it provides wrong results, becomes unavailable, or provides societally unacceptable results (for example discrimination)? Did you define thresholds and did you put governance procedures in place to trigger alternative/fallback plans? Did you define and test fallback plans? Accuracy Did you assess what level and definition of accuracy would be required in the context of the AI system and use case? Did you assess how accuracy is measured and assured? Did you put in place measures to ensure that the data used is comprehensive and up to date? Did you put in place measures in place to assess whether there is a need for additional data, for example to improve accuracy or to eliminate bias? Did you verify what harm would be caused if the AI system makes inaccurate predictions? Did you put in place ways to measure whether your system is making an unacceptable amount of inaccurate predictions? Did you put in place a series of steps to increase the system's accuracy? Reliability and reproducibility: Did you put in place a strategy to monitor and test if the AI system is meeting the goals, purposes and intended applications? Did you test whether specific contexts or particular conditions need to be taken into account to ensure reproducibility? Did you put in place verification methods to measure and ensure different aspects of the system's reliability and reproducibility? Did",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 53,
    "text": "you put in place processes to describe when an AI system fails in certain types of settings? Did you clearly document and operationalise these processes for the testing and verification of the reliability of AI systems? Did you establish mechanisms of communication to assure (end-)users of the system s reliability? 3. Privacy and data governance Respect for privacy and data Protection: Depending on the use case, did you establish a mechanism allowing others to flag issues related to privacy or data protection in the AI system s processes of data collection (for training and operation) and data processing? Did you assess the type and scope of data in your data sets (for example whether they contain personal data)? Did you consider ways to develop the AI system or train the model without or with minimal use of potentially sensitive or personal data? Did you build in mechanisms for notice and control over personal data depending on the use case (such as valid consent and possibility to revoke, when applicable)? Did you take measures to enhance privacy, such as via encryption, anonymisation and aggregation? Where a Data Privacy Officer (DPO) exists, did you involve this person at an early stage in the process? Quality and integrity of data: Did you align your system with relevant standards (for example ISO, IEEE) or widely adopted protocols for daily data management and governance? Did you establish oversight mechanisms for data collection, storage, processing and use? Did you assess the extent to which you are in control of the quality of the external data sources used? Did you put in place processes to ensure the quality and integrity of your data? Did you consider other processes? How are you verifying that your data sets have not been compromised or hacked? Access to data: What protocols,",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      31
    ],
    "titles": [
      "5. Diversity, non-discrimination and fairness"
    ],
    "chunk_index": 54,
    "text": "processes and procedures did you follow to manage and ensure proper data governance? Did you assess who can access users data, and under what circumstances? Did you ensure that these persons are qualified and required to access the data, and that they have the necessary competences to understand the details of data protection policy? Did you ensure an oversight mechanism to log when, where, how, by whom and for what purpose data was accessed? 4. Transparency Traceability: Did you establish measures that can ensure traceability? This could entail documenting the following methods: Methods used for designing and developing the algorithmic system: o Rule-based AI systems: the method of programming or how the model was built; o Learning-based AI systems; the method of training the algorithm, including which input data was gathered and selected, and how this occurred. Methods used to test and validate the algorithmic system: o Rule-based AI systems; the scenarios or cases used in order to test and validate; o Learning-based model: information about the data used to test and validate. Outcomes of the algorithmic system: o The outcomes of or decisions taken by the algorithm, as well as potential other decisions that would result from different cases (for example, for other subgroups of users). Explainability: Did you assess: to what extent the decisions and hence the outcome made by the AI system can be understood? to what degree the system s decision influences the organisation s decision-making processes? why this particular system was deployed in this specific area? what the system s business model is (for example, how does it create value for the organisation)? Did you ensure an explanation as to why the system took a certain choice resulting in a certain outcome that all users can understand? Did you design the AI system with interpretability",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 55,
    "text": "in mind from the start? Did you research and try to use the simplest and most interpretable model possible for the application in question? Did you assess whether you can analyse your training and testing data? Can you change and update this over time? Did you assess whether you can examine interpretability after the model s training and development, or whether you have access to the internal workflow of the model? Communication: Did you communicate to (end-)users through a disclaimer or any other means that they are interacting with an AI system and not with another human? Did you label your AI system as such? Did you establish mechanisms to inform (end-)users on the reasons and criteria behind the AI system s outcomes? Did you communicate this clearly and intelligibly to the intended audience? Did you establish processes that consider users feedback and use this to adapt the system? Did you communicate around potential or perceived risks, such as bias? Depending on the use case, did you consider communication and transparency towards other audiences, third parties or the general public? Did you clarify the purpose of the AI system and who or what may benefit from the product/service? Did you specify usage scenarios for the product and clearly communicate these to ensure that it is understandable and appropriate for the intended audience? Depending on the use case, did you think about human psychology and potential limitations, such as risk of confusion, confirmation bias or cognitive fatigue? Did you clearly communicate characteristics, limitations and potential shortcomings of the AI system? In case of the system's development: to whoever is deploying it into a product or service? In case of the system's deployment: to the (end-)user or consumer? 5. Diversity, non-discrimination and fairness Unfair bias avoidance: Did you establish a strategy or",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      32
    ],
    "titles": [
      "6. Societal and environmental well-being"
    ],
    "chunk_index": 56,
    "text": "a set of procedures to avoid creating or reinforcing unfair bias in the AI system, both regarding the use of input data as well as for the algorithm design? Did you assess and acknowledge the possible limitations stemming from the composition of the used data sets? Did you consider diversity and representativeness of users in the data? Did you test for specific populations or problematic use cases? Did you research and use available technical tools to improve your understanding of the data, model and performance? Did you put in place processes to test and monitor for potential biases during the development, deployment and use phase of the system? Depending on the use case, did you ensure a mechanism that allows others to flag issues related to bias, discrimination or poor performance of the AI system? Did you establish clear steps and ways of communicating on how and to whom such issues can be raised? Did you consider others, potentially indirectly affected by the AI system, in addition to the (end)- users? Did you assess whether there is any possible decision variability that can occur under the same conditions? If so, did you consider what the possible causes of this could be? In case of variability, did you establish a measurement or assessment mechanism of the potential impact of such variability on fundamental rights? Did you ensure an adequate working definition of fairness that you apply in designing AI systems? Is your definition commonly used? Did you consider other definitions before choosing this one? Did you ensure a quantitative analysis or metrics to measure and test the applied definition of fairness? Did you establish mechanisms to ensure fairness in your AI systems? Did you consider other potential mechanisms? Accessibility and universal design: Did you ensure that the AI system accommodates a",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 57,
    "text": "wide range of individual preferences and abilities? Did you assess whether the AI system usable by those with special needs or disabilities or those at risk of exclusion? How was this designed into the system and how is it verified? Did you ensure that information about the AI system is accessible also to users of assistive technologies? Did you involve or consult this community during the development phase of the AI system? Did you take the impact of your AI system on the potential user audience into account? Did you assess whether the team involved in building the AI system is representative of your target user audience? Is it representative of the wider population, considering also of other groups who might tangentially be impacted? Did you assess whether there could be persons or groups who might be disproportionately affected by negative implications? Did you get feedback from other teams or groups that represent different backgrounds and experiences? Stakeholder participation: Did you consider a mechanism to include the participation of different stakeholders in the AI system s development and use? Did you pave the way for the introduction of the AI system in your organisation by informing and involving impacted workers and their representatives in advance? 6. Societal and environmental well-being Sustainable and environmentally friendly AI: Did you establish mechanisms to measure the environmental impact of the AI system s development, deployment and use (for example the type of energy used by the data centres)? Did you ensure measures to reduce the environmental impact of your AI system s life cycle? Social impact: In case the AI system interacts directly with humans: Did you assess whether the AI system encourages humans to develop attachment and empathy towards the system? Did you ensure that the AI system clearly signals that its social",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      33
    ],
    "titles": [
      "7. Accountability"
    ],
    "chunk_index": 58,
    "text": "interaction is simulated and that it has no capacities of understanding and feeling ? Did you ensure that the social impacts of the AI system are well understood? For example, did you assess whether there is a risk of job loss or de-skilling of the workforce? What steps have been taken to counteract such risks? Society and democracy: Did you assess the broader societal impact of the AI system s use beyond the individual (end-)user, such as potentially indirectly affected stakeholders? 7. Accountability Auditability: Did you establish mechanisms that facilitate the system s auditability, such as ensuring traceability and logging of the AI system s processes and outcomes? Did you ensure, in applications affecting fundamental rights (including safety-critical applications) that the AI system can be audited independently? Minimising and reporting negative Impact: Did you carry out a risk or impact assessment of the AI system, which takes into account different stakeholders that are (in)directly affected? Did you provide training and education to help developing accountability practices? Which workers or branches of the team are involved? Does it go beyond the development phase? Do these trainings also teach the potential legal framework applicable to the AI system? Did you consider establishing an ethical AI review board or a similar mechanism to discuss overall accountability and ethics practices, including potentially unclear grey areas? Did you foresee any kind of external guidance or put in place auditing processes to oversee ethics and accountability, in addition to internal initiatives? Did you establish processes for third parties (e.g. suppliers, consumers, distributors/vendors) or workers to report potential vulnerabilities, risks or biases in the AI system? Documenting trade-offs: Did you establish a mechanism to identify relevant interests and values implicated by the AI system and potential trade-offs between them? How do you decide on such trade-offs? Did",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      34
    ],
    "titles": [
      "C."
    ],
    "chunk_index": 59,
    "text": "you ensure that the trade-off decision was documented? Ability to redress: Did you establish an adequate set of mechanisms that allows for redress in case of the occurrence of any harm or adverse impact? Did you put mechanisms in place both to provide information to (end-)users/third parties about opportunities for redress? We invite all stakeholders to pilot this Assessment List in practice and to provide feedback on its implementability, completeness, relevance for the specific AI application or domain, as well as overlap or complementarity with existing compliance or assessment processes. Based on this feedback, a revised version of the Trustworthy AI assessment list will be proposed to the Commission in early 2020 Key guidance derived from Chapter III: Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems, and adapt it to the specific use case in which the system is being applied. Keep in mind that such assessment list will never be exhaustive. Ensuring Trustworthy AI is not about ticking boxes, but about continuously identifying requirements, evaluating solutions and ensuring improved outcomes throughout the AI system s lifecycle, and involving stakeholders therein. C. EXAMPLES OF OPPORTUNITIES AND CRITICAL CONCERNS RAISED BY AI In the following section, we provide examples of AI development and use that should be encouraged, as well as examples of where AI development, deployment or use can run counter to our values and may raise specific concerns. A balance must be struck between what should and what can be done with AI, and due care must be given to what should not be done with AI. 1. Examples of Trustworthy AI s opportunities Trustworthy AI can represent a great opportunity to support the mitigation of pressing challenges facing society such as an ageing population, growing social inequality and environmental pollution. This potential is",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 60,
    "text": "also reflected globally, such as with the UN Sustainable Development Goals.57 The following section looks at how to encourage a European AI strategy that tackles some of these challenges. Climate action and sustainable infrastructure While tackling climate change should be a top priority for policy-makers across the world, digital transformation and Trustworthy AI have a great potential to reduce humans impact on the environment and enable the efficient and effective use of energy and natural resources.58 Trustworthy AI can, for instance, be coupled to big data in order to detect energy needs more accurately, resulting in more efficient energy infrastructure and consumption.59 Looking at sectors like public transportation, AI systems for intelligent transport systems60 can be used to minimise queuing, optimise routing, allow vision impaired people to be more independent,61 optimise energy efficient engines and thereby enhance decarbonisation efforts and reduce the environmental footprint, for a greener society. Currently, worldwide, one human dies every 23 seconds in a car accident.62 AI systems could help to reduce the number fatalities significantly, for instance through better reaction times and better adherence to rules.63 Health and well-being Trustworthy AI technologies can be used and are already being used to render treatment smarter and more targeted, and to help preventing life-threatening diseases.64 Doctors and medical professionals can potentially perform a more accurate and detailed analysis of a patient s complex health data, even before people get sick, and provide tailored preventive treatment.65 In the context of Europe s ageing population, AI technologies and robotics can be valuable tools to assist caregivers, support elderly care,66 and monitor patients conditions on a real time A number of EU projects aim for the development of Smart Grids and Energy Storage, which have the potential to contribute to a successful digitally supported energy transition, including through AI-based and",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      35
    ],
    "titles": [
      "2."
    ],
    "chunk_index": 61,
    "text": "other digital solutions. To complement the work of those individual projects, the Commission has launched the BRIDGE initiative, allowing ongoing Horizon New AI-based solutions help prepare cities for the future of mobility. See for instance the EU funded project called Fabulos: See for instance the PRO4VIP project, which is part of the European Vision 2020 strategy to combat preventable blindness, especially due to old age. Mobility and orientation was one of the project's priority areas. The European UP-Drive project for instance aims to address the outlined transport-related challenges by providing contributions enabling gradual automation of and collaboration among vehicles , facilitating a safer, more inclusive and more affordable treatment/87958/, or the Murab project which conducts more accurate biopsies, and which aims at diagnosing cancer and other industry to develop smart AI and other ICT solutions that enable lifestyle interventions in the perioperative process. The target concerns new innovative eHealth solutions that can influence patients in a personalised way to take the necessary actions both prior and after surgery in their lifestyle to optimise the healthcare outcome. The EU-funded project CARESSES deals with robots for elderly care, focusing on their cultural sensitivity: they adapt their way of acting and speaking to match the culture and habits of the elderly person they are assisting: basis, thus saving lives.67 Trustworthy AI can also assist on a broader scale. For example, it can examine and identify general trends in the healthcare and treatment sector,68 leading to earlier detection of diseases, more efficient development of medicines, more targeted treatments69 and ultimately more lives saved. Quality education and digital transformation New technological, economic and environmental changes mean that society needs to become more proactive. Governments, industry leaders, educational institutions and unions face a responsibility to bring the citizens into the new digital era ensuring they have",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 62,
    "text": "the right skills to fill the future jobs. Trustworthy AI technologies could assist in more accurately forecasting which jobs and professions will be disrupted by technology, which new roles will be created and which skills will be needed. This could help governments, unions and industry with planning the (re)skilling of workers. It could also give citizens who may fear redundancy a path of development into a new role. In addition, AI can be a great tool to fight educational inequalities and create personalised and adaptable education programmes that could help everyone acquire new qualifications, skills and competences according to his or her own ability to learn.70 It could increase both the learning speed and the quality of education reaching from primary school to university. 2. Examples of critical concerns raised by AI A critical AI concern arises one of the components of Trustworthy AI is violated. Many of the concerns listed below will already fall within the scope of existing legal requirements, which are mandatory and must therefore be complied with. Yet even in circumstances where compliance with legal requirements has been demonstrated, these may not address the full range of ethical concerns that may arise. As our understanding of the adequacy of rules and ethical principles invariably evolves and may change over time, the following non-exhaustive list of concerns may be shortened, expanded, edited or updated in the future. Identifying and tracking individuals with AI AI enables the ever more efficient identification of individual persons by both public and private entities. Noteworthy examples of a scalable AI identification technology are face recognition and other involuntary methods of identification using biometric data (i.e. lie detection, personality assessment through micro expressions, and automatic voice detection). Identification of individuals is sometimes the desirable outcome, aligned with ethical principles (for example in detecting",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      36
    ],
    "titles": [],
    "chunk_index": 63,
    "text": "fraud, money laundering, or terrorist financing). However, automatic identification raises strong concerns of both a legal and ethical nature, as it may have an unexpected impact on many psychological and sociocultural levels. A proportionate use of control techniques in AI is needed to uphold the autonomy of European citizens. Clearly defining if, when and how AI can be used for automated identification of individuals and differentiating between the identification of an individual vs the tracing and tracking of an individual, Moreover, the EMPATTICS project (EMpowering PAtients for a BeTTer Information and improvement of the Communication Systems) will research and define how health care professionals and patients use ICT technologies including AI systems to plan interventions The research project launched an app and an online platform that collects, and gives access to, your digital long-term health- status information. This takes on the form of a life-long health companion ('avatar'). MyHealthAvatar also predicts your risk for stroke, diabetes, cardiovascular disease and hypertension. ageing population. An integrated platform for Ambient Assisted Living (AAL) and a mobile service robot for long-term monitoring and interaction will help the elderly to remain independent and active for longer. See for instance the use of AI by Sophia Genetics, which leverages statistical inference, pattern recognition and machine learning See for instance the MaTHiSiS project, aimed at providing a solution for affect-based learning in a comfortable learning Classroom or Century Tech s platform. and between targeted surveillance and mass surveillance, will be crucial for the achievement of Trustworthy AI. The application of such technologies must be clearly warranted in existing law.71 Where the legal basis for such activity is consent , practical means72 must be developed which allow meaningful and verified consent to be given to being automatically identified by AI or equivalent technologies. This also applies to the",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 64,
    "text": "usage of anonymous personal data that can be re-personalised. Covert AI systems Human beings should always know if they are directly interacting with another human being or a machine, and it is the responsibility of AI practitioners that this is reliably achieved. AI practitioners should therefore ensure that humans are made aware of or able to request and validate the fact that they interact with an AI system (for instance, by issuing clear and transparent disclaimers). Note that borderline cases exist and complicate the matter (e.g. an AI-filtered voice spoken by a human). It should be borne in mind that the confusion between humans and machines could have multiple consequences such as attachment, influence, or reduction of the value of being human.73 The development of human-like robots74 should therefore undergo careful ethical assessment. AI enabled citizen scoring in violation of fundamental rights Societies should strive to protect the freedom and autonomy of all citizens. Any form of citizen scoring can lead to the loss of this autonomy and endanger the principle of non-discrimination. Scoring should only be used if there is a clear justification, and where measures are proportionate and fair. Normative citizen scoring (general assessment of moral personality or ethical integrity ) in all aspects and on a large scale by public authorities or private actors endangers these values, especially when used not in accordance with fundamental rights, and when used disproportionately and without a delineated and communicated legitimate purpose. Today, citizen scoring on a large or smaller scale is already often used in purely descriptive and domain-specific scorings (e.g. school systems, e-learning, and driver licences). Even in those more narrow applications, a fully transparent procedure should be made available to citizens, including information on the process, purpose and methodology of the scoring. Note that transparency cannot prevent non-discrimination",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 65,
    "text": "or ensure fairness, and is not the panacea against the problem of scoring. Ideally the possibility of opting out of the scoring mechanism when possible without detriment should be provided otherwise mechanisms for challenging and rectifying the scores must be given. This is particularly important in situations where an asymmetry of power exists between the parties. Such opt-out options should be ensured in the technology s design in circumstances where this is necessary to ensure compliance with fundamental rights and is necessary in a democratic society. Lethal autonomous weapon systems (LAWS) Currently, an unknown number of countries and industries are researching and developing lethal autonomous weapon systems, ranging from missiles capable of selective targeting to learning machines with cognitive skills to decide whom, when and where to fight without human intervention. This raises fundamental ethical concerns, such as the fact that it could lead to an uncontrollable arms race on a historically unprecedented level, and create military contexts in which human control is almost entirely relinquished and the risks of malfunction are not addressed. The European Parliament has called for the urgent development of a common, legally binding position addressing ethical and legal questions of human control, oversight, accountability and implementation of international human rights law, international humanitarian law and military strategies.75 Recalling the European Union s aim to promote peace as enshrined in Article 3 of the Treaty of the European Union, we stand with, and look to support, the Parliament s resolution of 12 September 2018 and all related efforts on LAWS. In this regard, Article 6 of the GDPR can be recalled, which provides, among other things, that processing of data shall only be lawful if it has a valid legal basis. As current mechanisms for giving informed consent in the internet show, consumers typically give consent",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [
      37
    ],
    "titles": [
      "D. CONCLUSION"
    ],
    "chunk_index": 66,
    "text": "without meaningful consideration. Hence, they can hardly be classified as practical. Madary & Metzinger (2016). Real Virtuality: A Code of Ethical Conduct. Recommendations for Good Scientific Practice and the Consumers of VR-Technology. Frontiers in Robotics and AI, 3(3). This also applies to AI-driven avatars. European Parliament s Resolution 2018/2752(RSP). Potential longer-term concerns AI development is still domain-specific and requires well-trained human scientists and engineers to precisely specify its targets. However, extrapolating into the future with a longer time horizon, certain critical long-term concerns can be hypothesized.76 A risk-based approach suggests that these concerns should be kept into consideration in view of possible unknown unknowns and black swans. 77 The high-impact nature of these concerns, combined with the current uncertainty in corresponding developments, calls for regular assessments of these topics. D. CONCLUSION This document constitutes the AI Ethics Guidelines produced by the High-Level Expert Group on Artificial Intelligence (AI HLEG). We recognise the positive impact that AI systems already have and will continue having, both commercially and societally. However, we are equally concerned to ensure that the risks and other adverse impacts with which these technologies are associated are properly and proportionately handled. AI is a technology that is both transformative and disruptive, and its evolution over the last several years has been facilitated by the availability of enormous amounts of digital data, major technological advances in computational power and storage capacity, as well as significant scientific and engineering innovation in AI methods and tools. AI systems will continue to impact society and citizens in ways that we cannot yet imagine. In this context, it is important to build AI systems that are worthy of trust, since human beings will only be able to confidently and fully reap its benefits when the technology, including the processes and people behind the technology,",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 67,
    "text": "are trustworthy. When drafting these Guidelines, Trustworthy AI has, therefore, been our foundational ambition. Trustworthy AI has three components: (1) it should be lawful, ensuring compliance with all applicable laws and regulations, (2) it should be ethical, ensuring adherence to ethical principles and values and (3) it should be robust, both from a technical and social perspective since to ensure that, even with good intentions, AI systems do not cause any unintentional harm. Each component is necessary but not sufficient to achieve Trustworthy AI. Ideally, all three components work in harmony and overlap in their operation. Where tensions arise, we should endeavour to align them. In Chapter I, we articulated the fundamental rights and a corresponding set of ethical principles that are crucial in an AI-context. In Chapter II, we listed seven key requirements that AI systems should meet in order to realise Trustworthy AI. We proposed technical and non-technical methods that can help with their implementation. Finally, in Chapter III we provided a Trustworthy AI assessment list that can help operationalising the seven requirements. In a final section, we provided examples of beneficial opportunities and critical concerns raised by AI systems, on which we hope to stimulate further discussion. Europe has a unique vantage point based on its focus on placing the citizen at the heart of its endeavours. This focus is written into the very DNA of the European Union through the Treaties upon which it is built. The current document forms part of a vision that promotes Trustworthy AI which we believe should be the foundation upon which Europe can build leadership in innovative, cutting-edge AI systems. This ambitious vision will help securing human flourishing of European citizens, both individually and collectively. Our goal is to create a culture of Trustworthy AI for Europe , whereby the",
    "n_words": 300
  },
  {
    "pdf": "ai_hleg_ethics_guidelines.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 68,
    "text": "benefits of AI can be reaped by all in a manner that ensures respect for our foundational values: fundamental rights, democracy and the rule of law. While some consider that Artificial General Intelligence, Artificial Consciousness, Artificial Moral Agents, Super-intelligence or Transformative AI can be examples of such long-term concerns (currently non-existent), many others believe these to be unrealistic. A black swan event is a very rare, yet high impact, event so rare, that it might not have been observed. Hence, probability of occurrence typically can only be estimated with high uncertainty.",
    "n_words": 91
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      5
    ],
    "titles": [
      "1.0 Privacy Framework Introduction"
    ],
    "chunk_index": 1,
    "text": "January 16, 2020 1.0 Privacy Framework Introduction For more than two decades, the Internet and associated information technologies have driven unprecedented innovation, economic value, and access to social services. Many of these benefits are fueled by data about individuals that flow through a complex ecosystem. As a result, individuals may not be able to understand the potential consequences for their privacy as they interact with systems, products, and services. Organizations may not fully realize the consequences either. Failure to manage privacy risks can have direct adverse consequences at both the individual and societal levels, with follow-on effects on organizations brands, bottom lines, and future prospects for growth. Finding ways to continue to derive benefits from data processing while simultaneously protecting individuals privacy is challenging, and not well-suited to one-size-fits-all solutions. Privacy is challenging because not only is it an all-encompassing concept that helps to safeguard important values such as human autonomy and dignity, but also the means for achieving it can vary.3 For example, privacy can be achieved through seclusion, limiting observation, or individuals control of facets of their identities (e.g., body, data, reputation).4 Moreover, human autonomy and dignity are not fixed, quantifiable constructs; they are filtered through cultural diversity and individual differences. This broad and shifting nature of privacy makes it difficult to communicate clearly about privacy risks within and between organizations and with individuals. What has been missing is a common language and practical tool that is flexible enough to address diverse privacy needs. This voluntary NIST Privacy Framework: A Tool for Improving Privacy through Enterprise Risk Management (Privacy Framework) is intended to be widely usable by organizations of all sizes and agnostic to any particular technology, sector, law, or jurisdiction. Using a common approach adaptable to any organization s role(s) in the data processing ecosystem the Privacy",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      6
    ],
    "titles": [
      "CURRENT"
    ],
    "chunk_index": 2,
    "text": "Framework s purpose is to help organizations manage privacy risks by: Taking privacy into account as they design and deploy systems, products, and services that affect individuals; Communicating about their privacy practices; and Encouraging cross-organizational workforce collaboration for example, among executives, legal, and information technology (IT) through the development of Profiles, selection of Tiers, and achievement of outcomes. Autonomy and dignity are concepts covered in the United Nations Universal Declaration of Human Rights at There are many publications that provide an in-depth treatment on the background of privacy or different aspects of the concept. For two examples, see Solove D (2010) Understanding Privacy (Harvard University Privacy, Spaces for the Future: A Companion to Philosophy of Technology, eds Pitt J, Shew A (Taylor & Francis, January 16, 2020 Overview of the Privacy Framework As shown in Figure 1, the Privacy Framework is composed of three parts: Core, Profiles, and Implementation Tiers. Each component reinforces how organizations manage privacy risk through the connection between business or mission drivers, organizational roles and responsibilities, and privacy protection activities. As further explained in section 2: The Core is a set of privacy protection activities and outcomes that allows for communicating prioritized privacy protection activities and outcomes across an organization from the executive level to the implementation/operations level. The Core is further divided into key Categories and Subcategories which are discrete outcomes for each Function. A Profile represents an organization s current privacy activities or desired outcomes. To develop a Profile, an organization can review all of the outcomes and activities in the Core to determine which are most important to focus on based on business or mission drivers, data processing ecosystem role(s), types of data processing, and individuals privacy needs. An organization can create or add Functions, Categories, and Subcategories as needed. Profiles can be",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      7
    ],
    "titles": [
      "1.2.1  Cybersecurity and Privacy Risk Management"
    ],
    "chunk_index": 3,
    "text": "used to identify opportunities for improving privacy posture by comparing a Current Profile (the as is state) with a Target Profile (the to be state). Profiles can be used to conduct self- assessments and to communicate within an organization or between organizations about how privacy risks are being managed. Implementation Tiers ( Tiers ) provide a point of reference on how an organization views privacy risk and whether it has sufficient processes and resources in place to manage that risk. Tiers reflect a progression from informal, reactive responses to approaches that are agile and risk informed. When selecting Tiers, an organization should consider its Target Profile(s) and how achievement may be supported or hampered by its current risk management practices, the degree of integration of privacy risk into its enterprise risk management portfolio, its data processing ecosystem relationships, and its workforce composition and training program. Privacy Risk Management While some organizations have a robust grasp of privacy risk management, a common understanding of many aspects of this topic is still not widespread.5 To promote broader understanding, this section covers concepts and considerations that organizations may use to develop, improve, or communicate See Summary Analysis of the Responses to the NIST Privacy Framework Request for Information [2] at p. 7. The Core provides an increasingly granular set of activities and outcomes that enable an organizational dialogue about managing privacy risk Profiles are a selection of specific Functions, Categories, and Subcategories from the Core that an organization has prioritized to help it manage privacy risk CURRENT TARGET Implementation Tiers support communication about whether an organization has sufficient processes and resources in place to manage privacy risk and achieve its Target Profile Figure 1: Core, Profiles, and Implementation Tiers January 16, 2020 about privacy risk management. Appendix D provides additional information on key",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 4,
    "text": "privacy risk management practices. 1.2.1 Cybersecurity and Privacy Risk Management Since its release in 2014, the Cybersecurity Framework has helped organizations to communicate and manage cybersecurity risk. [1] While managing cybersecurity risk contributes to managing privacy risk, it is not sufficient, as privacy risks can also arise by means unrelated to cybersecurity incidents, as illustrated by Figure 2. Having a general understanding of the different origins of cybersecurity and privacy risks is important for determining the most effective solutions to address the risks. The Privacy Framework approach to privacy risk is to consider privacy events as potential problems individuals could experience arising from system, product, or service operations with data, whether in digital or non-digital form, through a complete life cycle from data collection through disposal. The Privacy Framework describes these data operations in the singular as a data action and collectively as data processing. The problems individuals can experience as a result of data processing can be expressed in various ways, but NIST describes them as ranging from dignity-type effects such as embarrassment or stigmas to more tangible harms such as discrimination, economic loss, or physical harm.6 The basis for the problems that individuals may experience can vary. As depicted in Figure 2, problems arise as an adverse effect of data processing that organizations conduct to meet their mission or business objectives. An example is the concerns that certain communities had about the installation of smart meters as part of the Smart Grid, a nationwide technological effort to increase energy efficiency.7 The ability of these meters to collect, record, and distribute highly granular information about household electrical use could provide insight into people s behavior inside their homes.8 The meters were NIST has created an illustrative catalog of problems for use in privacy risk assessment. See NIST Privacy Risk",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      8
    ],
    "titles": [
      "1.2.2 Privacy Risk Assessment"
    ],
    "chunk_index": 5,
    "text": "Assessment Methodology [3]. Other organizations may have created other categories of problems, or may refer to them as adverse consequences or harms. See, for example, NIST Interagency or Internal Report (IR) 7628 Revision 1 Volume 1, Guidelines for Smart Grid Cybersecurity: Volume 1 Smart Grid Cybersecurity Strategy, Architecture, and High-Level Requirements at [4] p. 26. See NIST IR 8062, An Introduction to Privacy Engineering and Risk Management in Federal Systems at [5] p. 2. For additional types of privacy risks associated with adverse effects on individuals of data processing, see Appendix E of NIST IR 8062. Data Action A data life cycle operation, including, but not limited to collection, retention, logging, generation, transformation, use, disclosure, sharing, transmission, and disposal. Data Processing The collective set of data actions. Figure 2: Cybersecurity and Privacy Risk Relationship Cybersecurity Risks Privacy Risks associated with cybersecurity incidents arising from loss of confidentiality, integrity, or availability associated with privacy events arising from data processing cyber security- related privacy events January 16, 2020 operating as intended, but the data processing could lead to people feeling surveilled. In an increasingly connected world, some problems can arise simply from individuals interactions with systems, products, and services, even when the data being processed is not directly linked to identifiable individuals. For example, smart cities technologies could be used to alter or influence people s behavior such as where or how they move through the city.9 Problems also can arise where there is a loss of confidentiality, integrity, or availability at some point in the data processing, such as data theft by external attackers or the unauthorized access or use of data by employees. Figure 2 shows these types of cybersecurity-related privacy events as the overlap between privacy and cybersecurity risks. Once an organization can identify the likelihood of any given",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 6,
    "text": "problem arising from the data processing, which the Privacy Framework refers to as a problematic data action, it can assess the impact should the problematic data action occur. This impact assessment is where privacy risk and organizational risk intersect. Individuals, whether singly or in groups (including at a societal level) experience the direct impact of problems. As a result of the problems individuals experience, an organization may experience impacts such as noncompliance costs, revenue loss arising from customer abandonment of products and services, or harm to its external brand reputation or internal culture. Organizations commonly manage these types of impacts at the enterprise risk management level; by connecting problems that individuals experience to these well-understood organizational impacts, organizations can bring privacy risk into parity with other risks they are managing in their broader portfolio and drive more informed decision- making about resource allocation to strengthen privacy programs. Figure 3 illustrates this relationship between privacy risk and organizational risk. 1.2.2 Privacy Risk Assessment Privacy risk management is a cross-organizational set of processes that helps organizations to understand how their systems, products, and services may create problems for individuals and how to develop effective solutions to manage such risks. Privacy risk assessment is a sub-process for identifying and evaluating specific privacy risks. In general, privacy risk assessments produce the information that can help organizations to weigh the benefits of the data processing against the risks and to determine the appropriate response sometimes referred to as proportionality.10 Organizations may choose to See Newcombe T (2016) Security, Privacy, Governance Concerns About Smart City Technologies Grow. Smart-City-Technologies-Grow.html. 10 See European Data Protection Supervisor (2019) Necessity & Proportionality. Available at Problem arises from data processing Individual experiences direct impact (e.g., embarrassment, discrimination, economic loss) Organization resulting impact (e.g., customer abandonment, noncompliance costs, harm to reputation or",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      9
    ],
    "titles": [
      "11  See NIST Special Publication (SP) 800-39, Managing Information Security Risk: Organization, Mission, and"
    ],
    "chunk_index": 7,
    "text": "internal culture) Figure 3: Relationship Between Privacy Risk and Organizational Risk January 16, 2020 prioritize and respond to privacy risk in different ways, depending on the potential impact to individuals and resulting impacts to organizations. Response approaches include:11 Mitigating the risk (e.g., organizations may be able to apply technical and/or policy measures to the systems, products, or services that minimize the risk to an acceptable degree); Transferring or sharing the risk (e.g., contracts are a means of sharing or transferring risk to other organizations, privacy notices and consent mechanisms are a means of sharing risk with individuals); Avoiding the risk (e.g., organizations may determine that the risks outweigh the benefits, and forego or terminate the data processing); or Accepting the risk (e.g., organizations may determine that problems for individuals are minimal or unlikely to occur, therefore the benefits outweigh the risks, and it is not necessary to invest resources in mitigation). Privacy risk assessments are particularly important because, as noted above, privacy is a condition that safeguards multiple values. The methods for safeguarding these values may differ, and moreover, may be in tension with each other. Depending on its objectives, if an organization is trying to achieve privacy by limiting observation, this may lead to implementing measures such as distributed data architectures or privacy-enhancing cryptographic techniques that hide data even from the organization. If an organization is also trying to enable individual control, the measures could conflict. For example, if an individual requests access to data, the organization may not be able to produce the data if the data have been distributed or encrypted in ways the organization cannot access. Privacy risk assessments can help an organization understand in a given context the values to protect, the methods to employ, and how to balance implementation of different types of measures.",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      10
    ],
    "titles": [
      "2.0 Privacy Framework Basics"
    ],
    "chunk_index": 8,
    "text": "Lastly, privacy risk assessments help organizations distinguish between privacy risk and compliance risk. Identifying if data processing could create problems for individuals, even when an organization may be fully compliant with applicable laws or regulations, can help with ethical decision-making in system, product, and service design or deployment. Although there is no objective standard for ethical decision- making, it is grounded in the norms, values, and legal expectations in a given society. This facilitates optimizing beneficial uses of data while minimizing adverse consequences for individuals privacy and society as a whole, as well as avoiding losses of trust that damage organizations reputations, slow adoption, or cause abandonment of products and services. See Appendix D for more information on the operational aspects of privacy risk assessment. Document Overview The remainder of this document contains the following sections and appendices: Section 2 describes the Privacy Framework components: Core, Profiles, and Implementation Tiers. Section 3 presents examples of how the Privacy Framework can be used. The References section lists the references for the document. 11 See NIST Special Publication (SP) 800-39, Managing Information Security Risk: Organization, Mission, and Information System View [6]. January 16, 2020 Appendix A presents the Privacy Framework Core in a tabular format: Functions, Categories, and Subcategories. Appendix B contains a glossary of selected terms. Appendix C lists acronyms used in this document. Appendix D considers key practices that contribute to successful privacy risk management. Appendix E defines the Implementation Tiers. 2.0 Privacy Framework Basics The Privacy Framework provides a common language for understanding, managing, and communicating privacy risk with internal and external stakeholders. It is adaptable to any organization s role(s) in the data processing ecosystem. It can be used to help identify and prioritize actions for reducing privacy risk, and it is a tool for aligning policy, business,",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      11
    ],
    "titles": [
      "IDENTIFY-P"
    ],
    "chunk_index": 9,
    "text": "and technological approaches to managing that risk. Core Set forth in Appendix A, the Core provides an increasingly granular set of activities and outcomes that enable a dialogue about managing privacy risk. As depicted in Figure 4, the Core comprises Functions, Categories, and Subcategories. The Core elements work together: Functions organize foundational privacy activities at their highest level. They aid an organization in expressing its management of privacy risk by understanding and managing data processing, enabling risk management decisions, determining how to interact with individuals, and improving by learning from previous activities. They are not intended to form a serial path or lead to a static desired end state. Rather, the Functions should be performed concurrently and continuously to form or enhance an operational culture that addresses the dynamic nature of privacy risk. Categories are the subdivisions of a Function into groups of privacy outcomes closely tied to programmatic needs and particular activities. Subcategories further divide a Category into specific outcomes of technical and/or management activities. They provide a set of results that, while not exhaustive, help support achievement of the outcomes in each Category. The five Functions, Identify-P, Govern-P, Control-P, Communicate-P, and Protect-P, defined below, can be used to manage privacy risks arising from data processing.12 Protect-P is specifically focused on managing risks associated with cybersecurity-related privacy events (e.g., privacy breaches). The Cybersecurity Framework, although intended to cover all types of cybersecurity incidents, can be 12 The -P at the end of each Function name indicates that it is from the Privacy Framework in order to avoid confusion with Cybersecurity Framework Functions. CATEGORIES SUBCATEGORIES FUNCTIONS Identify-P Govern-P Protect-P Control-P Communicate-P Figure 4: Privacy Framework Core Structure January 16, 2020 leveraged to further support the management of risks associated with cybersecurity-related privacy events by using the Detect, Respond, and Recover",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 10,
    "text": "Functions. Alternatively, organizations may use all five of the Cybersecurity Framework Functions in conjunction with Identify-P, Govern-P, Control-P, and Communicate-P to collectively address privacy and cybersecurity risks. Figure 5 uses the Venn diagram from section 1.2.1 to demonstrate how the Functions from both frameworks can be used in varying combinations to manage different aspects of privacy and cybersecurity risks. The five Privacy Framework Functions are defined as follows: Identify-P Develop the organizational understanding to manage privacy risk for individuals arising from data processing. The activities in the Identify-P Function are foundational for effective use of the Privacy Framework. Inventorying the circumstances under which data are processed, understanding the privacy interests of individuals directly or indirectly served or affected by an organization, and conducting risk assessments enable an organization to understand the business environment in which it is operating and identify and prioritize privacy risks. Govern-P Develop and implement the organizational governance structure to enable an ongoing understanding of the organization s risk management priorities that are informed by privacy risk. The Govern-P Function is similarly foundational, but focuses on organizational-level activities such as establishing organizational privacy values and policies, identifying legal/regulatory requirements, and understanding organizational risk tolerance that enable an organization to focus and prioritize its efforts, consistent with its risk management strategy and business needs. Control-P Develop and implement appropriate activities to enable organizations or individuals to manage data with sufficient granularity to manage privacy risks. The Control-P Function considers data processing management from the standpoint of both organizations and individuals. Communicate-P Develop and implement appropriate activities to enable organizations and individuals to have a reliable understanding and engage in a dialogue about how data are processed and associated privacy risks. The Communicate-P Function recognizes that both organizations and individuals may need to know how data are processed in",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      12
    ],
    "titles": [
      "PROFILES"
    ],
    "chunk_index": 11,
    "text": "order to manage privacy risk effectively. Protect-P Develop and implement appropriate data processing safeguards. The Protect-P Function covers data protection to prevent cybersecurity-related privacy events, the overlap between privacy and cybersecurity risk management. Figure 5: Using Functions to Manage Cybersecurity and Privacy Risks Cybersecurity Risks Privacy Risks Cybersecurity- related privacy events IDENTIFY-P GOVERN-P CONTROL-P COMMUNICATE-P PROTECT-P RECOVER DETECT RESPOND IDENTIFY PROTECT RECOVER DETECT RESPOND January 16, 2020 Profiles Profiles are a selection of specific Functions, Categories, and Subcategories from the Core that an organization has prioritized to help it manage privacy risk. Profiles can be used to describe the current state and the desired target state of specific privacy activities. A Current Profile indicates privacy outcomes that an organization is currently achieving, while a Target Profile indicates the outcomes needed to achieve the desired privacy risk management goals. The differences between the two Profiles enable an organization to identify gaps, develop an action plan for improvement, and gauge the resources that would be needed (e.g., staffing, funding) to achieve privacy outcomes. This forms the basis of an organization s plan for reducing privacy risk in a cost-effective, prioritized manner. Profiles also can aid in communicating risk within and between organizations by helping organizations understand and compare the current and desired state of privacy outcomes. The Privacy Framework does not prescribe Profile templates to allow for flexibility in implementation. Under the Privacy Framework s risk- based approach, organizations may not need to achieve every outcome or activity reflected in the Core. When developing a Profile, an organization may select or tailor the Functions, Categories, and Subcategories to its specific needs, including developing its own additional Functions, Categories, and Subcategories to account for unique organizational risks. An organization determines these needs by considering its mission or business objectives, privacy values, and risk",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      13
    ],
    "titles": [
      "3.0 How to Use the Privacy Framework"
    ],
    "chunk_index": 12,
    "text": "tolerance; role(s) in the data processing ecosystem or industry sector; legal/regulatory requirements and industry best practices; risk management priorities and resources; and the privacy needs of individuals who are directly or indirectly served or affected by an organization s systems, products, or services. As illustrated in Figure 6, there is no specified order of development of Profiles. An organization may first develop a Target Profile in order to focus on its desired outcomes for privacy and then develop a Current Profile to identify gaps; alternatively, an organization may begin by identifying its current activities, and then consider how to adjust these activities for its Target Profile. An organization may choose to develop multiple Profiles for different roles, systems, products, or services, or categories of individuals (e.g., employees, customers) to enable better prioritization of activities and outcomes where there may be differing degrees of privacy risk. Organizations in a certain industry sector or with similar roles in the data processing ecosystem may coordinate to develop common Profiles. Implementation Tiers Tiers support organizational decision-making about how to manage privacy risk by taking into account the nature of the privacy risks engendered by an organization s systems, products, or services and the sufficiency of the processes and resources an organization has in place to manage such risks. When selecting Tiers, an organization should consider its Target Profile(s) and how achievement may be supported or hampered by its current risk management practices, the degree of integration of privacy PROFILES TARGET CORE Identify-P Govern-P Protect-P Control-P Communicate-P Identify-P Govern-P Protect-P Identify-P Govern-P Protect-P Control-P Communicate-P CURRENT Figure 6: Relationship Between Core and Profiles January 16, 2020 risk into its enterprise risk management portfolio, its data processing ecosystem relationships, and its workforce composition and training program. There are four distinct Tiers, Partial (Tier 1), Risk Informed",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 13,
    "text": "(Tier 2), Repeatable (Tier 3), and Adaptive (Tier 4), descriptions of which are in Appendix E. The Tiers represent a progression, albeit not a compulsory one. Although organizations at Tier 1 will likely benefit from moving to Tier 2, not all organizations need to achieve Tiers 3 or 4 (or may only focus on certain areas of these Tiers). Progression to higher Tiers is appropriate when an organization s processes or resources at its current Tier may be insufficient to help it manage its privacy risks. An organization can use the Tiers to communicate internally about resource allocations necessary to progress to a higher Tier or as general benchmarks to gauge progress in its capability to manage privacy risks. An organization can also use Tiers to understand the scale of resources and processes of other organizations in the data processing ecosystem and how they align with the organization s privacy risk management priorities. Nonetheless, successful implementation of the Privacy Framework is based upon achieving the outcomes described in an organization s Target Profile(s) and not upon Tier determination. 3.0 How to Use the Privacy Framework When used as a risk management tool, the Privacy Framework can assist an organization in its efforts to optimize beneficial uses of data and the development of innovative systems, products, and services while minimizing adverse consequences for individuals. The Privacy Framework can help organizations answer the fundamental question, How are we considering the impacts to individuals as we develop our systems, products, and services? To account for the unique needs of an organization, use of the Privacy Framework is flexible, although it is designed to complement existing business and system development operations. The decision about how to apply it is left to the implementing organization. For example, an organization may already have robust privacy risk",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      14
    ],
    "titles": [
      "13  See, e.g., Organisation for Economic Co-operation and Development (OECD) (2013) OECD Guidelines on the"
    ],
    "chunk_index": 14,
    "text": "management processes, but may use the Core s five Functions as a streamlined way to analyze and articulate any gaps. Alternatively, an organization seeking to establish a privacy program can use the Core s Categories and Subcategories as a reference. Other organizations may compare Profiles or Tiers to align privacy risk management priorities across different roles in the data processing ecosystem. The variety of ways in which the Privacy Framework can be used by organizations should discourage the notion of compliance with the Privacy Framework as a uniform or externally referenceable concept. The following subsections present a few options for use of the Privacy Framework. Mapping to Informative References Informative references are mappings to Subcategories to provide implementation support, including mappings of tools, technical guidance, standards, laws, regulations, and best practices. Crosswalks that map the provisions of standards, laws, and regulations to Subcategories can help organizations determine which activities or outcomes to prioritize to facilitate compliance. The Privacy Framework is technology neutral, but it supports technological innovation because any organization or industry sector can develop these mappings as technology and related business needs evolve. By relying on consensus- based standards, guidelines, and practices, the tools and methods available to achieve positive privacy outcomes can scale across borders and accommodate the global nature of privacy risks. The use of existing and emerging standards will enable economies of scale and drive the development of systems, products, and services that meet identified market needs while being mindful of the privacy needs of individuals. January 16, 2020 Gaps in mappings can also be used to identify where additional or revised standards, guidelines, and practices would help an organization to address emerging needs. An organization implementing a given Subcategory, or developing a new Subcategory, might discover that there is insufficient guidance for a related activity",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 15,
    "text": "or outcome. To address that need, an organization might collaborate with technology leaders and/or standards bodies to draft, develop, and coordinate standards, guidelines, or practices. resources can support organizations use of the Privacy Framework and achievement of better privacy practices. Strengthening Accountability Accountability is generally considered a key privacy principle, although conceptually it is not unique to privacy.13 Accountability occurs throughout an organization, and it can be expressed at varying degrees of abstraction, for example as a cultural value, as governance policies and procedures, or as traceability relationships between privacy requirements and controls. Privacy risk management can be a means of supporting accountability at all organizational levels as it connects senior executives, who can communicate an organization s privacy values and risk tolerance, to those at the business/process manager level, who can collaborate on the development and implementation of governance policies and procedures that support organizational privacy values. These policies and procedures can then be communicated to those at the implementation/operations level, who collaborate on defining the privacy requirements that support the expression of the policies and procedures in an organization s systems, products, and services. Personnel at the 13 See, e.g., Organisation for Economic Co-operation and Development (OECD) (2013) OECD Guidelines on the Protection of Privacy and Transborder Flows of Personal Data, available at sonaldata.htm; International Organization for Standardization (ISO)/International Electrotechnical Commission (IEC) (2011) ISO/IEC 29100:2011 Information technology Security techniques Privacy framework (ISO, Geneva, Switzerland), available at Automobile Manufacturers, Inc., Association of Global Automakers, Inc. (2014) Consumer Privacy Protection content/uploads/2017/01/Consumer_Privacy_Principlesfor_VehicleTechnologies_Services-03-21-19.pdf. Figure 7: Notional Collaboration and Communication Flows Within an Organization Business/Process Manager Level Responsibilities: Develop Profiles Allocate budget Inform Tier selection Senior Executive Level Express mission priorities, risk tolerance, organizational privacy values, and budget Accept/decline risk decisions Responsibilities: Implementation/ Operations Level Implement Profiles Monitor progress Conduct privacy risk assessments",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      15
    ],
    "titles": [
      "14  See, e.g., NIST SP 800-37, Rev. 2, Risk Management Framework for Information Systems and Organizations: A"
    ],
    "chunk_index": 16,
    "text": "Responsibilities: C O M M U N I C A T I O N Privacy posture, changes in risk, implementation progress, and incident management activities C O L L A B O R A T I O N Tier selection and Profile development January 16, 2020 implementation/operations level also select, implement, and assess controls as the technical and policy measures that meet the privacy requirements, and report on progress, gaps and deficiencies, incident management, and changing privacy risks so that those at the business/process manager level and the senior executives can better understand and respond appropriately. Figure 7 provides a graphical representation of this bi-directional collaboration and communication and how elements of the Privacy Framework can be incorporated to facilitate the process. In this way, organizations can use the Privacy Framework as a tool to support accountability. They can also use the Privacy Framework in conjunction with other frameworks and guidance that provide additional practices to achieve accountability within and between organizations.14 Establishing or Improving a Privacy Program Using a simple model of ready, set, go phases, the Privacy Framework can support the creation of a new privacy program or improvement of an existing program. As an organization goes through these phases, it may use informative references to provide guidance on prioritizing or achieving outcomes. See section 3.1 for more information about informative references. In addition, a repository can be found at Ready Effective privacy risk management requires an organization to understand its mission or business environment; its legal environment; its risk tolerance; the privacy risks engendered by its systems, products, or services; and its role(s) in the data processing ecosystem. An organization can use the Identify-P and Govern-P Functions to get ready by reviewing the Categories and Subcategories, and beginning to develop its Current Profile and Target Profile.15 Activities",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      16
    ],
    "titles": [
      "16  NIST SP 800-37, Rev. 2 [7] provides additional information on steps to execute on the action plan, including"
    ],
    "chunk_index": 17,
    "text": "and outcomes such as establishing organizational privacy values and policies, determining and expressing an organizational risk tolerance, and conducting privacy risk assessments (see Appendix D for more information on privacy risk assessments) provide a foundation for completing the Profiles in Set. Set An organization completes its Current Profile by indicating which Category and Subcategory outcomes from the remaining Functions are being achieved. If an outcome is partially achieved, noting this fact will help support subsequent steps by providing baseline information. Informed by the activities under Identify and Govern, such as organizational privacy values and policies, organizational risk tolerance, and privacy risk assessment results, an organization completes its Target Profile focused on the assessment of the Categories and Subcategories describing its desired privacy outcomes. An organization also may develop its own additional Functions, Categories, and Subcategories to account for unique organizational risks. It may also consider influences and requirements of external stakeholders such as 14 See, e.g., NIST SP 800-37, Rev. 2, Risk Management Framework for Information Systems and Organizations: A System Life Cycle Approach for Security and Privacy [7]; and Organization for the Advancement of Structured Information Standards (OASIS) (2016) Privacy Management Reference Model and Methodology (PMRM) 15 For additional information, see the Prepare step, Section 3.1, NIST SP 800-37, Rev. 2 [7]. A Simplified Method for Establishing or Improving a Privacy Program Ready: use the Identify-P and Govern-P Functions to get ready. Set: set an action plan based on the differences between Current and Target Profile(s). Go: go forward with implementing the action plan. January 16, 2020 business customers and partners when creating a Target Profile. An organization can develop multiple Profiles to support its different business lines or processes, which may have different business needs and associated risk tolerances. An organization compares the Current Profile and the Target",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 18,
    "text": "Profile to determine gaps. Next, it creates a prioritized action plan to address gaps reflecting mission drivers, costs and benefits, and risks to achieve the outcomes in the Target Profile. An organization using the Cybersecurity Framework and the Privacy Framework together may develop integrated action plans. It then determines resources, including funding and workforce needs, necessary to address the gaps, which can inform the selection of an appropriate Tier. Using Profiles in this manner encourages an organization to make informed decisions about privacy activities, supports risk management, and enables an organization to perform cost-effective, targeted improvements. Go With the action plan set, an organization prioritizes which actions to take to address any gaps, and then adjusts its current privacy practices in order to achieve the Target Profile.16 An organization can go through the phases nonsequentially as needed to continuously assess and improve its privacy posture. For instance, an organization may find that more frequent repetition of the Ready phase improves the quality of privacy risk assessments. Furthermore, an organization may monitor progress through iterative updates to the Current Profile or the Target Profile to adjust to changing risks, subsequently comparing the Current Profile to the Target Profile. Applying to the System Development Life Cycle The Target Profile can be aligned with the system development life cycle (SDLC) phases of plan, design, build/buy, deploy, operate, and decommission to support the achievement of the prioritized privacy outcomes.17 Beginning with the plan phase the prioritized privacy outcomes can be transformed into the privacy capabilities and requirements for the system, recognizing that requirements are likely to evolve during the remainder of the life cycle. A key milestone of the design phase is validating that the privacy capabilities and requirements match the needs and risk tolerance of an organization as expressed in the Target Profile.",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      17
    ],
    "titles": [],
    "chunk_index": 19,
    "text": "That same Target Profile can serve as an internal list to be assessed when deploying the system to verify that all privacy capabilities and requirements are implemented. The privacy outcomes determined by using the Privacy Framework should then serve as a basis for ongoing operation of the system. This includes occasional reassessment, capturing results in a Current Profile, to verify that privacy capabilities and requirements are still fulfilled. Privacy risk assessments typically focus on the data life cycle, the stages through which data passes, often characterized as creation or collection, processing, dissemination, use, storage, and disposition, to include destruction and deletion. Aligning the SDLC and the data lifecycle by identifying and understanding how data are processed during all stages of the SDLC helps organizations to better manage privacy risks and informs the selection and implementation of privacy controls to meet privacy requirements. 16 NIST SP 800-37, Rev. 2 [7] provides additional information on steps to execute on the action plan, including control selection, implementation, and assessment to close any gaps. 17 Within the SDLC, organizations may employ a variety of development methodologies (e.g., waterfall, spiral, or agile). January 16, 2020 Using within the Data Processing Ecosystem A key factor in the management of privacy risk is an entity s role(s) in the data processing ecosystem, which can affect not only its legal obligations, but also the measures it may take to manage privacy risk. As depicted in Figure 8, the data processing ecosystem encompasses a range of entities and roles that may have complex, multi-directional relationships with each other and individuals. Complexity can increase when entities are supported by a chain of sub-entities; for example, service providers may be supported by a series of service providers, or manufacturers may have multiple component suppliers. Figure 8 displays entities as having distinct",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 20,
    "text": "roles, but some may have multiple roles, such as an organization providing services to other organizations and providing retail products to consumers. The roles in Figure 8 are intended to be notional classifications. In practice, an entity s role(s) may be legally codified for example, some laws classify organizations as data controllers or data processors or classifications may be derived from industry sector designations. By developing one or more Profiles relevant to its role(s), an entity can use the Privacy Framework to consider how to manage privacy risk not only with regard to its own priorities, but also in relation to how the measures it may take affect other data processing ecosystem entities management of privacy risk. For example: An organization that makes decisions about how to collect and use data about individuals may use a Profile to express privacy requirements to an external service provider (e.g., a cloud provider to which it is exporting data); the external service provider that processes the data may use its Profile to demonstrate the measures it has adopted to process data in line with contractual obligations. An organization may express its privacy posture through a Current Profile to report results or to compare with acquisition requirements. An industry sector may establish a common Profile that can be used by its members to customize their own Profiles. A manufacturer may use a Target Profile to determine the capabilities to build into its products so that its business customers can meet the privacy needs of their end users. A developer may use a Target Profile to consider how to design an application that enables privacy protections when used within other organizations system environments. The Privacy Framework provides a common language to communicate privacy requirements with entities within the data processing ecosystem. The need for this",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      18
    ],
    "titles": [],
    "chunk_index": 21,
    "text": "communication can be particularly Individuals Developer Business Associates or Partners Commercial Product/ Services Civil Society Research/ Education Manufacturer Public/ Government Services Supplier or Service Provider Figure 8: Data Processing Ecosystem Relationships January 16, 2020 notable when the data processing ecosystem crosses national boundaries, such as with international data transfers. Organizational practices that support communication may include: Determining privacy requirements; Enacting privacy requirements through formal agreement (e.g., contracts, multi-party frameworks); Communicating how those privacy requirements will be verified and validated; Verifying that privacy requirements are met through a variety of assessment methodologies; and Governing and managing the above activities. Informing Buying Decisions Since either a Current or Target Profile can be used to generate a prioritized list of privacy requirements, these Profiles can also be used to inform decisions about buying products and services. By first selecting outcomes that are relevant to its privacy goals, an organization then can evaluate partners systems, products, or services against this outcome. For example, if a device is being purchased for environmental monitoring of a forest, manageability may be important to support capabilities for minimizing the processing of data about people using the forest and should drive a manufacturer evaluation against applicable Subcategories in the Core (e.g., CT.DP-P4: system or device configurations permit selective collection or disclosure of data elements). In circumstances where it may not be possible to impose a set of privacy requirements on the supplier, the objective should be to make the best buying decision among multiple suppliers, given a carefully determined list of privacy requirements. Often, this means some degree of trade-off, comparing multiple products or services with known gaps to the Profile. If the system, product, or service purchased did not meet all of the objectives described in the Profile, an organization could address the residual risk through mitigation measures",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      19
    ],
    "titles": [
      "12_0.pdf"
    ],
    "chunk_index": 22,
    "text": "or other management actions. January 16, 2020 References [1] National Institute of Standards and Technology (2018) Framework for Improving Critical Infrastructure Cybersecurity, Version 1.1. (National Institute of Standards and Technology, [2] National Institute of Standards and Technology (2019) Summary Analysis of the Responses to the NIST Privacy Framework Request for Information. (National Institute of Standards and Technology, Gaithersburg, MD). rfi_response_analysis_privacyframework_2.27.19.pdf [3] National Institute of Standards and Technology (2019) NIST Privacy Risk Assessment Methodology (PRAM). (National Institute of Standards and Technology, Gaithersburg, MD). [4] The Smart Grid Interoperability Panel Smart Grid Cybersecurity Committee (2014) Guidelines for Smart Grid Cybersecurity: Volume 1 - Smart Grid Cybersecurity Strategy, Architecture, and High-Level Requirements. (National Institute of Standards and Technology, Gaithersburg, MD), NIST Interagency or Internal Report (IR) 7628, Rev. 1, Vol. 1. [5] Brooks SW, Garcia ME, Lefkovitz NB, Lightman S, Nadeau EM (2017) An Introduction to Privacy Engineering and Risk Management in Federal Systems. (National Institute of Standards and Technology, Gaithersburg, MD), NIST Interagency or Internal Report (IR) 8062. [6] Joint Task Force Transformation Initiative (2011) Managing Information Security Risk: Organization, Mission, and Information System View. (National Institute of Standards and Technology, Gaithersburg, MD). NIST Special Publication (SP) 800-39. [7] Joint Task Force (2018) Risk Management Framework for Information Systems and Organizations: A System Life Cycle Approach for Security and Privacy. (National Institute of Standards and Technology, Gaithersburg, MD), NIST Special Publication (SP) 800-37 Rev. 2. [8] Grassi PA, Garcia ME, Fenton JL (2017) Digital Identity Guidelines. (National Institute of Standards and Technology, Gaithersburg, MD), NIST Special Publication (SP) 800-63-3, Includes updates as [9] Office of Management and Budget (2017) Preparing for and Responding to a Breach of Personally Identifiable Information. (The White House, Washington, DC), OMB Memorandum M-17-12, January 3, 2017. Available at 12_0.pdf [10] Joint Task Force Transformation Initiative (2013) Security",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      20,
      21
    ],
    "titles": [],
    "chunk_index": 23,
    "text": "and Privacy Controls for Federal Information Systems and Organizations. (National Institute of Standards and Technology, Gaithersburg, MD), NIST Special Publication (SP) 800-53, Rev. 4, Includes updates as of January [11] Grassi PA, Lefkovitz NB, Nadeau EM, Galluzzo RJ, Dinh AT (2018) Attribute Metadata: A Proposed Schema for Evaluating Federated Attributes. (National Institute of Standards and Technology, Gaithersburg, MD), NIST Interagency or Internal Report (IR) 8112. January 16, 2020 [12] Joint Task Force Transformation Initiative (2012) Guide for Conducting Risk Assessments. (National Institute of Standards and Technology, Gaithersburg, MD), NIST Special Publication (SP) [13] Definitions, Title 44 U.S. Code, Sec. 3542. 2011 ed. subchapIII-sec3542 January 16, 2020 Appendix A: Privacy Framework Core This appendix presents the Core: a table of Functions, Categories, and Subcategories that describe specific activities and outcomes that can support managing privacy risks when systems, products, and services are processing data. Note to Users Risk-based Approach: The Core is not a checklist of actions to perform. An organization selects Subcategories consistent with its risk strategy to protect individuals privacy, as noted in Category statements. An organization may not need to achieve every outcome or activity reflected in the Core. It is expected that an organization will use Profiles to select and prioritize the Functions, Categories, and Subcategories that best meet its specific needs by considering its goals, role(s) in the data processing ecosystem or industry sector, legal/regulatory requirements and industry best practices, risk management priorities, and the privacy needs of individuals who are directly or indirectly served or affected by an organization s systems, products, or services. It is not obligatory to achieve an outcome in its entirety. An organization may use its Profiles to express partial achievement of an outcome, as not all aspects of an outcome may be relevant for it to manage privacy risk, or",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 24,
    "text": "an organization may use a Target Profile to express an aspect of an outcome that it does not currently have the capability to achieve. It may be necessary to consider multiple outcomes in combination to appropriately manage privacy risk. For example, an organization that responds to individuals requests for data access may select for its Profile both the Subcategory CT.DM-P1: Data elements can be accessed for review and the Category Identity Management, Authentication, and Access Control (PR.AC-P) to ensure that only the individual to whom the data pertain gets access. Implementation: The tabular format of the Core is not intended to suggest a specific implementation order or imply a degree of importance between the Functions, Categories, and Subcategories. Implementation may be nonsequential, simultaneous, or iterative, depending on the SDLC stage, status of the privacy program, scale of the workforce, or role(s) of an organization in the data processing ecosystem. In addition, the Core is not exhaustive; it is extensible, allowing organizations, sectors, and other entities to adapt or add additional Functions, Categories, and Subcategories to their Profiles. Roles: Ecosystem Roles: The Core is intended to be usable by any organization or entity regardless of its role(s) in the data processing ecosystem. Although the Privacy Framework does not classify ecosystem roles, an organization should review the Core from its standpoint in the ecosystem. An organization s role(s) may be legally codified for example, some laws classify organizations as data controllers or data processors or classifications may be derived from industry designations. Since Core elements are not assigned by ecosystem role, an organization can use its Profiles to select Functions, Categories, and Subcategories that are relevant to its role(s). Organizational Roles: Different parts of an organization s workforce may take responsibility for different Categories or Subcategories. For example, the legal department may",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      22
    ],
    "titles": [],
    "chunk_index": 25,
    "text": "be responsible for carrying out activities under Governance Policies, Processes, and Procedures while the IT department is working on Inventory and Mapping. Ideally, the Core encourages cross- organization collaboration to develop Profiles and achieve outcomes. January 16, 2020 Scalability: Certain aspects of outcomes may be ambiguously worded. For example, outcomes may include terms like communicated or disclosed without stating to whom the communications or disclosures are being made. The ambiguity is intentional to allow for a wide range of organizations with different use cases to determine what is appropriate or required in a given context. Resource Repository: Standalone resources that can provide more information on how to prioritize or Cybersecurity Framework Alignment: As noted in section 2.1, organizations can use the five Privacy Framework Functions Identify-P, Govern-P, Control-P, Communicate-P, and Protect-P to manage privacy risks arising from data processing. Protect-P is specifically focused on managing risks associated with security-related privacy events (e.g., privacy breaches). To further support the management of risks associated with security-related privacy events, organizations may choose to use Detect, Respond, and Recover Functions from the Cybersecurity Framework. For this reason, these Functions are included in Table 1, but are greyed out. Alternatively, organizations may use all five of the Cybersecurity Framework Functions in conjunction with Identify-P, Govern-P, Control-P, and Communicate-P to collectively address privacy and security risks. See Figure 5 for an illustrated example of how the Functions from both frameworks can be used in varying combinations to manage different aspects of privacy and cybersecurity risks. Certain Functions, Categories, or Subcategories may be identical to or have been adapted from the Cybersecurity Framework. The following legend can be used to identify this relationship in Table 2. A complete crosswalk between the two frameworks can be found in the resource Core Identifiers: For ease of use, each component",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      23,
      24
    ],
    "titles": [
      "IDENTIFY-P (ID-",
      "ID-P"
    ],
    "chunk_index": 26,
    "text": "of the Core is given a unique identifier. Functions and Categories each have a unique alphabetic identifier, as shown in Table 1. Subcategories within each Category have a number added to the alphabetic identifier; the unique identifier for each Subcategory is included in Table 2. The Category or Subcategory is identical to the Cybersecurity Framework. The Function, Category, or Subcategory aligns with the Cybersecurity Framework, but the text has been adapted for the Privacy Framework. January 16, 2020 Table 1: Privacy Framework Function and Category Unique Identifiers Function Unique Identifier Function Category Unique Identifier Category ID-P Identify-P ID.IM-P Inventory and Mapping ID.BE-P Business Environment ID.RA-P Risk Assessment ID.DE-P Data Processing Ecosystem Risk Management GV-P Govern-P GV.PO-P Governance Policies, Processes, and Procedures GV.RM-P Risk Management Strategy GV.AT-P Awareness and Training GV.MT-P Monitoring and Review CT-P Control-P CT.PO-P Data Processing Policies, Processes, and Procedures CT.DM-P Data Processing Management CT.DP-P Disassociated Processing CM-P Communicate-P CM.PO-P Communication Policies, Processes, and Procedures CM.AW-P Data Processing Awareness PR-P Protect-P PR.PO-P Data Protection Policies, Processes, and Procedures PR.AC-P Identity Management, Authentication, and Access Control PR.DS-P Data Security PR.MA-P Maintenance PR.PT-P Protective Technology DE Detect DE.AE Anomalies and Events DE.CM Security Continuous Monitoring DE.DP Detection Processes RS Respond RS.RP Response Planning RS.CO Communications RS.AN Analysis RS.MI Mitigation RS.IM Improvements RC Recover RC.RP Recovery Planning RC.IM Improvements RC.CO Communications January 16, 2020 Table 2: Privacy Framework Core Function Category Subcategory IDENTIFY-P (ID- P): Develop the organizational understanding to manage privacy risk for individuals arising from data processing. Inventory and Mapping (ID.IM-P): Data processing by systems, products, or services is understood and informs the management of privacy risk. ID.IM-P1: Systems/products/services that process data are inventoried. ID.IM-P2: Owners or operators (e.g., the organization or third parties such as service providers, partners, customers, and developers) and their roles with respect to the",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      25
    ],
    "titles": [],
    "chunk_index": 27,
    "text": "systems/products/services and components (e.g., internal or external) that process data are inventoried. ID.IM-P3: Categories of individuals (e.g., customers, employees or prospective employees, consumers) whose data are being processed are inventoried. ID.IM-P4: Data actions of the systems/products/services are inventoried. ID.IM-P5: The purposes for the data actions are inventoried. ID.IM-P6: Data elements within the data actions are inventoried. ID.IM-P7: The data processing environment is identified (e.g., geographic location, internal, cloud, third parties). ID.IM-P8: Data processing is mapped, illustrating the data actions and associated data elements for systems/products/services, including components; roles of the component owners/operators; and interactions of individuals or third parties with the systems/products/services. Business Environment (ID.BE-P): The organization s mission, objectives, stakeholders, and activities are understood and prioritized; this information is used to inform privacy roles, responsibilities, and risk management decisions. ID.BE-P1: The organization s role(s) in the data processing ecosystem are identified and communicated. ID.BE-P2: Priorities for organizational mission, objectives, and activities are established and communicated. ID.BE-P3: Systems/products/services that support organizational priorities are identified and key requirements communicated. January 16, 2020 Function Category Subcategory Risk Assessment (ID.RA-P): The organization understands the privacy risks to individuals and how such privacy risks may create follow-on impacts on organizational operations, including mission, functions, other risk management priorities (e.g., compliance, financial), reputation, workforce, and culture. ID.RA-P1: Contextual factors related to the systems/products/services and the data actions are identified (e.g., individuals demographics and privacy interests or perceptions, data sensitivity and/or types, visibility of data processing to individuals and third parties). ID.RA-P2: Data analytic inputs and outputs are identified and evaluated for bias. ID.RA-P3: Potential problematic data actions and associated problems are identified. ID.RA-P4: Problematic data actions, likelihoods, and impacts are used to determine and prioritize risk. ID.RA-P5: Risk responses are identified, prioritized, and implemented. Data Processing Ecosystem Risk Management (ID.DE-P): The organization s priorities, constraints,",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      26
    ],
    "titles": [
      "GOVERN-P (GV-P):"
    ],
    "chunk_index": 28,
    "text": "risk tolerance, and assumptions are established and used to support risk decisions associated with managing privacy risk and third parties within the data processing ecosystem. The organization has established and implemented the processes to identify, assess, and manage privacy risks within the data processing ecosystem. ID.DE-P1: Data processing ecosystem risk management policies, processes, and procedures are identified, established, assessed, managed, and agreed to by organizational stakeholders. ID.DE-P2: Data processing ecosystem parties (e.g., service providers, customers, partners, product manufacturers, application developers) are identified, prioritized, and assessed using a privacy risk assessment process. ID.DE-P3: Contracts with data processing ecosystem parties are used to implement appropriate measures designed to meet the objectives of an organization s privacy program. ID.DE-P4: Interoperability frameworks or similar multi-party approaches are used to manage data processing ecosystem privacy risks. ID.DE-P5: Data processing ecosystem parties are routinely assessed using audits, test results, or other forms of evaluations to confirm they are meeting their contractual, interoperability framework, or other obligations. January 16, 2020 Function Category Subcategory GOVERN-P (GV-P): Develop and implement the organizational governance structure to enable an ongoing understanding of the organization s risk management priorities that are informed by privacy risk. Governance Policies, Processes, and Procedures (GV.PO-P): The policies, processes, and procedures to manage and monitor the organization s regulatory, legal, risk, environmental, and operational requirements are understood and inform the management of privacy risk. GV.PO-P1: Organizational privacy values and policies (e.g., conditions on data processing such as data uses or retention periods, individuals prerogatives with respect to data processing) are established and communicated. GV.PO-P2: Processes to instill organizational privacy values within system/product/service development and operations are established and in place. GV.PO-P3: Roles and responsibilities for the workforce are established with respect to privacy. GV.PO-P4: Privacy roles and responsibilities are coordinated and aligned with third-party stakeholders (e.g., service",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      27
    ],
    "titles": [
      "CONTROL-P (CT-"
    ],
    "chunk_index": 29,
    "text": "providers, customers, partners). GV.PO-P5: Legal, regulatory, and contractual requirements regarding privacy are understood and managed. GV.PO-P6: Governance and risk management policies, processes, and procedures address privacy risks. Risk Management Strategy (GV.RM-P): The organization s priorities, constraints, risk tolerances, and assumptions are established and used to support operational risk decisions. GV.RM-P1: Risk management processes are established, managed, and agreed to by organizational stakeholders. GV.RM-P2: Organizational risk tolerance is determined and clearly expressed. GV.RM-P3: The organization s determination of risk tolerance is informed by its role(s) in the data processing ecosystem. Awareness and Training (GV.AT-P): The organization s workforce and third parties engaged in data processing are provided privacy awareness education and are trained to perform their privacy-related duties and responsibilities consistent with related policies, processes, procedures, and agreements and organizational privacy values. GV.AT-P1: The workforce is informed and trained on its roles and responsibilities. GV.AT-P2: Senior executives understand their roles and responsibilities. GV.AT-P3: Privacy personnel understand their roles and responsibilities. GV.AT-P4: Third parties (e.g., service providers, customers, partners) understand their roles and responsibilities. January 16, 2020 Function Category Subcategory Monitoring and Review (GV.MT-P): The policies, processes, and procedures for ongoing review of the organization s privacy posture are understood and inform the management of privacy risk. GV.MT-P1: Privacy risk is re-evaluated on an ongoing basis and as key factors, including the organization s business environment (e.g., introduction of new technologies), governance (e.g., legal obligations, risk tolerance), data processing, and systems/products/services change. GV.MT-P2: Privacy values, policies, and training are reviewed and any updates are communicated. GV.MT-P3: Policies, processes, and procedures for assessing compliance with legal requirements and privacy policies are established and in place. GV.MT-P4: Policies, processes, and procedures for communicating progress on managing privacy risks are established and in place. GV.MT-P5: Policies, processes, and procedures are established and in place to",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      28
    ],
    "titles": [],
    "chunk_index": 30,
    "text": "receive, analyze, and respond to problematic data actions disclosed to the organization from internal and external sources (e.g., internal discovery, privacy researchers, professional events). GV.MT-P6: Policies, processes, and procedures incorporate lessons learned from problematic data actions. GV.MT-P7: Policies, processes, and procedures for receiving, tracking, and responding to complaints, concerns, and questions from individuals about organizational privacy practices are established and in place. CONTROL-P (CT- P): Develop and implement appropriate activities to enable organizations or individuals to manage data with sufficient granularity to Data Processing Policies, Processes, and Procedures (CT.PO-P): Policies, processes, and procedures are maintained and used to manage data processing (e.g., purpose, scope, roles and responsibilities in the data processing ecosystem, and management commitment) consistent with the organization s risk strategy to protect individuals privacy. CT.PO-P1: Policies, processes, and procedures for authorizing data processing (e.g., organizational decisions, individual consent), revoking authorizations, and maintaining authorizations are established and in place. CT.PO-P2: Policies, processes, and procedures for enabling data review, transfer, sharing or disclosure, alteration, and deletion are established and in place (e.g., to maintain data quality, manage data retention). CT.PO-P3: Policies, processes, and procedures for enabling individuals data processing preferences and requests are established and in place. January 16, 2020 Function Category Subcategory manage privacy risks. CT.PO-P4: A data life cycle to manage data is aligned and implemented with the system development life cycle to manage systems. Data Processing Management (CT.DM-P): Data are managed consistent with the organization s risk strategy to protect individuals privacy, increase manageability, and enable the implementation of privacy principles (e.g., individual participation, data quality, data minimization). CT.DM-P1: Data elements can be accessed for review. CT.DM-P2: Data elements can be accessed for transmission or disclosure. CT.DM-P3: Data elements can be accessed for alteration. CT.DM-P4: Data elements can be accessed for deletion. CT.DM-P5: Data are destroyed according",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      29
    ],
    "titles": [
      "COMMUNICATE-P"
    ],
    "chunk_index": 31,
    "text": "to policy. CT.DM-P6: Data are transmitted using standardized formats. CT.DM-P7: Mechanisms for transmitting processing permissions and related data values with data elements are established and in place. CT.DM-P8: Audit/log records are determined, documented, implemented, and reviewed in accordance with policy and incorporating the principle of data minimization. CT.DM-P9: Technical measures implemented to manage data processing are tested and assessed. CT.DM-P10: Stakeholder privacy preferences are included in algorithmic design objectives and outputs are evaluated against these preferences. Disassociated Processing (CT.DP-P): Data processing solutions increase disassociability consistent with the organization s risk strategy to protect individuals privacy and enable implementation of privacy principles (e.g., data minimization). CT.DP-P1: Data are processed to limit observability and linkability (e.g., data actions take place on local devices, privacy-preserving cryptography). CT.DP-P2: Data are processed to limit the identification of individuals (e.g., de-identification privacy techniques, tokenization). CT.DP-P3: Data are processed to limit the formulation of inferences about individuals behavior or activities (e.g., data processing is decentralized, distributed architectures). CT.DP-P4: System or device configurations permit selective collection or disclosure of data elements. CT.DP-P5: Attribute references are substituted for attribute values. January 16, 2020 Function Category Subcategory COMMUNICATE-P (CM-P): Develop and implement appropriate activities to enable organizations and individuals to have a reliable understanding and engage in a dialogue about how data are processed and associated privacy risks. Communication Policies, Processes, and Procedures (CM.PO-P): Policies, processes, and procedures are maintained and used to increase transparency of the organization s data processing practices (e.g., purpose, scope, roles and responsibilities in the data processing ecosystem, and management commitment) and associated privacy risks. CM.PO-P1: Transparency policies, processes, and procedures for communicating data processing purposes, practices, and associated privacy risks are established and in place. CM.PO-P2: Roles and responsibilities (e.g., public relations) for communicating data processing purposes, practices, and associated privacy risks are established.",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      30
    ],
    "titles": [],
    "chunk_index": 32,
    "text": "Data Processing Awareness (CM.AW-P): Individuals and organizations have reliable knowledge about data processing practices and associated privacy risks, and effective mechanisms are used and maintained to increase predictability consistent with the organization s risk strategy to protect individuals privacy. CM.AW-P1: Mechanisms (e.g., notices, internal or public reports) for communicating data processing purposes, practices, associated privacy risks, and options for enabling individuals data processing preferences and requests are established and in place. CM.AW-P2: Mechanisms for obtaining feedback from individuals (e.g., surveys or focus groups) about data processing and associated privacy risks are established and in place. CM.AW-P3: System/product/service design enables data processing visibility. CM.AW-P4: Records of data disclosures and sharing are maintained and can be accessed for review or transmission/disclosure. CM.AW-P5: Data corrections or deletions can be communicated to individuals or organizations (e.g., data sources) in the data processing ecosystem. CM.AW-P6: Data provenance and lineage are maintained and can be accessed for review or transmission/disclosure. CM.AW-P7: Impacted individuals and organizations are notified about a privacy breach or event. CM.AW-P8: Individuals are provided with mitigation mechanisms (e.g., credit monitoring, consent withdrawal, data alteration or deletion) to address impacts of problematic data actions. PROTECT-P (PR-P): Develop and implement Data Protection Policies, Processes, and Procedures (PR.PO-P): Security and privacy policies (e.g., purpose, scope, roles PR.PO-P1: A baseline configuration of information technology is created and maintained incorporating security principles (e.g., concept of least functionality). January 16, 2020 Function Category Subcategory appropriate data processing safeguards. and responsibilities in the data processing ecosystem, and management commitment), processes, and procedures are maintained and used to manage the protection of data. PR.PO-P2: Configuration change control processes are established and in place. PR.PO-P3: Backups of information are conducted, maintained, and tested. PR.PO-P4: Policy and regulations regarding the physical operating environment for organizational assets are met. PR.PO-P5: Protection processes are improved.",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      31
    ],
    "titles": [],
    "chunk_index": 33,
    "text": "PR.PO-P6: Effectiveness of protection technologies is shared. PR.PO-P7: Response plans (Incident Response and Business Continuity) and recovery plans (Incident Recovery and Disaster Recovery) are established, in place, and managed. PR.PO-P8: Response and recovery plans are tested. PR.PO-P9: Privacy procedures are included in human resources practices (e.g., deprovisioning, personnel screening). PR.PO-P10: A vulnerability management plan is developed and implemented. Identity Management, Authentication, and Access Control (PR.AC-P): Access to data and devices is limited to authorized individuals, processes, and devices, and is managed consistent with the assessed risk of unauthorized access. PR.AC-P1: Identities and credentials are issued, managed, verified, revoked, and audited for authorized individuals, processes, and devices. PR.AC-P2: Physical access to data and devices is managed. PR.AC-P3: Remote access is managed. PR.AC-P4: Access permissions and authorizations are managed, incorporating the principles of least privilege and separation of duties. PR.AC-P5: Network integrity is protected (e.g., network segregation, network segmentation). PR.AC-P6: Individuals and devices are proofed and bound to credentials, and authenticated commensurate with the risk of the transaction (e.g., individuals security and privacy risks and other organizational risks). Data Security (PR.DS-P): Data are managed consistent with the PR.DS-P1: Data-at-rest are protected. PR.DS-P2: Data-in-transit are protected. January 16, 2020 Function Category Subcategory organization s risk strategy to protect individuals privacy and maintain data confidentiality, integrity, and availability. PR.DS-P3: Systems/products/services and associated data are formally managed throughout removal, transfers, and disposition. PR.DS-P4: Adequate capacity to ensure availability is maintained. PR.DS-P5: Protections against data leaks are implemented. PR.DS-P6: Integrity checking mechanisms are used to verify software, firmware, and information integrity. PR.DS-P7: The development and testing environment(s) are separate from the production environment. PR.DS-P8: Integrity checking mechanisms are used to verify hardware integrity. Maintenance (PR.MA-P): System maintenance and repairs are performed consistent with policies, processes, and procedures. PR.MA-P1: Maintenance and repair of organizational assets are performed",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      32
    ],
    "titles": [
      "(NIST SP 800-63-3 [8])"
    ],
    "chunk_index": 34,
    "text": "and logged, with approved and controlled tools. PR.MA-P2: Remote maintenance of organizational assets is approved, logged, and performed in a manner that prevents unauthorized access. Protective Technology (PR.PT-P): Technical security solutions are managed to ensure the security and resilience of systems/products/services and associated data, consistent with related policies, processes, procedures, and agreements. PR.PT-P1: Removable media is protected and its use restricted according to policy. PR.PT-P2: The principle of least functionality is incorporated by configuring systems to provide only essential capabilities. PR.PT-P3: Communications and control networks are protected. PR.PT-P4: Mechanisms (e.g., failsafe, load balancing, hot swap) are implemented to achieve resilience requirements in normal and adverse situations. January 16, 2020 Appendix B: Glossary This appendix defines selected terms used for the purposes of this publication. Attribute Reference (NIST SP 800-63-3 [8]) A statement asserting a property of a subscriber without necessarily containing identity information, independent of format. For example, for the attribute birthday, a reference could be older than 18 or born in December. Attribute Value (NIST SP 800-63-3 [8]) A complete statement asserting a property of a subscriber, independent of format. For example, for the attribute birthday, a value could be 12/1/1980 or December 1, 1980. Availability (44 U.S.C. [13]) Ensuring timely and reliable access to and use of information. Category The subdivision of a Function into groups of privacy outcomes closely tied to programmatic needs and particular activities. Communicate-P (Function) Develop and implement appropriate activities to enable organizations and individuals to have a reliable understanding and engage in a dialogue about how data are processed and associated privacy risks. Confidentiality (44 U.S.C. [13]) Preserving authorized restrictions on information access and disclosure, including means for protecting personal privacy and proprietary information. Control-P (Function) Develop and implement appropriate activities to enable organizations or individuals to manage data with sufficient granularity to",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      33
    ],
    "titles": [
      "8062 [5])"
    ],
    "chunk_index": 35,
    "text": "manage privacy risks. Core A set of privacy protection activities and outcomes. The Framework Core comprises three elements: Functions, Categories, and Subcategories. Cybersecurity Incident (Framework for Improving Critical Infrastructure Cybersecurity [1]) A cybersecurity event that has been determined to have an impact on the organization prompting the need for response and recovery. (OMB 17-12 [9]) An occurrence that (1) actually or imminently jeopardizes, without lawful authority, the integrity, confidentiality, or availability of information or an information system; or (2) constitutes a violation or imminent threat of violation of law, security policies, security procedures, or acceptable use policies. Data A representation of information, including digital and non-digital formats. January 16, 2020 Data Action (Adapted from NIST IR 8062 [5]) A system/product/service data life cycle operation, including, but not limited to collection, retention, logging, generation, transformation, use, disclosure, sharing, transmission, and disposal. Data Element The smallest named item of data that conveys meaningful information. Data Processing (Adapted from NIST IR 8062 [5]) The collective set of data actions (i.e., the complete data life cycle, including, but not limited to collection, retention, logging, generation, transformation, use, disclosure, sharing, transmission, and disposal). Data Processing Ecosystem The complex and interconnected relationships among entities involved in creating or deploying systems, products, or services or any components that process data. Disassociability (Adapted from NIST IR 8062 [5]) Enabling the processing of data or events without association to individuals or devices beyond the operational requirements of the system. Function A component of the Core that provides the highest level of structure for organizing basic privacy activities into Categories and Subcategories. Govern-P (Function) Develop and implement the organizational governance structure to enable an ongoing understanding of the organization s risk management priorities that are informed by privacy risk. Identify-P (Function) Develop the organizational understanding to manage privacy risk for",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      34
    ],
    "titles": [
      "8062 [5])"
    ],
    "chunk_index": 36,
    "text": "individuals arising from data processing. Implementation Tier Provides a point of reference on how an organization views privacy risk and whether it has sufficient processes and resources in place to manage that risk. Individual A single person or a group of persons, including at a societal level. Integrity (44 U.S.C. [13]) Guarding against improper information modification or destruction, and includes ensuring information non-repudiation and authenticity. Lineage The history of processing of a data element, which may include point-to- point data flows and the data actions performed upon the data element. Manageability (Adapted from NIST IR 8062 [5]) Providing the capability for granular administration of data, including alteration, deletion, and selective disclosure. Metadata (Adapted from NIST SP 800-53 [10]) Information describing the characteristics of data. This may include, for example, structural metadata describing data structures (i.e., data format, syntax, semantics) and descriptive metadata describing data contents. Predictability (Adapted from NIST IR 8062 [5]) Enabling reliable assumptions by individuals, owners, and operators about data and their processing by a system, product, or service. January 16, 2020 Privacy Breach (Adapted from OMB M-17-12 [9]) The loss of control, compromise, unauthorized disclosure, unauthorized acquisition, or any similar occurrence where (1) a person other than an authorized user accesses or potentially accesses data or (2) an authorized user accesses data for an other than authorized purpose. Privacy Control (Adapted from NIST SP 800-37 [7]) The administrative, technical, and physical safeguards employed within an organization to satisfy privacy requirements. Privacy Event The occurrence or potential occurrence of problematic data actions. Privacy Requirement A specification for system/product/service functionality to meet stakeholders desired privacy outcomes. Privacy Risk The likelihood that individuals will experience problems resulting from data processing, and the impact should they occur. Privacy Risk Assessment A privacy risk management sub-process for identifying and evaluating specific privacy",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      35,
      36
    ],
    "titles": [
      "IEC"
    ],
    "chunk_index": 37,
    "text": "risks. Privacy Risk Management A cross-organizational set of processes for identifying, assessing, and responding to privacy risks. Problematic Data Action (Adapted from NIST IR 8062 [5]) A data action that could cause an adverse effect for individuals. Processing See Data Processing. Profile A selection of specific Functions, Categories, and Subcategories from the Core that an organization has prioritized to help it manage privacy risk. Protect-P (Function) Develop and implement appropriate data processing safeguards. Provenance (Adapted from NIST IR 8112 [11]) Metadata pertaining to the origination or source of specified data. Risk (NIST SP 800-30 [12]) A measure of the extent to which an entity is threatened by a potential circumstance or event, and typically a function of: (i) the adverse impacts that would arise if the circumstance or event occurs; and (ii) the likelihood of occurrence. Risk Management The process of identifying, assessing, and responding to risk. Risk Tolerance (NIST SP 800-39 [6]) The level of risk or degree of uncertainty that is acceptable to organizations. Subcategory The further divisions of a Category into specific outcomes of technical and/or management activities. January 16, 2020 Appendix C: Acronyms This appendix defines selected acronyms used in the publication. IEC International Electrotechnical Commission IR Interagency or Internal Report ISO International Organization for Standardization IT Information Technology NIST National Institute of Standards and Technology OASIS Organization for the Advancement of Structured Information Standards OECD Organisation for Economic Co-operation and Development OMB Office of Management and Budget PMRM Privacy Management Reference Model and Methodology PRAM Privacy Risk Assessment Methodology RFC Request for Comment RFI Request for Information SDLC System Development Life Cycle SP Special Publication January 16, 2020 Appendix D: Privacy Risk Management Practices Section 1.2 introduces a number of considerations around privacy risk management, including the relationship between cybersecurity and privacy risk and the",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 38,
    "text": "role of privacy risk assessment. This appendix considers some of the key practices that contribute to successful privacy risk management, including organizing preparatory resources, determining privacy capabilities, defining privacy requirements, conducting privacy risk assessments, creating privacy requirements traceability, and monitoring for changing privacy risks. Category and Subcategory references are included to facilitate use of the Core to support these practices; these references appear in parentheticals. Organizing Preparatory Resources The appropriate resources facilitate informed decision-making about privacy risks at all levels of an organization. As a practical matter, the responsibility for the development of various resources may belong to different components of an organization. Therefore, a component of an organization depending on certain resources may find that they either do not exist, or may not sufficiently address privacy. In these circumstances, the dependent component can consider the purpose of the resource and either seek the information through other sources or make the best decision it can with the available information. In short, good resources are helpful, but any deficiencies should not prevent organizational components from making the best risk decisions they can within their capabilities. The following resources, while not exhaustive, build a foundation for better decision-making. Risk management role assignments (GV.PO-P3, GV.PO-P4) Establishing and enabling cross-organizational understanding of who is accountable and who has responsibility for privacy risk management as well as other risk management tasks in an organization supports better coordination and accountability for decision-making. In addition, a broad range of perspectives can improve the process of identifying, assessing, and responding to privacy risks. A diverse and cross-functional team can help to identify a more comprehensive range of risks to individuals privacy, and to select a wider set of mitigations. Determining which roles to include in the risk management discussions depends on organizational context and makeup, although collaboration between",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      37
    ],
    "titles": [],
    "chunk_index": 39,
    "text": "an organization s privacy and cybersecurity programs will be important. If one individual is being assigned to multiple roles, managing potential conflicts of interest should be considered. Enterprise risk management strategy (GV.RM-P) An organization s enterprise risk management strategy helps to align an organization s mission and values with organizational risk tolerance, assumptions, constraints, and priorities. Limitations on resources to achieve mission or business objectives and to manage a broad portfolio of risks will likely require trade-offs. Enabling personnel involved in the privacy risk management process to better understand an organization s risk tolerance should help to guide decisions about how to allocate resources and improve decisions around risk response. Key stakeholders (GV.PO-P4, ID.DE-P) Privacy stakeholders are those who have an interest or concern in the privacy outcomes of the system, product, or service. For example, legal concerns likely focus on whether the system, product, or service is operating in a way that would cause an organization to be out of compliance with privacy laws or regulations or its business agreements. Business owners that want to maximize usage may be concerned about loss of trust in the system, product, or service January 16, 2020 due to poor privacy. Individuals whose data are being processed or who are interacting with the system, product, or service will be interested in not experiencing problems or adverse consequences. Understanding the stakeholders and the types of privacy outcomes they are interested in will facilitate system/product/service design that appropriately addresses stakeholders needs. Organizational-level privacy requirements (GV.PO-P) Organizational-level privacy requirements are a means of expressing the legal obligations, privacy values, and policies to which an organization intends to adhere. Understanding these requirements is key to ensuring that the system/product/service design complies with its obligations. Organizational-level privacy requirements may be derived from a variety of sources, including: o",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 40,
    "text": "Legal environment (e.g., laws, regulations, contracts); o Organizational policies or cultural values; o Relevant standards; and o Privacy principles. System/product/service design artifacts (ID.BE-P3) Design artifacts may take many forms such as system design architectures or data flow diagrams. These artifacts help an organization determine how its systems, products, and services will operate. Therefore, they can help privacy programs understand how systems, products, and services need to function so that controls or measures that help to mitigate privacy risk can be selected and implemented in ways that maintain functionality while protecting privacy. Data maps (ID.IM-P) Data maps illustrate data processing and individuals interactions with systems, products, and services. A data map shows the data processing environment and includes the components through which data are being processed or with which individuals are interacting, the owners or operators of the components, and discrete data actions and the specific data elements being processed. Data maps can be illustrated in different ways, and the level of detail may vary based on an organization s needs. A data map can be overlaid on existing system/product/service design artifacts for convenience and ease of communication between organizational components. As discussed below, a data map is an important artifact in privacy risk assessment. Determining Privacy Capabilities Privacy capabilities can be used to describe the system, product, or service property or feature that achieves the desired privacy outcome (e.g., the service enables data minimization ). The security objectives confidentiality, integrity, and availability along with security requirements are used to inform the security capabilities for a system, product, or service. As set forth in Table 3, an additional set of privacy engineering objectives can support the determination of privacy capabilities. An organization may also use the privacy engineering objectives as a high-level prioritization tool. Systems, products, or services that are low",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      38
    ],
    "titles": [
      "18  The privacy engineering objectives are adapted from NIST IR 8062 [5]. The security objectives are from NIST SP"
    ],
    "chunk_index": 41,
    "text": "in predictability, manageability, or disassociability may be a signal of increased privacy risk, and therefore merit a more comprehensive privacy risk assessment. January 16, 2020 In determining privacy capabilities, an organization may consider which of the privacy engineering and security objectives are most important with respect to its mission or business needs, risk tolerance, and organizational-level privacy requirements (see Organizing Preparatory Resources above). Not all of the objectives may be equally important, or trade-offs may be necessary among them. Although the privacy capabilities inform the privacy risk assessment by supporting risk prioritization decisions, the privacy capabilities may also be informed by the risk assessment and adjusted to support the management of specific privacy risks or address changes in the environment, including design changes to the system, product, or service. Table 3: Privacy Engineering and Security Objectives18 Objective Definition Principal Related Functions from the Privacy Framework Core Privacy Engineering Objectives Predictability Enabling reliable assumptions by individuals, owners, and operators about data and their processing by a system Identify-P, Govern-P, Control-P, Communicate- P, Protect-P Manageability Providing the capability for granular administration of data, including collection, alteration, deletion, and selective disclosure Identify-P, Govern-P, Control-P Disassociability Enabling the processing of data or events without association to individuals or devices beyond the operational requirements of the system Identify-P, Govern-P, Control-P Security Objectives Confidentiality Preserving authorized restrictions on information access and disclosure, including means for protecting personal privacy and proprietary information Identify-P, Govern-P, Protect-P Integrity Guarding against improper information modification or destruction; includes ensuring information non-repudiation and authenticity Identify-P, Govern-P, Protect-P Availability Ensuring timely and reliable access to and use of information Identify-P, Govern-P, Protect-P Defining Privacy Requirements Privacy requirements specify the way a system, product, or service needs to function to meet stakeholders desired privacy outcomes (e.g., the application is configured to allow users to",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      39
    ],
    "titles": [
      "19  NIST has developed a Privacy Risk Assessment Methodology (PRAM) that can help organizations identify,"
    ],
    "chunk_index": 42,
    "text": "select specific data elements ). To define privacy requirements, consider organizational-level privacy requirements (see Organizing Preparatory Resources above) and the outputs of a privacy risk assessment. This process helps an organization to answer two questions: 1) What can a system, product, or service do with data processing and interactions with individuals? 2) What should it do? Then an organization can allocate resources to design a system, product, or service in a way that achieves the defined requirements. Ultimately, defining privacy requirements can lead to the development of 18 The privacy engineering objectives are adapted from NIST IR 8062 [5]. The security objectives are from NIST SP 800-37, Rev. 2 [7]. January 16, 2020 systems, products, and services that are more mindful of individuals privacy, and are based on informed risk decisions. Conducting Privacy Risk Assessments Conducting a privacy risk assessment helps an organization to identify privacy risks engendered by the system, product, or service and prioritize them to be able to make informed decisions about how to respond to the risks (ID.RA-P, GV.RM-P). Methodologies for conducting privacy risk assessments may vary, but organizations should consider the following characteristics:19 Risk model (ID.RA-P, GV.MT-P1) Risk models define the risk factors to be assessed and the relationships among those factors.20 If an organization is not using a pre-defined risk model, an organization should clearly define which risk factors it will be assessing and the relationships among these factors. Although cybersecurity has a widely used risk model based on the risk factors of threats, vulnerabilities, likelihood, and impact, there is not one commonly accepted privacy risk model. NIST has developed a privacy risk model to calculate risk based on the likelihood of a problematic data action multiplied by the impact of a problematic data action; each of the three risk factors are explained below.",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      40
    ],
    "titles": [
      "23  The NIST PRAM uses organizational costs such as non-compliance costs, direct business costs, reputational"
    ],
    "chunk_index": 43,
    "text": "o A problematic data action is any action a system takes to process data that could result in a problem for individuals. Organizations consider the type of problems that are relevant to the population of individuals. Problems can take any form and may consider the experience of individuals.21 o Likelihood is defined as a contextual analysis that a data action is likely to create a problem for a representative set of individuals. Context can include organizational factors (e.g., geographic location, the public perception about participating organizations with respect to privacy), system factors (e.g., the nature and history of individuals interactions with the system, visibility of data processing to individuals and third parties), or individual factors (e.g., individuals demographics, privacy interests or perceptions, data sensitivity).22 A data map can help with this contextual analysis (see Organizing Preparatory Resources). o Impact is an analysis of the costs should the problem occur. As noted in section 1.2, organizations do not experience these problems directly. Moreover, individuals experiences may be subjective. Thus, impact may be difficult to assess accurately. Organizations should 19 NIST has developed a Privacy Risk Assessment Methodology (PRAM) that can help organizations identify, assess, and respond to privacy risks. It is comprised of a set of worksheets available at [3]. 20 See NIST SP 800-30, Rev. 1, Guide for Conducting Risk Assessments [12] at p. 8. 21 As part of its PRAM, NIST has created an illustrative catalog of problematic data actions and problems for consideration [3]. Other organizations may have created additional problem sets, or may refer to them as adverse consequences or harms. 22 See NIST PRAM for more information about contextual factors. Id. at Worksheet 2. Privacy Risk Factors: Problematic Data Action | Likelihood | Impact January 16, 2020 consider the best means of internalizing impact to individuals",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 44,
    "text": "in order to appropriately prioritize and respond to privacy risks.23 Assessment approach The assessment approach is the mechanism by which identified risks are prioritized. Assessment approaches can be categorized as quantitative, semi-quantitative, or qualitative.24 25 Prioritizing risks (ID.RA-P4) Given the applicable limits of an organization s resources, organizations prioritize the risks to facilitate communication about how to respond.26 Responding to risks (ID.RA-P5) As described in section 1.2.2, response approaches include mitigation, transfer/sharing, avoidance, or acceptance.27 Creating Privacy Requirements Traceability Once an organization has determined which risks to mitigate, it can refine the privacy requirements and then select and implement controls (i.e., technical, physical, and/or policy safeguards) to meet the requirements.28 An organization may use a variety of sources to select controls, such as NIST SP 800-53, Security and Privacy Controls for Information Systems and Organizations.29 After implementation, an organization iteratively assesses the controls for their effectiveness in meeting the privacy requirements and managing privacy risk. In this way, an organization creates traceability between the controls and the privacy requirements, and demonstrates accountability between its systems, products, and services and its organizational privacy goals. Monitoring Change Privacy risk management is not a static process. An organization monitors how changes in its business environment including new laws and regulations and emerging technologies and corresponding changes to its systems, products, and services may be affecting privacy risk, and iteratively uses the practices in this appendix to adjust accordingly. (GV.MT-P1) 23 The NIST PRAM uses organizational costs such as non-compliance costs, direct business costs, reputational costs, and internal culture costs as drivers for considering how to assess individual impact. Id. at Worksheet 3, Impact Tab. 24 See NIST SP 800-30, Rev. 1, Guide for Conducting Risk Assessments at [12] p. 14. 25 The NIST PRAM uses a semi-quantitative approach based on a scale of",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      41
    ],
    "titles": [],
    "chunk_index": 45,
    "text": "1-10. 26 The NIST PRAM provides various prioritization representations, including a heat map. See [3] Worksheet 3. 27 The NIST PRAM provides a process for responding to prioritized privacy risks. Id. at Worksheet 4. 28 See NIST SP 800-37, Rev. 2 [7]. 29 See NIST SP 800-53 as updated [10]. January 16, 2020 Appendix E: Implementation Tiers Definitions The four Tiers summarized below are each defined with four elements: Tier 1: Partial Privacy Risk Management Process Organizational privacy risk management practices are not formalized, and risk is managed in an ad hoc and sometimes reactive manner. Prioritization of privacy activities may not be directly informed by organizational risk management priorities, privacy risk assessments, or mission or business objectives. Integrated Privacy Risk Management Program There is limited awareness of privacy risk at the organizational level. The organization implements privacy risk management on an irregular, case-by-case basis due to varied experience or information gained from outside sources. The organization may not have processes that enable the sharing of information about data processing and resulting privacy risks within the organization. Data Processing Ecosystem Relationships There is limited understanding of an organization s role(s) in the larger ecosystem with respect to other entities (e.g., buyers, suppliers, service providers, business associates, partners). The organization does not have processes for identifying how privacy risks may proliferate throughout the ecosystem or for communicating privacy risks or requirements to other entities in the ecosystem. Workforce Some personnel may have a limited understanding of privacy risks or privacy risk management processes, but have no specific privacy responsibilities. If available, privacy training is ad hoc and the content is not kept current with best practices. Tier 2: Risk Informed Privacy Risk Management Process Risk management practices are approved by management but may not be established as organization-wide policy. Prioritization of",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      42
    ],
    "titles": [],
    "chunk_index": 46,
    "text": "privacy activities is directly informed by organizational risk management priorities, privacy risk assessments, or mission or business objectives. Integrated Privacy Risk Management Program There is an awareness of privacy risk at the organizational level, but an organization-wide approach to managing privacy risk has not been established. Information about data processing and resulting privacy risks is shared within the organization on an informal basis. Consideration of privacy in organizational objectives and programs may occur at some but not all levels of the organization. Privacy risk assessment occurs, but is not typically repeatable or reoccurring. Data Processing Ecosystem Relationships There is some understanding of an organization s role(s) in the larger ecosystem with respect to other entities (e.g., buyers, suppliers, service providers, business associates, partners). The organization is aware of the privacy ecosystem risks associated with the products and services it provides and uses, but does not act consistently or formally upon those risks. Workforce There are personnel with specific privacy responsibilities, but they may have non- privacy responsibilities as well. Privacy training is conducted regularly for privacy personnel, although there is no consistent process for updates on best practices. January 16, 2020 Tier 3: Repeatable Privacy Risk Management Process The organization s risk management practices are formally approved and expressed as policy. Organizational privacy practices are regularly updated based on the application of risk management processes to changes in mission or business objectives and a changing risk, policy, and technology landscape. Integrated Privacy Risk Management Program There is an organization-wide approach to manage privacy risk. Risk-informed policies, processes, and procedures are defined, implemented as intended, and reviewed. Consistent methods are in place to respond effectively to changes in risk. The organization consistently and accurately monitors privacy risk. Senior privacy and non-privacy executives communicate regularly regarding privacy risk. Senior executives ensure consideration",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 47,
    "text": "of privacy through all lines of operation in the organization. Data Processing Ecosystem Relationships The organization understands its role(s), dependencies, and dependents in the larger ecosystem and may contribute to the community s broader understanding of risks. The organization is aware of the privacy ecosystem risks associated with the products and services it provides and it uses. Additionally, it usually acts formally upon those risks, including mechanisms such as written agreements to communicate privacy requirements, governance structures, and policy implementation and monitoring. Workforce Dedicated privacy personnel possess the knowledge and skills to perform their appointed roles and responsibilities. There is regular, up-to-date privacy training for all personnel. Tier 4: Adaptive Privacy Risk Management Process The organization adapts its privacy practices based on lessons learned from privacy events, and identification of new privacy risks. Through a process of continuous improvement incorporating advanced privacy technologies and practices, the organization actively adapts to a changing policy and technology landscape and responds in a timely and effective manner to evolving privacy risks. Integrated Privacy Risk Management Program There is an organization-wide approach to managing privacy risk that uses risk-informed policies, processes, and procedures to address problematic data actions. The relationship between privacy risk and organizational objectives is clearly understood and considered when making decisions. Senior executives monitor privacy risk in the same context as cybersecurity risk, financial risk, and other organizational risks. The organizational budget is based on an understanding of the current and predicted risk environment and risk tolerance. Business units implement executive vision and analyze system- level risks in the context of the organizational risk tolerances. Privacy risk management is part of the organizational culture and evolves from lessons learned and continuous awareness of data processing and resulting privacy risks. The organization can quickly and efficiently account for changes to business/mission objectives",
    "n_words": 300
  },
  {
    "pdf": "nist_privacy_framework_v1.pdf",
    "pages": [
      43
    ],
    "titles": [],
    "chunk_index": 48,
    "text": "in how risk is approached and communicated. Data Processing Ecosystem Relationships The organization understands its role(s), dependencies, and dependents in the larger ecosystem and contributes to the community s broader understanding of risks. The organization uses real-time or near-real-time information to understand and consistently act upon privacy ecosystem risks associated with the products and services it provides and it uses. Additionally, it communicates proactively, using formal (e.g., agreements) and informal mechanisms to develop and maintain strong ecosystem relationships. January 16, 2020 Workforce The organization has specialized privacy skillsets throughout the organizational structure; personnel with diverse perspectives contribute to the management of privacy risks. There is regular, up-to-date, specialized privacy training for all personnel. Personnel at all levels understand the organizational privacy values and their role in maintaining them.",
    "n_words": 128
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      2
    ],
    "titles": [
      "EXPLANATORY MEMORANDUM"
    ],
    "chunk_index": 1,
    "text": "EXPLANATORY MEMORANDUM 1. CONTEXT OF THE PROPOSAL 1.1. Reasons for and objectives of the proposal This explanatory memorandum accompanies the proposal for a Regulation laying down harmonised rules on artificial intelligence (Artificial Intelligence Act). Artificial Intelligence (AI) is a fast evolving family of technologies that can bring a wide array of economic and societal benefits across the entire spectrum of industries and social activities. By improving prediction, optimising operations and resource allocation, and personalising service delivery, the use of artificial intelligence can support socially and environmentally beneficial outcomes and provide key competitive advantages to companies and the European economy. Such action is especially needed in high-impact sectors, including climate change, environment and health, the public sector, finance, mobility, home affairs and agriculture. However, the same elements and techniques that power the socio-economic benefits of AI can also bring about new risks or negative consequences for individuals or the society. In light of the speed of technological change and possible challenges, the EU is committed to strive for a balanced approach. It is in the Union interest to preserve the EU s technological leadership and to ensure that Europeans can benefit from new technologies developed and functioning according to Union values, fundamental rights and principles. This proposal delivers on the political commitment by President von der Leyen, who announced in her political guidelines for the 2019-2024 Commission A Union that strives for more 1, that the Commission would put forward legislation for a coordinated European approach on the human and ethical implications of AI. Following on that announcement, on 19 February 2020 the Commission published the White Paper on AI - A European approach to excellence and trust2. The White Paper sets out policy options on how to achieve the twin objective of promoting the uptake of AI and of addressing",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      3
    ],
    "titles": [
      "2020/2012(INL)."
    ],
    "chunk_index": 2,
    "text": "the risks associated with certain uses of such technology. This proposal aims to implement the second objective for the development of an ecosystem of trust by proposing a legal framework for trustworthy AI. The proposal is based on EU values and fundamental rights and aims to give people and other users the confidence to embrace AI-based solutions, while encouraging businesses to develop them. AI should be a tool for people and be a force for good in society with the ultimate aim of increasing human well-being. Rules for AI available in the Union market or otherwise affecting people in the Union should therefore be human centric, so that people can trust that the technology is used in a way that is safe and compliant with the law, including the respect of fundamental rights. Following the publication of the White Paper, the Commission launched a broad stakeholder consultation, which was met with a great interest by a large number of stakeholders who were largely supportive of regulatory intervention to address the challenges and concerns raised by the increasing use of AI. The proposal also responds to explicit requests from the European Parliament (EP) and the European Council, which have repeatedly expressed calls for legislative action to ensure a well-functioning internal market for artificial intelligence systems ( AI systems ) where both benefits and risks of AI are adequately addressed at Union level. It supports the objective of the Union being a global leader in the development of secure, trustworthy and ethical artificial European Commission, White Paper on Artificial Intelligence - A European approach to excellence and trust, COM(2020) 65 final, 2020. intelligence as stated by the European Council3 and ensures the protection of ethical principles as specifically requested by the European Parliament4. In 2017, the European Council called for a sense",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 3,
    "text": "of urgency to address emerging trends including issues such as artificial intelligence , while at the same time ensuring a high level of data protection, digital rights and ethical standards 5. In its 2019 Conclusions on the Coordinated Plan on the development and use of artificial intelligence Made in Europe6, the Council further highlighted the importance of ensuring that European citizens rights are fully respected and called for a review of the existing relevant legislation to make it fit for purpose for the new opportunities and challenges raised by AI. The European Council has also called for a clear determination of the AI applications that should be considered high-risk7. The most recent Conclusions from 21 October 2020 further called for addressing the opacity, complexity, bias, a certain degree of unpredictability and partially autonomous behaviour of certain AI systems, to ensure their compatibility with fundamental rights and to facilitate the enforcement of legal rules8. The European Parliament has also undertaken a considerable amount of work in the area of AI. In October 2020, it adopted a number of resolutions related to AI, including on ethics9, liability10 and copyright11. In 2021, those were followed by resolutions on AI in criminal matters12 and in education, culture and the audio-visual sector13. The EP Resolution on a Framework of Ethical Aspects of Artificial Intelligence, Robotics and Related Technologies specifically recommends to the Commission to propose legislative action to harness the opportunities and benefits of AI, but also to ensure protection of ethical principles. The resolution includes a text of the legislative proposal for a regulation on ethical principles for the development, deployment and use of AI, robotics and related technologies. In accordance with the political commitment made by President von der Leyen in her Political Guidelines as regards resolutions adopted by the European Parliament under",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      4
    ],
    "titles": [],
    "chunk_index": 4,
    "text": "Article 225 TFEU, this European Council, Special meeting of the European Council (1 and 2 October 2020) Conclusions, EUCO 13/20, 2020, p. 6. European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(INL). European Council, European Council meeting (19 October 2017) Conclusion EUCO 14/17, 2017, p. 8. Council of the European Union, Artificial intelligence b) Conclusions on the coordinated plan on artificial intelligence-Adoption 6177/19, 2019. European Council, Special meeting of the European Council (1and 2 October 2020) Conclusions EUCO 13/20, 2020. Council of the European Union, Presidency conclusions - The Charter of Fundamental Rights in the context of Artificial Intelligence and Digital Change, 11481/20, 2020. European Parliament resolution of 20 October 2020 on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(INL). European Parliament resolution of 20 October 2020 on a civil liability regime for artificial intelligence, 2020/2014(INL). European Parliament resolution of 20 October 2020 on intellectual property rights for the development of artificial intelligence technologies, 2020/2015(INI). European Parliament Draft Report, Artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters, 2020/2016(INI). European Parliament Draft Report, Artificial intelligence in education, culture and the audiovisual sector, 2020/2017(INI). In that regard, the Commission has adopted the Digital Education Action Plan 2021-2027: Resetting education and training for the digital age, which foresees the development of ethical guidelines in AI and Data usage in education Commission Communication COM(2020) 624 final. proposal takes into account the aforementioned resolution of the European Parliament in full respect of proportionality, subsidiarity and better law making principles. Against this political context, the Commission puts forward the proposed regulatory framework on Artificial Intelligence with the following specific objectives: ensure that AI systems placed",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 5,
    "text": "on the Union market and used are safe and respect existing law on fundamental rights and Union values; ensure legal certainty to facilitate investment and innovation in AI; enhance governance and effective enforcement of existing law on fundamental rights and safety requirements applicable to AI systems; facilitate the development of a single market for lawful, safe and trustworthy AI applications and prevent market fragmentation. To achieve those objectives, this proposal presents a balanced and proportionate horizontal regulatory approach to AI that is limited to the minimum necessary requirements to address the risks and problems linked to AI, without unduly constraining or hindering technological development or otherwise disproportionately increasing the cost of placing AI solutions on the market. The proposal sets a robust and flexible legal framework. On the one hand, it is comprehensive and future-proof in its fundamental regulatory choices, including the principle-based requirements that AI systems should comply with. On the other hand, it puts in place a proportionate regulatory system centred on a well-defined risk-based regulatory approach that does not create unnecessary restrictions to trade, whereby legal intervention is tailored to those concrete situations where there is a justified cause for concern or where such concern can reasonably be anticipated in the near future. At the same time, the legal framework includes flexible mechanisms that enable it to be dynamically adapted as the technology evolves and new concerning situations emerge. The proposal sets harmonised rules for the development, placement on the market and use of AI systems in the Union following a proportionate risk-based approach. It proposes a single future-proof definition of AI. Certain particularly harmful AI practices are prohibited as contravening Union values, while specific restrictions and safeguards are proposed in relation to certain uses of remote biometric identification systems for the purpose of law enforcement. The",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      5
    ],
    "titles": [
      "1.2."
    ],
    "chunk_index": 6,
    "text": "proposal lays down a solid risk methodology to define high-risk AI systems that pose significant risks to the health and safety or fundamental rights of persons. Those AI systems will have to comply with a set of horizontal mandatory requirements for trustworthy AI and follow conformity assessment procedures before those systems can be placed on the Union market. Predictable, proportionate and clear obligations are also placed on providers and users of those systems to ensure safety and respect of existing legislation protecting fundamental rights throughout the whole AI systems lifecycle. For some specific AI systems, only minimum transparency obligations are proposed, in particular when chatbots or deep fakes are used. The proposed rules will be enforced through a governance system at Member States level, building on already existing structures, and a cooperation mechanism at Union level with the establishment of a European Artificial Intelligence Board. Additional measures are also proposed to support innovation, in particular through AI regulatory sandboxes and other measures to reduce the regulatory burden and to support Small and Medium-Sized Enterprises ( SMEs ) and start-ups. 1.2. Consistency with existing policy provisions in the policy area The horizontal nature of the proposal requires full consistency with existing Union legislation applicable to sectors where high-risk AI systems are already used or likely to be used in the near future. Consistency is also ensured with the EU Charter of Fundamental Rights and the existing secondary Union legislation on data protection, consumer protection, non-discrimination and gender equality. The proposal is without prejudice and complements the General Data Protection Regulation (Regulation (EU) 2016/679) and the Law Enforcement Directive (Directive (EU) 2016/680) with a set of harmonised rules applicable to the design, development and use of certain high-risk AI systems and restrictions on certain uses of remote biometric identification systems. Furthermore, the",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 7,
    "text": "proposal complements existing Union law on non-discrimination with specific requirements that aim to minimise the risk of algorithmic discrimination, in particular in relation to the design and the quality of data sets used for the development of AI systems complemented with obligations for testing, risk management, documentation and human oversight throughout the AI systems lifecycle. The proposal is without prejudice to the application of Union competition law. As regards high-risk AI systems which are safety components of products, this proposal will be integrated into the existing sectoral safety legislation to ensure consistency, avoid duplications and minimise additional burdens. In particular, as regards high-risk AI systems related to products covered by the New Legislative Framework (NLF) legislation (e.g. machinery, medical devices, toys), the requirements for AI systems set out in this proposal will be checked as part of the existing conformity assessment procedures under the relevant NLF legislation. With regard to the interplay of requirements, while the safety risks specific to AI systems are meant to be covered by the requirements of this proposal, NLF legislation aims at ensuring the overall safety of the final product and therefore may contain specific requirements regarding the safe integration of an AI system into the final product. The proposal for a Machinery Regulation, which is adopted on the same day as this proposal fully reflects this approach. As regards high-risk AI systems related to products covered by relevant Old Approach legislation (e.g. aviation, cars), this proposal would not directly apply. However, the ex-ante essential requirements for high-risk AI systems set out in this proposal will have to be taken into account when adopting relevant implementing or delegated legislation under those acts. As regards AI systems provided or used by regulated credit institutions, the authorities responsible for the supervision of the Union s financial services",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      6
    ],
    "titles": [
      "1.3."
    ],
    "chunk_index": 8,
    "text": "legislation should be designated as competent authorities for supervising the requirements in this proposal to ensure a coherent enforcement of the obligations under this proposal and the Union s financial services legislation where AI systems are to some extent implicitly regulated in relation to the internal governance system of credit institutions. To further enhance consistency, the conformity assessment procedure and some of the providers procedural obligations under this proposal are integrated into the procedures under Directive 2013/36/EU on access to the activity of credit institutions and the prudential supervision14. Directive 2013/36/EU of the European Parliament and of the Council of 26 June 2013 on access to the activity of credit institutions and the prudential supervision of credit institutions and investment firms, amending Directive 2002/87/EC and repealing Directives 2006/48/EC and 2006/49/EC Text with EEA relevance, OJ L 176, 27.6.2013, p. 338 436. This proposal is also consistent with the applicable Union legislation on services, including on intermediary services regulated by the e-Commerce Directive 2000/31/EC15 and the Commission s recent proposal for the Digital Services Act (DSA)16. In relation to AI systems that are components of large-scale IT systems in the Area of Freedom, Security and Justice managed by the European Union Agency for the Operational Management of Large-Scale IT Systems (eu-LISA), the proposal will not apply to those AI systems that have been placed on the market or put into service before one year has elapsed from the date of application of this Regulation, unless the replacement or amendment of those legal acts leads to a significant change in the design or intended purpose of the AI system or AI systems concerned. 1.3. Consistency with other Union policies The proposal is part of a wider comprehensive package of measures that address problems posed by the development and use of AI, as",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 9,
    "text": "examined in the White Paper on AI. Consistency and complementarity is therefore ensured with other ongoing or planned initiatives of the Commission that also aim to address those problems, including the revision of sectoral product legislation (e.g. the Machinery Directive, the General Product Safety Directive) and initiatives that address liability issues related to new technologies, including AI systems. Those initiatives will build on and complement this proposal in order to bring legal clarity and foster the development of an ecosystem of trust in AI in Europe. The proposal is also coherent with the Commission s overall digital strategy in its contribution to promoting technology that works for people, one of the three main pillars of the policy orientation and objectives announced in the Communication Shaping Europe's digital future 17. It lays down a coherent, effective and proportionate framework to ensure AI is developed in ways that respect people s rights and earn their trust, making Europe fit for the digital age and turning the next ten years into the Digital Decade18. Furthermore, the promotion of AI-driven innovation is closely linked to the Data Governance Act19, the Open Data Directive20 and other initiatives under the EU strategy for data21, which will establish trusted mechanisms and services for the re-use, sharing and pooling of data that are essential for the development of data-driven AI models of high quality. The proposal also strengthens significantly the Union s role to help shape global norms and standards and promote trustworthy AI that is consistent with Union values and interests. It provides the Union with a powerful basis to engage further with its external partners, including third countries, and at international fora on issues relating to AI. Directive 2000/31/EC of the European Parliament and of the Council of 8 June 2000 on certain legal aspects of",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      7
    ],
    "titles": [
      "2."
    ],
    "chunk_index": 10,
    "text": "information society services, in particular electronic commerce, in the Internal Market ('Directive on electronic commerce'), OJ L 178, 17.7.2000, p. 1 16. See Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL on a Single Market For Digital Services (Digital Services Act) and amending Directive 2000/31/EC COM/2020/825 final. Communication from the Commission, Shaping Europe's Digital Future, COM/2020/67 final. 2030 Digital Compass: the European way for the Digital Decade. Proposal for a Regulation on European data governance (Data Governance Act) COM/2020/767. Directive (EU) 2019/1024 of the European Parliament and of the Council of 20 June 2019 on open data and the re-use of public sector information, PE/28/2019/REV/1, OJ L 172, 26.6.2019, p. 56 83. Commission Communication, A European strategy for data COM/2020/66 final. 2. LEGAL BASIS, SUBSIDIARITY AND PROPORTIONALITY 2.1. Legal basis The legal basis for the proposal is in the first place Article 114 of the Treaty on the Functioning of the European Union (TFEU), which provides for the adoption of measures to ensure the establishment and functioning of the internal market. This proposal constitutes a core part of the EU digital single market strategy. The primary objective of this proposal is to ensure the proper functioning of the internal market by setting harmonised rules in particular on the development, placing on the Union market and the use of products and services making use of AI technologies or provided as stand-alone AI systems. Some Member States are already considering national rules to ensure that AI is safe and is developed and used in compliance with fundamental rights obligations. This will likely lead to two main problems: i) a fragmentation of the internal market on essential elements regarding in particular the requirements for the AI products and services, their marketing, their use, the liability and the supervision by",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 11,
    "text": "public authorities, and ii) the substantial diminishment of legal certainty for both providers and users of AI systems on how existing and new rules will apply to those systems in the Union. Given the wide circulation of products and services across borders, these two problems can be best solved through EU harmonizing legislation. Indeed, the proposal defines common mandatory requirements applicable to the design and development of certain AI systems before they are placed on the market that will be further operationalised through harmonised technical standards. The proposal also addresses the situation after AI systems have been placed on the market by harmonising the way in which ex-post controls are conducted. In addition, considering that this proposal contains certain specific rules on the protection of individuals with regard to the processing of personal data, notably restrictions of the use of AI systems for real-time remote biometric identification in publicly accessible spaces for the purpose of law enforcement, it is appropriate to base this regulation, in as far as those specific rules are concerned, on Article 16 of the TFEU. 2.2. Subsidiarity (for non-exclusive competence) The nature of AI, which often relies on large and varied datasets and which may be embedded in any product or service circulating freely within the internal market, entails that the objectives of this proposal cannot be effectively achieved by Member States alone. Furthermore, an emerging patchwork of potentially divergent national rules will hamper the seamless circulation of products and services related to AI systems across the EU and will be ineffective in ensuring the safety and protection of fundamental rights and Union values across the different Member States. National approaches in addressing the problems will only create additional legal uncertainty and barriers, and will slow market uptake of AI. The objectives of this proposal can",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      8
    ],
    "titles": [
      "2.3."
    ],
    "chunk_index": 12,
    "text": "be better achieved at Union level to avoid a further fragmentation of the Single Market into potentially contradictory national frameworks preventing the free circulation of goods and services embedding AI. A solid European regulatory framework for trustworthy AI will also ensure a level playing field and protect all people, while strengthening Europe s competitiveness and industrial basis in AI. Only common action at Union level can also protect the Union s digital sovereignty and leverage its tools and regulatory powers to shape global rules and standards. 2.3. Proportionality The proposal builds on existing legal frameworks and is proportionate and necessary to achieve its objectives, since it follows a risk-based approach and imposes regulatory burdens only when an AI system is likely to pose high risks to fundamental rights and safety. For other, non-high-risk AI systems, only very limited transparency obligations are imposed, for example in terms of the provision of information to flag the use of an AI system when interacting with humans. For high-risk AI systems, the requirements of high quality data, documentation and traceability, transparency, human oversight, accuracy and robustness, are strictly necessary to mitigate the risks to fundamental rights and safety posed by AI and that are not covered by other existing legal frameworks. Harmonised standards and supporting guidance and compliance tools will assist providers and users in complying with the requirements laid down by the proposal and minimise their costs. The costs incurred by operators are proportionate to the objectives achieved and the economic and reputational benefits that operators can expect from this proposal. 2.4. Choice of the instrument The choice of a regulation as a legal instrument is justified by the need for a uniform application of the new rules, such as definition of AI, the prohibition of certain harmful AI- enabled practices and the",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      9
    ],
    "titles": [
      "27. Depending on the question, between 81 and 598 of the respondents used the free text"
    ],
    "chunk_index": 13,
    "text": "classification of certain AI systems. The direct applicability of a Regulation, in accordance with Article 288 TFEU, will reduce legal fragmentation and facilitate the development of a single market for lawful, safe and trustworthy AI systems. It will do so, in particular, by introducing a harmonised set of core requirements with regard to AI systems classified as high-risk and obligations for providers and users of those systems, improving the protection of fundamental rights and providing legal certainty for operators and consumers alike. At the same time, the provisions of the regulation are not overly prescriptive and leave room for different levels of Member State action for elements that do not undermine the objectives of the initiative, in particular the internal organisation of the market surveillance system and the uptake of measures to foster innovation. 3. RESULTS OF EX-POST EVALUATIONS, STAKEHOLDER CONSULTATIONS AND IMPACT ASSESSMENTS 3.1. Stakeholder consultation This proposal is the result of extensive consultation with all major stakeholders, in which the general principles and minimum standards for consultation of interested parties by the Commission were applied. An online public consultation was launched on 19 February 2020 along with the publication of the White Paper on Artificial Intelligence and ran until 14 June 2020. The objective of that consultation was to collect views and opinions on the White Paper. It targeted all interested stakeholders from the public and private sectors, including governments, local authorities, commercial and non-commercial organisations, social partners, experts, academics and citizens. After analysing all the responses received, the Commission published a summary outcome and the individual responses on its website22. In total, 1215 contributions were received, of which 352 were from companies or business organisations/associations, 406 from individuals (92%individuals from EU ), 152 on behalf of See all consultation results here. academic/research institutions, and 73 from public",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 14,
    "text": "authorities. Civil society s voices were represented by 160 respondents (among which 9 consumers organisations, 129 non- governmental organisations and 22 trade unions), 72 respondents contributed as others . Of the 352 business and industry representatives, 222 were companies and business representatives, 41.5% of which were micro, small and medium-sized enterprises. The rest were business associations. Overall, 84% of business and industry replies came from the EU- 27. Depending on the question, between 81 and 598 of the respondents used the free text option to insert comments. Over 450 position papers were submitted through the EU Survey website, either in addition to questionnaire answers (over 400) or as stand-alone contributions (over 50). Overall, there is a general agreement amongst stakeholders on a need for action. A large majority of stakeholders agree that legislative gaps exist or that new legislation is needed. However, several stakeholders warn the Commission to avoid duplication, conflicting obligations and overregulation. There were many comments underlining the importance of a technology neutral and proportionate regulatory framework. Stakeholders mostly requested a narrow, clear and precise definition for AI. Stakeholders also highlighted that besides the clarification of the term of AI, it is important to define risk , high-risk , low-risk , remote biometric identification and harm . Most of the respondents are explicitly in favour of the risk-based approach. Using a risk-based framework was considered a better option than blanket regulation of all AI systems. The types of risks and threats should be based on a sector-by-sector and case-by-case approach. Risks also should be calculated taking into account the impact on rights and safety. Regulatory sandboxes could be very useful for the promotion of AI and are welcomed by certain stakeholders, especially the Business Associations. Among those who formulated their opinion on the enforcement models, more than 50%,",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      10
    ],
    "titles": [
      "130 comments27. Additional stakeholder workshops and events were also organised the"
    ],
    "chunk_index": 15,
    "text": "especially from the business associations, were in favour of a combination of an ex-ante risk self-assessment and an ex-post enforcement for high-risk AI systems. 3.2. Collection and use of expertise The proposal builds on two years of analysis and close involvement of stakeholders, including academics, businesses, social partners, non-governmental organisations, Member States and citizens. The preparatory work started in 2018 with the setting up of a High-Level Expert Group on AI (HLEG) which had an inclusive and broad composition of 52 well-known experts tasked to advise the Commission on the implementation of the Commission s Strategy on Artificial Intelligence. In April 2019, the Commission supported23 the key requirements set out in the HLEG ethics guidelines for Trustworthy AI24, which had been revised to take into account more than 500 submissions from stakeholders. The key requirements reflect a widespread and common approach, as evidenced by a plethora of ethical codes and principles developed by many private and public organisations in Europe and beyond, that AI development and use should be guided by certain essential value-oriented principles. The Assessment List for Trustworthy Artificial Intelligence (ALTAI)25 made those requirements operational in a piloting process with over 350 organisations. European Commission, Building Trust in Human-Centric Artificial Intelligence, COM(2019) 168. HLEG, Ethics Guidelines for Trustworthy AI, 2019. HLEG, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment, 2020. In addition, the AI Alliance26 was formed as a platform for approximately 4000 stakeholders to debate the technological and societal implications of AI, culminating in a yearly AI Assembly. The White Paper on AI further developed this inclusive approach, inciting comments from more than 1250 stakeholders, including over 450 additional position papers. As a result, the Commission published an Inception Impact Assessment, which in turn attracted more than 130 comments27. Additional stakeholder workshops and events were also",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 16,
    "text": "organised the results of which support the analysis in the impact assessment and the policy choices made in this proposal28. An external study was also procured to feed into the impact assessment. 3.3. Impact assessment In line with its Better Regulation policy, the Commission conducted an impact assessment for this proposal examined by the Commission's Regulatory Scrutiny Board. A meeting with the Regulatory Scrutiny Board was held on 16 December 2020, which was followed by a negative opinion. After substantial revision of the impact assessment to address the comments and a resubmission of the impact assessment, the Regulatory Scrutiny Board issued a positive opinion on 21 March 2021. The opinions of the Regulatory Scrutiny Board, the recommendations and an explanation of how they have been taken into account are presented in Annex 1 of the impact assessment. The Commission examined different policy options to achieve the general objective of the proposal, which is to ensure the proper functioning of the single market by creating the conditions for the development and use of trustworthy AI in the Union. Four policy options of different degrees of regulatory intervention were assessed: Option 1: EU legislative instrument setting up a voluntary labelling scheme; Option 2: a sectoral, ad-hoc approach; Option 3: Horizontal EU legislative instrument following a proportionate risk- based approach; Option 3+: Horizontal EU legislative instrument following a proportionate risk- based approach + codes of conduct for non-high-risk AI systems; Option 4: Horizontal EU legislative instrument establishing mandatory requirements for all AI systems, irrespective of the risk they pose. According to the Commission's established methodology, each policy option was evaluated against economic and societal impacts, with a particular focus on impacts on fundamental rights. The preferred option is option 3+, a regulatory framework for high-risk AI systems only, with the possibility for all",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      11
    ],
    "titles": [
      "3.4."
    ],
    "chunk_index": 17,
    "text": "providers of non-high-risk AI systems to follow a code of conduct. The requirements will concern data, documentation and traceability, provision of information and transparency, human oversight and robustness and accuracy and would be mandatory for high-risk AI systems. Companies that introduced codes of conduct for other AI systems would do so voluntarily. The AI Alliance is a multi-stakeholder forum launched in June 2018, AI Alliance European Commission, Inception Impact Assessment For a Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence. For details of all the consultations that have been carried out see Annex 2 of the impact assessment. The preferred option was considered suitable to address in the most effective way the objectives of this proposal. By requiring a restricted yet effective set of actions from AI developers and users, the preferred option limits the risks of violation of fundamental rights and safety of people and foster effective supervision and enforcement, by targeting the requirements only to systems where there is a high risk that such violations could occur. As a result, that option keeps compliance costs to a minimum, thus avoiding an unnecessary slowing of uptake due to higher prices and compliance costs. In order to address possible disadvantages for SMEs, this option includes several provisions to support their compliance and reduce their costs, including creation of regulatory sandboxes and obligation to consider SMEs interests when setting fees related to conformity assessment. The preferred option will increase people s trust in AI, companies will gain in legal certainty, and Member States will see no reason to take unilateral action that could fragment the single market. As a result of higher demand due to higher trust, more available offers due to legal certainty, and the absence of obstacles to cross-border",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 18,
    "text": "movement of AI systems, the single market for AI will likely flourish. The European Union will continue to develop a fast- growing AI ecosystem of innovative services and products embedding AI technology or stand-alone AI systems, resulting in increased digital autonomy. Businesses or public authorities that develop or use AI applications that constitute a high risk for the safety or fundamental rights of citizens would have to comply with specific requirements and obligations. Compliance with these requirements would imply costs amounting to approximately EUR 6000 to EUR 7000 for the supply of an average high- risk AI system of around EUR 170000 by 2025. For AI users, there would also be the annual cost for the time spent on ensuring human oversight where this is appropriate, depending on the use case. Those have been estimated at approximately EUR 5000 to EUR 8000 per year. Verification costs could amount to another EUR 3000 to EUR 7500 for suppliers of high-risk AI. Businesses or public authorities that develop or use any AI applications not classified as high risk would only have minimal obligations of information. However, they could choose to join others and together adopt a code of conduct to follow suitable requirements, and to ensure that their AI systems are trustworthy. In such a case, costs would be at most as high as for high-risk AI systems, but most probably lower. The impacts of the policy options on different categories of stakeholders (economic operators/ business; conformity assessment bodies, standardisation bodies and other public bodies; individuals/citizens; researchers) are explained in detail in Annex 3 of the Impact assessment supporting this proposal. 3.4. Regulatory fitness and simplification This proposal lays down obligation that will apply to providers and users of high-risk AI systems. For providers who develop and place such systems on the",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      12
    ],
    "titles": [
      "3.5."
    ],
    "chunk_index": 19,
    "text": "Union market, it will create legal certainty and ensure that no obstacle to the cross-border provision of AI-related services and products emerge. For companies using AI, it will promote trust among their customers. For national public administrations, it will promote public trust in the use of AI and strengthen enforcement mechanisms (by introducing a European coordination mechanism, providing for appropriate capacities, and facilitating audits of the AI systems with new requirements for documentation, traceability and transparency). Moreover, the framework will envisage specific measures supporting innovation, including regulatory sandboxes and specific measures supporting small-scale users and providers of high-risk AI systems to comply with the new rules. The proposal also specifically aims at strengthening Europe s competitiveness and industrial basis in AI. Full consistency is ensured with existing sectoral Union legislation applicable to AI systems (e.g. on products and services) that will bring further clarity and simplify the enforcement of the new rules. 3.5. Fundamental rights The use of AI with its specific characteristics (e.g. opacity, complexity, dependency on data, autonomous behaviour) can adversely affect a number of fundamental rights enshrined in the EU Charter of Fundamental Rights ( the Charter ). This proposal seeks to ensure a high level of protection for those fundamental rights and aims to address various sources of risks through a clearly defined risk-based approach. With a set of requirements for trustworthy AI and proportionate obligations on all value chain participants, the proposal will enhance and promote the protection of the rights protected by the Charter: the right to human dignity (Article 1), respect for private life and protection of personal data (Articles 7 and 8), non- discrimination (Article 21) and equality between women and men (Article 23). It aims to prevent a chilling effect on the rights to freedom of expression (Article 11) and freedom",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 20,
    "text": "of assembly (Article 12), to ensure protection of the right to an effective remedy and to a fair trial, the rights of defence and the presumption of innocence (Articles 47 and 48), as well as the general principle of good administration. Furthermore, as applicable in certain domains, the proposal will positively affect the rights of a number of special groups, such as the workers rights to fair and just working conditions (Article 31), a high level of consumer protection (Article 28), the rights of the child (Article 24) and the integration of persons with disabilities (Article 26). The right to a high level of environmental protection and the improvement of the quality of the environment (Article 37) is also relevant, including in relation to the health and safety of people. The obligations for ex ante testing, risk management and human oversight will also facilitate the respect of other fundamental rights by minimising the risk of erroneous or biased AI-assisted decisions in critical areas such as education and training, employment, important services, law enforcement and the judiciary. In case infringements of fundamental rights still happen, effective redress for affected persons will be made possible by ensuring transparency and traceability of the AI systems coupled with strong ex post controls. This proposal imposes some restrictions on the freedom to conduct business (Article 16) and the freedom of art and science (Article 13) to ensure compliance with overriding reasons of public interest such as health, safety, consumer protection and the protection of other fundamental rights ( responsible innovation ) when high-risk AI technology is developed and used. Those restrictions are proportionate and limited to the minimum necessary to prevent and mitigate serious safety risks and likely infringements of fundamental rights. The increased transparency obligations will also not disproportionately affect the right to protection",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      13
    ],
    "titles": [
      "5."
    ],
    "chunk_index": 21,
    "text": "of intellectual property (Article 17(2)), since they will be limited only to the minimum necessary information for individuals to exercise their right to an effective remedy and to the necessary transparency towards supervision and enforcement authorities, in line with their mandates. Any disclosure of information will be carried out in compliance with relevant legislation in the field, including Directive 2016/943 on the protection of undisclosed know-how and business information (trade secrets) against their unlawful acquisition, use and disclosure. When public authorities and notified bodies need to be given access to confidential information or source code to examine compliance with substantial obligations, they are placed under binding confidentiality obligations. 4. BUDGETARY IMPLICATIONS Member States will have to designate supervisory authorities in charge of implementing the legislative requirements. Their supervisory function could build on existing arrangements, for example regarding conformity assessment bodies or market surveillance, but would require sufficient technological expertise and human and financial resources. Depending on the pre- existing structure in each Member State, this could amount to 1 to 25 Full Time Equivalents per Member State. A detailed overview of the costs involved is provided in the financial statement linked to this proposal. 5. OTHER ELEMENTS 5.1. Implementation plans and monitoring, evaluation and reporting arrangements Providing for a robust monitoring and evaluation mechanism is crucial to ensure that the proposal will be effective in achieving its specific objectives. The Commission will be in charge of monitoring the effects of the proposal. It will establish a system for registering stand-alone high-risk AI applications in a public EU-wide database. This registration will also enable competent authorities, users and other interested people to verify if the high-risk AI system complies with the requirements laid down in the proposal and to exercise enhanced oversight over those AI systems posing high risks to fundamental",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 22,
    "text": "rights. To feed this database, AI providers will be obliged to provide meaningful information about their systems and the conformity assessment carried out on those systems. Moreover, AI providers will be obliged to inform national competent authorities about serious incidents or malfunctioning that constitute a breach of fundamental rights obligations as soon as they become aware of them, as well as any recalls or withdrawals of AI systems from the market. National competent authorities will then investigate the incidents/or malfunctioning, collect all the necessary information and regularly transmit it to the Commission with adequate metadata. The Commission will complement this information on the incidents by a comprehensive analysis of the overall market for AI. The Commission will publish a report evaluating and reviewing the proposed AI framework five years following the date on which it becomes applicable. 5.2. Detailed explanation of the specific provisions of the proposal 5.2.1. SCOPE AND DEFINITIONS (TITLE I) Title I defines the subject matter of the regulation and the scope of application of the new rules that cover the placing on the market, putting into service and use of AI systems. It also sets out the definitions used throughout the instrument. The definition of AI system in the legal framework aims to be as technology neutral and future proof as possible, taking into account the fast technological and market developments related to AI. In order to provide the needed legal certainty, Title I is complemented by Annex I, which contains a detailed list of approaches and techniques for the development of AI to be adapted by the Commission in line with new technological developments. Key participants across the AI value chain are also clearly defined such as providers and users of AI systems that cover both public and private operators to ensure a level playing",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      14
    ],
    "titles": [
      "5.2.3."
    ],
    "chunk_index": 23,
    "text": "field. 5.2.2. PROHIBITED ARTIFICIAL INTELLIGENCE PRACTICES (TITLE II) Title II establishes a list of prohibited AI. The regulation follows a risk-based approach, differentiating between uses of AI that create (i) an unacceptable risk, (ii) a high risk, and (iii) low or minimal risk. The list of prohibited practices in Title II comprises all those AI systems whose use is considered unacceptable as contravening Union values, for instance by violating fundamental rights. The prohibitions covers practices that have a significant potential to manipulate persons through subliminal techniques beyond their consciousness or exploit vulnerabilities of specific vulnerable groups such as children or persons with disabilities in order to materially distort their behaviour in a manner that is likely to cause them or another person psychological or physical harm. Other manipulative or exploitative practices affecting adults that might be facilitated by AI systems could be covered by the existing data protection, consumer protection and digital service legislation that guarantee that natural persons are properly informed and have free choice not to be subject to profiling or other practices that might affect their behaviour. The proposal also prohibits AI-based social scoring for general purposes done by public authorities. Finally, the use of real time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement is also prohibited unless certain limited exceptions apply. 5.2.3. HIGH-RISK AI SYSTEMS (TITLE III) Title III contains specific rules for AI systems that create a high risk to the health and safety or fundamental rights of natural persons. In line with a risk-based approach, those high-risk AI systems are permitted on the European market subject to compliance with certain mandatory requirements and an ex-ante conformity assessment. The classification of an AI system as high-risk is based on the intended purpose of the AI system, in line",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 24,
    "text": "with existing product safety legislation. Therefore, the classification as high-risk does not only depend on the function performed by the AI system, but also on the specific purpose and modalities for which that system is used. Chapter 1 of Title III sets the classification rules and identifies two main categories of high- risk AI systems: AI systems intended to be used as safety component of products that are subject to third party ex-ante conformity assessment; other stand-alone AI systems with mainly fundamental rights implications that are explicitly listed in Annex III. This list of high-risk AI systems in Annex III contains a limited number of AI systems whose risks have already materialised or are likely to materialise in the near future. To ensure that the regulation can be adjusted to emerging uses and applications of AI, the Commission may expand the list of high-risk AI systems used within certain pre-defined areas, by applying a set of criteria and risk assessment methodology. Chapter 2 sets out the legal requirements for high-risk AI systems in relation to data and data governance, documentation and recording keeping, transparency and provision of information to users, human oversight, robustness, accuracy and security. The proposed minimum requirements are already state-of-the-art for many diligent operators and the result of two years of preparatory work, derived from the Ethics Guidelines of the HLEG29, piloted by more than 350 organisations30. They are also largely consistent with other international recommendations and principles, which ensures that the proposed AI framework is compatible with those adopted by the EU s international trade partners. The precise technical solutions to achieve compliance with those requirements may be provided by standards or by other technical specifications or otherwise be developed in accordance with general engineering or scientific knowledge at the discretion of the provider of the",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      15
    ],
    "titles": [
      "5.2.4."
    ],
    "chunk_index": 25,
    "text": "AI system. This flexibility is particularly important, because it allows providers of AI systems to choose the High-Level Expert Group on Artificial Intelligence, Ethics Guidelines for Trustworthy AI, 2019. They were also endorsed by the Commission in its 2019 Communication on human-centric approach to AI. way to meet their requirements, taking into account the state-of-the-art and technological and scientific progress in this field. Chapter 3 places a clear set of horizontal obligations on providers of high-risk AI systems. Proportionate obligations are also placed on users and other participants across the AI value chain (e.g., importers, distributors, authorized representatives). Chapter 4 sets the framework for notified bodies to be involved as independent third parties in conformity assessment procedures, while Chapter 5 explains in detail the conformity assessment procedures to be followed for each type of high-risk AI system. The conformity assessment approach aims to minimise the burden for economic operators as well as for notified bodies, whose capacity needs to be progressively ramped up over time. AI systems intended to be used as safety components of products that are regulated under the New Legislative Framework legislation (e.g. machinery, toys, medical devices, etc.) will be subject to the same ex-ante and ex-post compliance and enforcement mechanisms of the products of which they are a component. The key difference is that the ex-ante and ex-post mechanisms will ensure compliance not only with the requirements established by sectorial legislation, but also with the requirements established by this regulation. As regards stand-alone high-risk AI systems that are referred to in Annex III, a new compliance and enforcement system will be established. This follows the model of the New Legislative Framework legislation implemented through internal control checks by the providers with the exception of remote biometric identification systems that would be subject to third party conformity",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 26,
    "text": "assessment. A comprehensive ex-ante conformity assessment through internal checks, combined with a strong ex-post enforcement, could be an effective and reasonable solution for those systems, given the early phase of the regulatory intervention and the fact the AI sector is very innovative and expertise for auditing is only now being accumulated. An assessment through internal checks for stand-alone high-risk AI systems would require a full, effective and properly documented ex ante compliance with all requirements of the regulation and compliance with robust quality and risk management systems and post-market monitoring. After the provider has performed the relevant conformity assessment, it should register those stand-alone high-risk AI systems in an EU database that will be managed by the Commission to increase public transparency and oversight and strengthen ex post supervision by competent authorities. By contrast, for reasons of consistency with the existing product safety legislation, the conformity assessments of AI systems that are safety components of products will follow a system with third party conformity assessment procedures already established under the relevant sectoral product safety legislation. New ex ante re-assessments of the conformity will be needed in case of substantial modifications to the AI systems (and notably changes which go beyond what is pre-determined by the provider in its technical documentation and checked at the moment of the ex-ante conformity assessment). 5.2.4. TRANSPARENCY OBLIGATIONS FOR CERTAIN AI SYSTEMS (TITLE IV) Title IV concerns certain AI systems to take account of the specific risks of manipulation they pose. Transparency obligations will apply for systems that (i) interact with humans, (ii) are used to detect emotions or determine association with (social) categories based on biometric data, or (iii) generate or manipulate content ( deep fakes ). When persons interact with an AI system or their emotions or characteristics are recognised through automated means,",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      16
    ],
    "titles": [
      "5.2.5."
    ],
    "chunk_index": 27,
    "text": "people must be informed of that circumstance. If an AI system is used to generate or manipulate image, audio or video content that appreciably resembles authentic content, there should be an obligation to disclose that the content is generated through automated means, subject to exceptions for legitimate purposes (law enforcement, freedom of expression). This allows persons to make informed choices or step back from a given situation. 5.2.5. MEASURES IN SUPPORT OF INNOVATION (TITLE V) Title V contributes to the objective to create a legal framework that is innovation-friendly, future-proof and resilient to disruption. To that end, it encourages national competent authorities to set up regulatory sandboxes and sets a basic framework in terms of governance, supervision and liability. AI regulatory sandboxes establish a controlled environment to test innovative technologies for a limited time on the basis of a testing plan agreed with the competent authorities. Title V also contains measures to reduce the regulatory burden on SMEs and start-ups. 5.2.6. GOVERNANCE AND IMPLEMENTATION (TITLES VI, VII AND VII) Title VI sets up the governance systems at Union and national level. At Union level, the proposal establishes a European Artificial Intelligence Board (the Board ), composed of representatives from the Member States and the Commission. The Board will facilitate a smooth, effective and harmonised implementation of this regulation by contributing to the effective cooperation of the national supervisory authorities and the Commission and providing advice and expertise to the Commission. It will also collect and share best practices among the Member States. At national level, Member States will have to designate one or more national competent authorities and, among them, the national supervisory authority, for the purpose of supervising the application and implementation of the regulation. The European Data Protection Supervisor will act as the competent authority for the supervision",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 28,
    "text": "of the Union institutions, agencies and bodies when they fall within the scope of this regulation. Title VII aims to facilitate the monitoring work of the Commission and national authorities through the establishment of an EU-wide database for stand-alone high-risk AI systems with mainly fundamental rights implications. The database will be operated by the Commission and provided with data by the providers of the AI systems, who will be required to register their systems before placing them on the market or otherwise putting them into service. Title VIII sets out the monitoring and reporting obligations for providers of AI systems with regard to post-market monitoring and reporting and investigating on AI-related incidents and malfunctioning. Market surveillance authorities would also control the market and investigate compliance with the obligations and requirements for all high-risk AI systems already placed on the market. Market surveillance authorities would have all powers under Regulation (EU) 2019/1020 on market surveillance. Ex-post enforcement should ensure that once the AI system has been put on the market, public authorities have the powers and resources to intervene in case AI systems generate unexpected risks, which warrant rapid action. They will also monitor compliance of operators with their relevant obligations under the regulation. The proposal does not foresee the automatic creation of any additional bodies or authorities at Member State level. Member States may therefore appoint (and draw upon the expertise of) existing sectorial authorities, who would be entrusted also with the powers to monitor and enforce the provisions of the regulation. All this is without prejudice to the existing system and allocation of powers of ex-post enforcement of obligations regarding fundamental rights in the Member States. When necessary for their mandate, existing supervision and enforcement authorities will also have the power to request and access any documentation maintained following",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      17,
      18
    ],
    "titles": [
      "5.2.7.",
      "2021/0106 (COD)"
    ],
    "chunk_index": 29,
    "text": "this regulation and, where needed, request market surveillance authorities to organise testing of the high-risk AI system through technical means. 5.2.7. CODES OF CONDUCT (TITLE IX) Title IX creates a framework for the creation of codes of conduct, which aim to encourage providers of non-high-risk AI systems to apply voluntarily the mandatory requirements for high-risk AI systems (as laid out in Title III). Providers of non-high-risk AI systems may create and implement the codes of conduct themselves. Those codes may also include voluntary commitments related, for example, to environmental sustainability, accessibility for persons with disability, stakeholders participation in the design and development of AI systems, and diversity of development teams. 5.2.8. FINAL PROVISIONS (TITLES X, XI AND XII) Title X emphasizes the obligation of all parties to respect the confidentiality of information and data and sets out rules for the exchange of information obtained during the implementation of the regulation. Title X also includes measures to ensure the effective implementation of the regulation through effective, proportionate, and dissuasive penalties for infringements of the provisions. Title XI sets out rules for the exercise of delegation and implementing powers. The proposal empowers the Commission to adopt, where appropriate, implementing acts to ensure uniform application of the regulation or delegated acts to update or complement the lists in Annexes I to VII. Title XII contains an obligation for the Commission to assess regularly the need for an update of Annex III and to prepare regular reports on the evaluation and review of the regulation. It also lays down final provisions, including a differentiated transitional period for the initial date of the applicability of the regulation to facilitate the smooth implementation for all parties concerned. 2021/0106 (COD) Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL LAYING DOWN HARMONISED RULES ON",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 30,
    "text": "ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLATIVE ACTS THE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION, Having regard to the Treaty on the Functioning of the European Union, and in particular Articles 16 and 114 thereof, Having regard to the proposal from the European Commission, After transmission of the draft legislative act to the national parliaments, Having regard to the opinion of the European Economic and Social Committee31, Having regard to the opinion of the Committee of the Regions32, Acting in accordance with the ordinary legislative procedure, Whereas: (1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, marketing and use of artificial intelligence in conformity with Union values. This Regulation pursues a number of overriding reasons of public interest, such as a high level of protection of health, safety and fundamental rights, and it ensures the free movement of AI-based goods and services cross-border, thus preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by this Regulation. (2) Artificial intelligence systems (AI systems) can be easily deployed in multiple sectors of the economy and society, including cross border, and circulate throughout the Union. Certain Member States have already explored the adoption of national rules to ensure that artificial intelligence is safe and is developed and used in compliance with fundamental rights obligations. Differing national rules may lead to fragmentation of the internal market and decrease legal certainty for operators that develop or use AI systems. A consistent and high level of protection throughout the Union should therefore be ensured, while divergences hampering the free circulation of AI systems and related products and services within the internal market should",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      19
    ],
    "titles": [
      "2020/2012(INL)."
    ],
    "chunk_index": 31,
    "text": "be prevented, by laying down uniform obligations for operators and guaranteeing the uniform protection of overriding reasons of public interest and of rights of persons throughout the internal market based on Article 114 of the Treaty on the Functioning of the European Union (TFEU). To the extent that this Regulation contains specific rules on the protection of individuals with regard to the processing of personal data concerning OJ C [ ], [ ], p. [ ]. OJ C [ ], [ ], p. [ ]. restrictions of the use of AI systems for real-time remote biometric identification in publicly accessible spaces for the purpose of law enforcement, it is appropriate to base this Regulation, in as far as those specific rules are concerned, on Article 16 of the TFEU. In light of those specific rules and the recourse to Article 16 TFEU, it is appropriate to consult the European Data Protection Board. (3) Artificial intelligence is a fast evolving family of technologies that can contribute to a wide array of economic and societal benefits across the entire spectrum of industries and social activities. By improving prediction, optimising operations and resource allocation, and personalising digital solutions available for individuals and organisations, the use of artificial intelligence can provide key competitive advantages to companies and support socially and environmentally beneficial outcomes, for example in healthcare, farming, education and training, infrastructure management, energy, transport and logistics, public services, security, justice, resource and energy efficiency, and climate change mitigation and adaptation. (4) At the same time, depending on the circumstances regarding its specific application and use, artificial intelligence may generate risks and cause harm to public interests and rights that are protected by Union law. Such harm might be material or immaterial. (5) A Union legal framework laying down harmonised rules on artificial intelligence",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 32,
    "text": "is therefore needed to foster the development, use and uptake of artificial intelligence in the internal market that at the same time meets a high level of protection of public interests, such as health and safety and the protection of fundamental rights, as recognised and protected by Union law. To achieve that objective, rules regulating the placing on the market and putting into service of certain AI systems should be laid down, thus ensuring the smooth functioning of the internal market and allowing those systems to benefit from the principle of free movement of goods and services. By laying down those rules, this Regulation supports the objective of the Union of being a global leader in the development of secure, trustworthy and ethical artificial intelligence, as stated by the European Council33, and it ensures the protection of ethical principles, as specifically requested by the European Parliament34. (6) The notion of AI system should be clearly defined to ensure legal certainty, while providing the flexibility to accommodate future technological developments. The definition should be based on the key functional characteristics of the software, in particular the ability, for a given set of human-defined objectives, to generate outputs such as content, predictions, recommendations, or decisions which influence the environment with which the system interacts, be it in a physical or digital dimension. AI systems can be designed to operate with varying levels of autonomy and be used on a stand-alone basis or as a component of a product, irrespective of whether the system is physically integrated into the product (embedded) or serve the functionality of the product without being integrated therein (non-embedded). The definition of AI system should be complemented by a list of specific techniques and approaches used for its development, which should be kept up-to date in the light of",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      20
    ],
    "titles": [
      "3(18) of Regulation (EU) 2018/1725 of the European Parliament and of the Council36"
    ],
    "chunk_index": 33,
    "text": "market and technological European Council, Special meeting of the European Council (1 and 2 October 2020) Conclusions, EUCO 13/20, 2020, p. 6. European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(INL). developments through the adoption of delegated acts by the Commission to amend that list. (7) The notion of biometric data used in this Regulation is in line with and should be interpreted consistently with the notion of biometric data as defined in Article 4(14) of Regulation (EU) 2016/679 of the European Parliament and of the Council35, Article 3(18) of Regulation (EU) 2018/1725 of the European Parliament and of the Council36 and Article 3(13) of Directive (EU) 2016/680 of the European Parliament and of the Council37. (8) The notion of remote biometric identification system as used in this Regulation should be defined functionally, as an AI system intended for the identification of natural persons at a distance through the comparison of a person s biometric data with the biometric data contained in a reference database, and without prior knowledge whether the targeted person will be present and can be identified, irrespectively of the particular technology, processes or types of biometric data used. Considering their different characteristics and manners in which they are used, as well as the different risks involved, a distinction should be made between real-time and post remote biometric identification systems. In the case of real-time systems, the capturing of the biometric data, the comparison and the identification occur all instantaneously, near-instantaneously or in any event without a significant delay. In this regard, there should be no scope for circumventing the rules of this Regulation on the real-time use of the AI systems in question by providing for minor delays. Real-time",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 34,
    "text": "systems involve the use of live or near- live material, such as video footage, generated by a camera or other device with similar functionality. In the case of post systems, in contrast, the biometric data have already been captured and the comparison and identification occur only after a significant delay. This involves material, such as pictures or video footage generated by closed circuit television cameras or private devices, which has been generated before the use of the system in respect of the natural persons concerned. (9) For the purposes of this Regulation the notion of publicly accessible space should be understood as referring to any physical place that is accessible to the public, irrespective of whether the place in question is privately or publicly owned. Therefore, the notion does not cover places that are private in nature and normally not freely accessible for third parties, including law enforcement authorities, unless those parties have been specifically invited or authorised, such as homes, private clubs, offices, warehouses and factories. Online spaces are not covered either, as they are not physical spaces. However, the mere fact that certain conditions for accessing a Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1). Regulation (EU) 2018/1725 of the European Parliament and of the Council of 23 October 2018 on the protection of natural persons with regard to the processing of personal data by the Union institutions, bodies, offices and agencies and on the free movement of such data, and repealing Regulation (EC) No 45/2001 and Decision No 1247/2002/EC (OJ L 295, 21.11.2018, p.",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      21
    ],
    "titles": [],
    "chunk_index": 35,
    "text": "39) Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008/977/JHA (Law Enforcement Directive) (OJ L 119, 4.5.2016, p. 89). particular space may apply, such as admission tickets or age restrictions, does not mean that the space is not publicly accessible within the meaning of this Regulation. Consequently, in addition to public spaces such as streets, relevant parts of government buildings and most transport infrastructure, spaces such as cinemas, theatres, shops and shopping centres are normally also publicly accessible. Whether a given space is accessible to the public should however be determined on a case-by- case basis, having regard to the specificities of the individual situation at hand. (10) In order to ensure a level playing field and an effective protection of rights and freedoms of individuals across the Union, the rules established by this Regulation should apply to providers of AI systems in a non-discriminatory manner, irrespective of whether they are established within the Union or in a third country, and to users of AI systems established within the Union. (11) In light of their digital nature, certain AI systems should fall within the scope of this Regulation even when they are neither placed on the market, nor put into service, nor used in the Union. This is the case for example of an operator established in the Union that contracts certain services to an operator established outside the Union in relation to an activity to be performed by an AI system that would",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 36,
    "text": "qualify as high-risk and whose effects impact natural persons located in the Union. In those circumstances, the AI system used by the operator outside the Union could process data lawfully collected in and transferred from the Union, and provide to the contracting operator in the Union the output of that AI system resulting from that processing, without that AI system being placed on the market, put into service or used in the Union. To prevent the circumvention of this Regulation and to ensure an effective protection of natural persons located in the Union, this Regulation should also apply to providers and users of AI systems that are established in a third country, to the extent the output produced by those systems is used in the Union. Nonetheless, to take into account existing arrangements and special needs for cooperation with foreign partners with whom information and evidence is exchanged, this Regulation should not apply to public authorities of a third country and international organisations when acting in the framework of international agreements concluded at national or European level for law enforcement and judicial cooperation with the Union or with its Member States. Such agreements have been concluded bilaterally between Member States and third countries or between the European Union, Europol and other EU agencies and third countries and international organisations. (12) This Regulation should also apply to Union institutions, offices, bodies and agencies when acting as a provider or user of an AI system. AI systems exclusively developed or used for military purposes should be excluded from the scope of this Regulation where that use falls under the exclusive remit of the Common Foreign and Security Policy regulated under Title V of the Treaty on the European Union (TEU). This Regulation should be without prejudice to the provisions regarding the liability",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      22
    ],
    "titles": [],
    "chunk_index": 37,
    "text": "of intermediary service providers set out in Directive 2000/31/EC of the European Parliament and of the Council [as amended by the Digital Services Act]. (13) In order to ensure a consistent and high level of protection of public interests as regards health, safety and fundamental rights, common normative standards for all high-risk AI systems should be established. Those standards should be consistent with the Charter of fundamental rights of the European Union (the Charter) and should be non-discriminatory and in line with the Union s international trade commitments. (14) In order to introduce a proportionate and effective set of binding rules for AI systems, a clearly defined risk-based approach should be followed. That approach should tailor the type and content of such rules to the intensity and scope of the risks that AI systems can generate. It is therefore necessary to prohibit certain artificial intelligence practices, to lay down requirements for high-risk AI systems and obligations for the relevant operators, and to lay down transparency obligations for certain AI systems. (15) Aside from the many beneficial uses of artificial intelligence, that technology can also be misused and provide novel and powerful tools for manipulative, exploitative and social control practices. Such practices are particularly harmful and should be prohibited because they contradict Union values of respect for human dignity, freedom, equality, democracy and the rule of law and Union fundamental rights, including the right to non-discrimination, data protection and privacy and the rights of the child. (16) The placing on the market, putting into service or use of certain AI systems intended to distort human behaviour, whereby physical or psychological harms are likely to occur, should be forbidden. Such AI systems deploy subliminal components individuals cannot perceive or exploit vulnerabilities of children and people due to their age, physical or mental",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 38,
    "text": "incapacities. They do so with the intention to materially distort the behaviour of a person and in a manner that causes or is likely to cause harm to that or another person. The intention may not be presumed if the distortion of human behaviour results from factors external to the AI system which are outside of the control of the provider or the user. Research for legitimate purposes in relation to such AI systems should not be stifled by the prohibition, if such research does not amount to use of the AI system in human-machine relations that exposes natural persons to harm and such research is carried out in accordance with recognised ethical standards for scientific research. (17) AI systems providing social scoring of natural persons for general purpose by public authorities or on their behalf may lead to discriminatory outcomes and the exclusion of certain groups. They may violate the right to dignity and non-discrimination and the values of equality and justice. Such AI systems evaluate or classify the trustworthiness of natural persons based on their social behaviour in multiple contexts or known or predicted personal or personality characteristics. The social score obtained from such AI systems may lead to the detrimental or unfavourable treatment of natural persons or whole groups thereof in social contexts, which are unrelated to the context in which the data was originally generated or collected or to a detrimental treatment that is disproportionate or unjustified to the gravity of their social behaviour. Such AI systems should be therefore prohibited. (18) The use of AI systems for real-time remote biometric identification of natural persons in publicly accessible spaces for the purpose of law enforcement is considered particularly intrusive in the rights and freedoms of the concerned persons, to the extent that it may affect the",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      23
    ],
    "titles": [
      "2002/584/JHA, some are in practice likely to be more relevant than others, in that the"
    ],
    "chunk_index": 39,
    "text": "private life of a large part of the population, evoke a feeling of constant surveillance and indirectly dissuade the exercise of the freedom of assembly and other fundamental rights. In addition, the immediacy of the impact and the limited opportunities for further checks or corrections in relation to the use of such systems operating in real-time carry heightened risks for the rights and freedoms of the persons that are concerned by law enforcement activities. (19) The use of those systems for the purpose of law enforcement should therefore be prohibited, except in three exhaustively listed and narrowly defined situations, where the use is strictly necessary to achieve a substantial public interest, the importance of which outweighs the risks. Those situations involve the search for potential victims of crime, including missing children; certain threats to the life or physical safety of natural persons or of a terrorist attack; and the detection, localisation, identification or prosecution of perpetrators or suspects of the criminal offences referred to in Council Framework Decision 2002/584/JHA38 if those criminal offences are punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least three years and as they are defined in the law of that Member State. Such threshold for the custodial sentence or detention order in accordance with national law contributes to ensure that the offence should be serious enough to potentially justify the use of real-time remote biometric identification systems. Moreover, of the 32 criminal offences listed in the Council Framework Decision 2002/584/JHA, some are in practice likely to be more relevant than others, in that the recourse to real-time remote biometric identification will foreseeably be necessary and proportionate to highly varying degrees for the practical pursuit of the detection, localisation, identification or prosecution of a",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 40,
    "text": "perpetrator or suspect of the different criminal offences listed and having regard to the likely differences in the seriousness, probability and scale of the harm or possible negative consequences. (20) In order to ensure that those systems are used in a responsible and proportionate manner, it is also important to establish that, in each of those three exhaustively listed and narrowly defined situations, certain elements should be taken into account, in particular as regards the nature of the situation giving rise to the request and the consequences of the use for the rights and freedoms of all persons concerned and the safeguards and conditions provided for with the use. In addition, the use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement should be subject to appropriate limits in time and space, having regard in particular to the evidence or indications regarding the threats, the victims or perpetrator. The reference database of persons should be appropriate for each use case in each of the three situations mentioned above. (21) Each use of a real-time remote biometric identification system in publicly accessible spaces for the purpose of law enforcement should be subject to an express and specific authorisation by a judicial authority or by an independent administrative authority of a Member State. Such authorisation should in principle be obtained prior to the use, except in duly justified situations of urgency, that is, situations where the need to use the systems in question is such as to make it effectively and objectively impossible to obtain an authorisation before commencing the use. In such situations of urgency, the use should be restricted to the absolute minimum necessary and be subject to appropriate safeguards and conditions, as determined in national law and specified in the context of",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      24
    ],
    "titles": [
      "2016/680. However, the use of  real-time  remote biometric identification systems in"
    ],
    "chunk_index": 41,
    "text": "each individual urgent use case by the law enforcement authority itself. In addition, the law enforcement authority should in such situations seek to obtain an authorisation as soon as possible, whilst providing the reasons for not having been able to request it earlier. (22) Furthermore, it is appropriate to provide, within the exhaustive framework set by this Regulation that such use in the territory of a Member State in accordance with this Regulation should only be possible where and in as far as the Member State in question has decided to expressly provide for the possibility to authorise such use in its Council Framework Decision 2002/584/JHA of 13 June 2002 on the European arrest warrant and the surrender procedures between Member States (OJ L 190, 18.7.2002, p. 1). detailed rules of national law. Consequently, Member States remain free under this Regulation not to provide for such a possibility at all or to only provide for such a possibility in respect of some of the objectives capable of justifying authorised use identified in this Regulation. (23) The use of AI systems for real-time remote biometric identification of natural persons in publicly accessible spaces for the purpose of law enforcement necessarily involves the processing of biometric data. The rules of this Regulation that prohibit, subject to certain exceptions, such use, which are based on Article 16 TFEU, should apply as lex specialis in respect of the rules on the processing of biometric data contained in Article 10 of Directive (EU) 2016/680, thus regulating such use and the processing of biometric data involved in an exhaustive manner. Therefore, such use and processing should only be possible in as far as it is compatible with the framework set by this Regulation, without there being scope, outside that framework, for the competent authorities, where they",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 42,
    "text": "act for purpose of law enforcement, to use such systems and process such data in connection thereto on the grounds listed in Article 10 of Directive (EU) 2016/680. In this context, this Regulation is not intended to provide the legal basis for the processing of personal data under Article 8 of Directive 2016/680. However, the use of real-time remote biometric identification systems in publicly accessible spaces for purposes other than law enforcement, including by competent authorities, should not be covered by the specific framework regarding such use for the purpose of law enforcement set by this Regulation. Such use for purposes other than law enforcement should therefore not be subject to the requirement of an authorisation under this Regulation and the applicable detailed rules of national law that may give effect to it. (24) Any processing of biometric data and other personal data involved in the use of AI systems for biometric identification, other than in connection to the use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement as regulated by this Regulation, including where those systems are used by competent authorities in publicly accessible spaces for other purposes than law enforcement, should continue to comply with all requirements resulting from Article 9(1) of Regulation (EU) 2016/679, Article 10(1) of Regulation (EU) 2018/1725 and Article 10 of Directive (EU) 2016/680, as applicable. (25) In accordance with Article 6a of Protocol No 21 on the position of the United Kingdom and Ireland in respect of the area of freedom, security and justice, as annexed to the TEU and to the TFEU, Ireland is not bound by the rules laid down in Article 5(1), point (d), (2) and (3) of this Regulation adopted on the basis of Article 16 of the TFEU which relate",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      25
    ],
    "titles": [
      "167/2013 of the European Parliament and of the Council40, Regulation (EU) No"
    ],
    "chunk_index": 43,
    "text": "to the processing of personal data by the Member States when carrying out activities falling within the scope of Chapter 4 or Chapter 5 of Title V of Part Three of the TFEU, where Ireland is not bound by the rules governing the forms of judicial cooperation in criminal matters or police cooperation which require compliance with the provisions laid down on the basis of Article 16 of the TFEU. (26) In accordance with Articles 2 and 2a of Protocol No 22 on the position of Denmark, annexed to the TEU and TFEU, Denmark is not bound by rules laid down in Article 5(1), point (d), (2) and (3) of this Regulation adopted on the basis of Article 16 of the TFEU, or subject to their application, which relate to the processing of personal data by the Member States when carrying out activities falling within the scope of Chapter 4 or Chapter 5 of Title V of Part Three of the TFEU. (27) High-risk AI systems should only be placed on the Union market or put into service if they comply with certain mandatory requirements. Those requirements should ensure that high-risk AI systems available in the Union or whose output is otherwise used in the Union do not pose unacceptable risks to important Union public interests as recognised and protected by Union law. AI systems identified as high-risk should be limited to those that have a significant harmful impact on the health, safety and fundamental rights of persons in the Union and such limitation minimises any potential restriction to international trade, if any. (28) AI systems could produce adverse outcomes to health and safety of persons, in particular when such systems operate as components of products. Consistently with the objectives of Union harmonisation legislation to facilitate the free movement of",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 44,
    "text": "products in the internal market and to ensure that only safe and otherwise compliant products find their way into the market, it is important that the safety risks that may be generated by a product as a whole due to its digital components, including AI systems, are duly prevented and mitigated. For instance, increasingly autonomous robots, whether in the context of manufacturing or personal assistance and care should be able to safely operate and performs their functions in complex environments. Similarly, in the health sector where the stakes for life and health are particularly high, increasingly sophisticated diagnostics systems and systems supporting human decisions should be reliable and accurate. The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of particular relevance when classifying an AI system as high-risk. Those rights include the right to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom of assembly and of association, and non-discrimination, consumer protection, workers rights, rights of persons with disabilities, right to an effective remedy and to a fair trial, right of defence and the presumption of innocence, right to good administration. In addition to those rights, it is important to highlight that children have specific rights as enshrined in Article 24 of the EU Charter and in the United Nations Convention on the Rights of the Child (further elaborated in the UNCRC General Comment No. 25 as regards the digital environment), both of which require consideration of the children s vulnerabilities and provision of such protection and care as necessary for their well-being. The fundamental right to a high level of environmental protection enshrined in the Charter and implemented in Union policies should also be considered when assessing the severity",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      26
    ],
    "titles": [
      "661/2009 of the European Parliament and of the Council and Commission Regulations (EC) No"
    ],
    "chunk_index": 45,
    "text": "of the harm that an AI system can cause, including in relation to the health and safety of persons. (29) As regards high-risk AI systems that are safety components of products or systems, or which are themselves products or systems falling within the scope of Regulation (EC) No 300/2008 of the European Parliament and of the Council39, Regulation (EU) No 167/2013 of the European Parliament and of the Council40, Regulation (EU) No 168/2013 of the European Parliament and of the Council41, Directive 2014/90/EU of Regulation (EC) No 300/2008 of the European Parliament and of the Council of 11 March 2008 on common rules in the field of civil aviation security and repealing Regulation (EC) No 2320/2002 (OJ L 97, 9.4.2008, p. 72). Regulation (EU) No 167/2013 of the European Parliament and of the Council of 5 February 2013 on the approval and market surveillance of agricultural and forestry vehicles (OJ L 60, 2.3.2013, p. 1). Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15 January 2013 on the approval and market surveillance of two- or three-wheel vehicles and quadricycles (OJ L 60, 2.3.2013, p. 52). the European Parliament and of the Council42, Directive (EU) 2016/797 of the European Parliament and of the Council43, Regulation (EU) 2018/858 of the European Parliament and of the Council44, Regulation (EU) 2018/1139 of the European Parliament and of the Council45, and Regulation (EU) 2019/2144 of the European Parliament and of the Council46, it is appropriate to amend those acts to ensure that the Commission takes into account, on the basis of the technical and regulatory specificities of each sector, and without interfering with existing governance, conformity assessment and enforcement mechanisms and authorities established therein, the mandatory requirements for high-risk AI systems laid down in this Regulation when adopting any relevant",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 46,
    "text": "future delegated or implementing acts on the basis of those acts. (30) As regards AI systems that are safety components of products, or which are themselves products, falling within the scope of certain Union harmonisation legislation, it is appropriate to classify them as high-risk under this Regulation if the product in question undergoes the conformity assessment procedure with a third-party conformity assessment body pursuant to that relevant Union harmonisation legislation. In particular, such products are machinery, toys, lifts, equipment and protective systems intended for use in potentially explosive atmospheres, radio equipment, pressure equipment, recreational craft equipment, cableway installations, appliances burning gaseous fuels, medical devices, and in vitro diagnostic medical devices. (31) The classification of an AI system as high-risk pursuant to this Regulation should not necessarily mean that the product whose safety component is the AI system, or the AI system itself as a product, is considered high-risk under the criteria established in the relevant Union harmonisation legislation that applies to the product. This is notably the case for Regulation (EU) 2017/745 of the European Parliament and of the Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on marine equipment and repealing Council Directive 96/98/EC (OJ L 257, 28.8.2014, p. 146). Directive (EU) 2016/797 of the European Parliament and of the Council of 11 May 2016 on the interoperability of the rail system within the European Union (OJ L 138, 26.5.2016, p. 44). Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 on the approval and market surveillance of motor vehicles and their trailers, and of systems, components and separate technical units intended for such vehicles, amending Regulations (EC) No 715/2007 and (EC) No 595/2009 and repealing Directive 2007/46/EC (OJ L 151, 14.6.2018, p. 1). Regulation (EU) 2018/1139",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      27
    ],
    "titles": [
      "1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (OJ L 117, 5.5.2017, p. 1)."
    ],
    "chunk_index": 47,
    "text": "of the European Parliament and of the Council of 4 July 2018 on common rules in the field of civil aviation and establishing a European Union Aviation Safety Agency, and amending Regulations (EC) No 2111/2005, (EC) No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the European Parliament and of the Council, and repealing Regulations (EC) No 552/2004 and (EC) No 216/2008 of the European Parliament and of the Council and Council Regulation (EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1). Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on type-approval requirements for motor vehicles and their trailers, and systems, components and separate technical units intended for such vehicles, as regards their general safety and the protection of vehicle occupants and vulnerable road users, amending Regulation (EU) 2018/858 of the European Parliament and of the Council and repealing Regulations (EC) No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of the European Parliament and of the Council and Commission Regulations (EC) No 631/2009, (EU) No 406/2010, (EU) No 672/2010, (EU) No 1003/2010, (EU) No 1005/2010, (EU) No 1008/2010, (EU) No 1009/2010, (EU) No 19/2011, (EU) No 109/2011, (EU) No 458/2011, (EU) No 65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU) No 1230/2012 and (EU) 2015/166 (OJ L 325, 16.12.2019, p. 1). Council47 and Regulation (EU) 2017/746 of the European Parliament and of the Council48, where a third-party conformity assessment is provided for medium-risk and high-risk products. (32) As regards stand-alone AI systems, meaning high-risk AI systems other than those that are safety components of products, or which are themselves products, it is appropriate to classify them as high-risk if, in the light of their intended purpose, they pose a high risk of harm to",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 48,
    "text": "the health and safety or the fundamental rights of persons, taking into account both the severity of the possible harm and its probability of occurrence and they are used in a number of specifically pre-defined areas specified in the Regulation. The identification of those systems is based on the same methodology and criteria envisaged also for any future amendments of the list of high-risk AI systems. (33) Technical inaccuracies of AI systems intended for the remote biometric identification of natural persons can lead to biased results and entail discriminatory effects. This is particularly relevant when it comes to age, ethnicity, sex or disabilities. Therefore, real-time and post remote biometric identification systems should be classified as high-risk. In view of the risks that they pose, both types of remote biometric identification systems should be subject to specific requirements on logging capabilities and human oversight. (34) As regards the management and operation of critical infrastructure, it is appropriate to classify as high-risk the AI systems intended to be used as safety components in the management and operation of road traffic and the supply of water, gas, heating and electricity, since their failure or malfunctioning may put at risk the life and health of persons at large scale and lead to appreciable disruptions in the ordinary conduct of social and economic activities. (35) AI systems used in education or vocational training, notably for determining access or assigning persons to educational and vocational training institutions or to evaluate persons on tests as part of or as a precondition for their education should be considered high-risk, since they may determine the educational and professional course of a person s life and therefore affect their ability to secure their livelihood. When improperly designed and used, such systems may violate the right to education and training as",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      28
    ],
    "titles": [],
    "chunk_index": 49,
    "text": "well as the right not to be discriminated against and perpetuate historical patterns of discrimination. (36) AI systems used in employment, workers management and access to self-employment, notably for the recruitment and selection of persons, for making decisions on promotion and termination and for task allocation, monitoring or evaluation of persons in work-related contractual relationships, should also be classified as high-risk, since those systems may appreciably impact future career prospects and livelihoods of these persons. Relevant work-related contractual relationships should involve employees and persons providing services through platforms as referred to in the Commission Work Programme 2021. Such persons should in principle not be considered users within the meaning of this Regulation. Throughout the recruitment process and in the Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (OJ L 117, 5.5.2017, p. 1). Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 April 2017 on in vitro diagnostic medical devices and repealing Directive 98/79/EC and Commission Decision 2010/227/EU (OJ L 117, 5.5.2017, p. 176). evaluation, promotion, or retention of persons in work-related contractual relationships, such systems may perpetuate historical patterns of discrimination, for example against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation. AI systems used to monitor the performance and behaviour of these persons may also impact their rights to data protection and privacy. (37) Another area in which the use of AI systems deserves special consideration is the access to and enjoyment of certain essential private and public services and benefits necessary for people to fully participate in society or to improve one s standard",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 50,
    "text": "of living. In particular, AI systems used to evaluate the credit score or creditworthiness of natural persons should be classified as high-risk AI systems, since they determine those persons access to financial resources or essential services such as housing, electricity, and telecommunication services. AI systems used for this purpose may lead to discrimination of persons or groups and perpetuate historical patterns of discrimination, for example based on racial or ethnic origins, disabilities, age, sexual orientation, or create new forms of discriminatory impacts. Considering the very limited scale of the impact and the available alternatives on the market, it is appropriate to exempt AI systems for the purpose of creditworthiness assessment and credit scoring when put into service by small-scale providers for their own use. Natural persons applying for or receiving public assistance benefits and services from public authorities are typically dependent on those benefits and services and in a vulnerable position in relation to the responsible authorities. If AI systems are used for determining whether such benefits and services should be denied, reduced, revoked or reclaimed by authorities, they may have a significant impact on persons livelihood and may infringe their fundamental rights, such as the right to social protection, non- discrimination, human dignity or an effective remedy. Those systems should therefore be classified as high-risk. Nonetheless, this Regulation should not hamper the development and use of innovative approaches in the public administration, which would stand to benefit from a wider use of compliant and safe AI systems, provided that those systems do not entail a high risk to legal and natural persons. Finally, AI systems used to dispatch or establish priority in the dispatching of emergency first response services should also be classified as high-risk since they make decisions in very critical situations for the life and health of",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      29
    ],
    "titles": [
      "2013/32/EU of the European Parliament and of the Council49, the Regulation (EC) No"
    ],
    "chunk_index": 51,
    "text": "persons and their property. (38) Actions by law enforcement authorities involving certain uses of AI systems are characterised by a significant degree of power imbalance and may lead to surveillance, arrest or deprivation of a natural person s liberty as well as other adverse impacts on fundamental rights guaranteed in the Charter. In particular, if the AI system is not trained with high quality data, does not meet adequate requirements in terms of its accuracy or robustness, or is not properly designed and tested before being put on the market or otherwise put into service, it may single out people in a discriminatory or otherwise incorrect or unjust manner. Furthermore, the exercise of important procedural fundamental rights, such as the right to an effective remedy and to a fair trial as well as the right of defence and the presumption of innocence, could be hampered, in particular, where such AI systems are not sufficiently transparent, explainable and documented. It is therefore appropriate to classify as high-risk a number of AI systems intended to be used in the law enforcement context where accuracy, reliability and transparency is particularly important to avoid adverse impacts, retain public trust and ensure accountability and effective redress. In view of the nature of the activities in question and the risks relating thereto, those high-risk AI systems should include in particular AI systems intended to be used by law enforcement authorities for individual risk assessments, polygraphs and similar tools or to detect the emotional state of natural person, to detect deep fakes , for the evaluation of the reliability of evidence in criminal proceedings, for predicting the occurrence or reoccurrence of an actual or potential criminal offence based on profiling of natural persons, or assessing personality traits and characteristics or past criminal behaviour of natural persons or",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 52,
    "text": "groups, for profiling in the course of detection, investigation or prosecution of criminal offences, as well as for crime analytics regarding natural persons. AI systems specifically intended to be used for administrative proceedings by tax and customs authorities should not be considered high-risk AI systems used by law enforcement authorities for the purposes of prevention, detection, investigation and prosecution of criminal offences. (39) AI systems used in migration, asylum and border control management affect people who are often in particularly vulnerable position and who are dependent on the outcome of the actions of the competent public authorities. The accuracy, non- discriminatory nature and transparency of the AI systems used in those contexts are therefore particularly important to guarantee the respect of the fundamental rights of the affected persons, notably their rights to free movement, non-discrimination, protection of private life and personal data, international protection and good administration. It is therefore appropriate to classify as high-risk AI systems intended to be used by the competent public authorities charged with tasks in the fields of migration, asylum and border control management as polygraphs and similar tools or to detect the emotional state of a natural person; for assessing certain risks posed by natural persons entering the territory of a Member State or applying for visa or asylum; for verifying the authenticity of the relevant documents of natural persons; for assisting competent public authorities for the examination of applications for asylum, visa and residence permits and associated complaints with regard to the objective to establish the eligibility of the natural persons applying for a status. AI systems in the area of migration, asylum and border control management covered by this Regulation should comply with the relevant procedural requirements set by the Directive 2013/32/EU of the European Parliament and of the Council49, the Regulation",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      30
    ],
    "titles": [],
    "chunk_index": 53,
    "text": "(EC) No 810/2009 of the European Parliament and of the Council50 and other relevant legislation. (40) Certain AI systems intended for the administration of justice and democratic processes should be classified as high-risk, considering their potentially significant impact on democracy, rule of law, individual freedoms as well as the right to an effective remedy and to a fair trial. In particular, to address the risks of potential biases, errors and opacity, it is appropriate to qualify as high-risk AI systems intended to assist judicial authorities in researching and interpreting facts and the law and in applying the law to a concrete set of facts. Such qualification should not extend, however, to AI systems intended for purely ancillary administrative activities that do not affect the actual administration of justice in individual cases, such as anonymisation or pseudonymisation of judicial decisions, documents or data, communication between personnel, administrative tasks or allocation of resources. Directive 2013/32/EU of the European Parliament and of the Council of 26 June 2013 on common procedures for granting and withdrawing international protection (OJ L 180, 29.6.2013, p. 60). Regulation (EC) No 810/2009 of the European Parliament and of the Council of 13 July 2009 establishing a Community Code on Visas (Visa Code) (OJ L 243, 15.9.2009, p. 1). (41) The fact that an AI system is classified as high risk under this Regulation should not be interpreted as indicating that the use of the system is necessarily lawful under other acts of Union law or under national law compatible with Union law, such as on the protection of personal data, on the use of polygraphs and similar tools or other systems to detect the emotional state of natural persons. Any such use should continue to occur solely in accordance with the applicable requirements resulting from the Charter and",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 54,
    "text": "from the applicable acts of secondary Union law and national law. This Regulation should not be understood as providing for the legal ground for processing of personal data, including special categories of personal data, where relevant. (42) To mitigate the risks from high-risk AI systems placed or otherwise put into service on the Union market for users and affected persons, certain mandatory requirements should apply, taking into account the intended purpose of the use of the system and according to the risk management system to be established by the provider. (43) Requirements should apply to high-risk AI systems as regards the quality of data sets used, technical documentation and record-keeping, transparency and the provision of information to users, human oversight, and robustness, accuracy and cybersecurity. Those requirements are necessary to effectively mitigate the risks for health, safety and fundamental rights, as applicable in the light of the intended purpose of the system, and no other less trade restrictive measures are reasonably available, thus avoiding unjustified restrictions to trade. (44) High data quality is essential for the performance of many AI systems, especially when techniques involving the training of models are used, with a view to ensure that the high-risk AI system performs as intended and safely and it does not become the source of discrimination prohibited by Union law. High quality training, validation and testing data sets require the implementation of appropriate data governance and management practices. Training, validation and testing data sets should be sufficiently relevant, representative and free of errors and complete in view of the intended purpose of the system. They should also have the appropriate statistical properties, including as regards the persons or groups of persons on which the high-risk AI system is intended to be used. In particular, training, validation and testing data sets should",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      31
    ],
    "titles": [],
    "chunk_index": 55,
    "text": "take into account, to the extent required in the light of their intended purpose, the features, characteristics or elements that are particular to the specific geographical, behavioural or functional setting or context within which the AI system is intended to be used. In order to protect the right of others from the discrimination that might result from the bias in AI systems, the providers shouldbe able to process also special categories of personal data, as a matter of substantial public interest, in order to ensure the bias monitoring, detection and correction in relation to high-risk AI systems. (45) For the development of high-risk AI systems, certain actors, such as providers, notified bodies and other relevant entities, such as digital innovation hubs, testing experimentation facilities and researchers, should be able to access and use high quality datasets within their respective fields of activities which are related to this Regulation. European common data spaces established by the Commission and the facilitation of data sharing between businesses and with government in the public interest will be instrumental to provide trustful, accountable and non-discriminatory access to high quality data for the training, validation and testing of AI systems. For example, in health, the European health data space will facilitate non-discriminatory access to health data and the training of artificial intelligence algorithms on those datasets, in a privacy-preserving, secure, timely, transparent and trustworthy manner, and with an appropriate institutional governance. Relevant competent authorities, including sectoral ones, providing or supporting the access to data may also support the provision of high-quality data for the training, validation and testing of AI systems. (46) Having information on how high-risk AI systems have been developed and how they perform throughout their lifecycle is essential to verify compliance with the requirements under this Regulation. This requires keeping records and the",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 56,
    "text": "availability of a technical documentation, containing information which is necessary to assess the compliance of the AI system with the relevant requirements. Such information should include the general characteristics, capabilities and limitations of the system, algorithms, data, training, testing and validation processes used as well as documentation on the relevant risk management system. The technical documentation should be kept up to date. (47) To address the opacity that may make certain AI systems incomprehensible to or too complex for natural persons, a certain degree of transparency should be required for high-risk AI systems. Users should be able to interpret the system output and use it appropriately. High-risk AI systems should therefore be accompanied by relevant documentation and instructions of use and include concise and clear information, including in relation to possible risks to fundamental rights and discrimination, where appropriate. (48) High-risk AI systems should be designed and developed in such a way that natural persons can oversee their functioning. For this purpose, appropriate human oversight measures should be identified by the provider of the system before its placing on the market or putting into service. In particular, where appropriate, such measures should guarantee that the system is subject to in-built operational constraints that cannot be overridden by the system itself and is responsive to the human operator, and that the natural persons to whom human oversight has been assigned have the necessary competence, training and authority to carry out that role. (49) High-risk AI systems should perform consistently throughout their lifecycle and meet an appropriate level of accuracy, robustness and cybersecurity in accordance with the generally acknowledged state of the art. The level of accuracy and accuracy metrics should be communicated to the users. (50) The technical robustness is a key requirement for high-risk AI systems. They should be resilient",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      32
    ],
    "titles": [
      "2019/1020 of the European Parliament and of the Council53 on market surveillance"
    ],
    "chunk_index": 57,
    "text": "against risks connected to the limitations of the system (e.g. errors, faults, inconsistencies, unexpected situations) as well as against malicious actions that may compromise the security of the AI system and result in harmful or otherwise undesirable behaviour. Failure to protect against these risks could lead to safety impacts or negatively affect the fundamental rights, for example due to erroneous decisions or wrong or biased outputs generated by the AI system. (51) Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system s vulnerabilities. Cyberattacks against AI systems can leverage AI specific assets, such as training data sets (e.g. data poisoning) or trained models (e.g. adversarial attacks), or exploit vulnerabilities in the AI system s digital assets or the underlying ICT infrastructure. To ensure a level of cybersecurity appropriate to the risks, suitable measures should therefore be taken by the providers of high-risk AI systems, also taking into account as appropriate the underlying ICT infrastructure. (52) As part of Union harmonisation legislation, rules applicable to the placing on the market, putting into service and use of high-risk AI systems should be laid down consistently with Regulation (EC) No 765/2008 of the European Parliament and of the Council51 setting out the requirements for accreditation and the market surveillance of products, Decision No 768/2008/EC of the European Parliament and of the Council52 on a common framework for the marketing of products and Regulation (EU) 2019/1020 of the European Parliament and of the Council53 on market surveillance and compliance of products ( New Legislative Framework for the marketing of products ). (53) It is appropriate that a specific natural or legal person, defined as the provider, takes the responsibility for the",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 58,
    "text": "placing on the market or putting into service of a high-risk AI system, regardless of whether that natural or legal person is the person who designed or developed the system. (54) The provider should establish a sound quality management system, ensure the accomplishment of the required conformity assessment procedure, draw up the relevant documentation and establish a robust post-market monitoring system. Public authorities which put into service high-risk AI systems for their own use may adopt and implement the rules for the quality management system as part of the quality management system adopted at a national or regional level, as appropriate, taking into account the specificities of the sector and the competences and organisation of the public authority in question. (55) Where a high-risk AI system that is a safety component of a product which is covered by a relevant New Legislative Framework sectorial legislation is not placed on the market or put into service independently from the product, the manufacturer of the final product as defined under the relevant New Legislative Framework legislation should comply with the obligations of the provider established in this Regulation and notably ensure that the AI system embedded in the final product complies with the requirements of this Regulation. (56) To enable enforcement of this Regulation and create a level-playing field for operators, and taking into account the different forms of making available of digital products, it is important to ensure that, under all circumstances, a person established in the Union can provide authorities with all the necessary information on the compliance of an AI system. Therefore, prior to making their AI systems available in the Union, where an importer cannot be identified, providers established outside the Union shall, by written mandate, appoint an authorised representative established in the Union. (57) In line with",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      33
    ],
    "titles": [
      "94/9/EC, 94/25/EC, 95/16/EC, 97/23/EC, 98/34/EC, 2004/22/EC, 2007/23/EC, 2009/23/EC and"
    ],
    "chunk_index": 59,
    "text": "New Legislative Framework principles, specific obligations for relevant economic operators, such as importers and distributors, should be set to ensure legal certainty and facilitate regulatory compliance by those relevant operators. Regulation (EC) No 765/2008 of the European Parliament and of the Council of 9 July 2008 setting out the requirements for accreditation and market surveillance relating to the marketing of products and repealing Regulation (EEC) No 339/93 (OJ L 218, 13.8.2008, p. 30). Decision No 768/2008/EC of the European Parliament and of the Council of 9 July 2008 on a common framework for the marketing of products, and repealing Council Decision 93/465/EEC (OJ L 218, 13.8.2008, p. 82). Regulation (EU) 2019/1020 of the European Parliament and of the Council of 20 June 2019 on market surveillance and compliance of products and amending Directive 2004/42/EC and Regulations (EC) No 765/2008 and (EU) No 305/2011 (Text with EEA relevance) (OJ L 169, 25.6.2019, p. 1 44). (58) Given the nature of AI systems and the risks to safety and fundamental rights possibly associated with their use, including as regard the need to ensure proper monitoring of the performance of an AI system in a real-life setting, it is appropriate to set specific responsibilities for users. Users should in particular use high-risk AI systems in accordance with the instructions of use and certain other obligations should be provided for with regard to monitoring of the functioning of the AI systems and with regard to record-keeping, as appropriate. (59) It is appropriate to envisage that the user of the AI system should be the natural or legal person, public authority, agency or other body under whose authority the AI system is operated except where the use is made in the course of a personal non- professional activity. (60) In the light of the complexity",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 60,
    "text": "of the artificial intelligence value chain, relevant third parties, notably the ones involved in the sale and the supply of software, software tools and components, pre-trained models and data, or providers of network services, should cooperate, as appropriate, with providers and users to enable their compliance with the obligations under this Regulation and with competent authorities established under this Regulation. (61) Standardisation should play a key role to provide technical solutions to providers to ensure compliance with this Regulation. Compliance with harmonised standards as defined in Regulation (EU) No 1025/2012 of the European Parliament and of the Council54 should be a means for providers to demonstrate conformity with the requirements of this Regulation. However, the Commission could adopt common technical specifications in areas where no harmonised standards exist or where they are insufficient. (62) In order to ensure a high level of trustworthiness of high-risk AI systems, those systems should be subject to a conformity assessment prior to their placing on the market or putting into service. (63) It is appropriate that, in order to minimise the burden on operators and avoid any possible duplication, for high-risk AI systems related to products which are covered by existing Union harmonisation legislation following the New Legislative Framework approach, the compliance of those AI systems with the requirements of this Regulation should be assessed as part of the conformity assessment already foreseen under that legislation. The applicability of the requirements of this Regulation should thus not affect the specific logic, methodology or general structure of conformity assessment under the relevant specific New Legislative Framework legislation. This approach is fully reflected in the interplay between this Regulation and the [Machinery Regulation]. While safety risks of AI systems ensuring safety functions in machinery are addressed by the requirements of this Regulation, certain specific requirements in",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      34
    ],
    "titles": [],
    "chunk_index": 61,
    "text": "the [Machinery Regulation] will ensure the safe integration of the AI system into the overall machinery, so as not to compromise the safety of the machinery as a whole. Regulation (EU) No 1025/2012 of the European Parliament and of the Council of 25 October 2012 on European standardisation, amending Council Directives 89/686/EEC and 93/15/EEC and Directives 94/9/EC, 94/25/EC, 95/16/EC, 97/23/EC, 98/34/EC, 2004/22/EC, 2007/23/EC, 2009/23/EC and 2009/105/EC of the European Parliament and of the Council and repealing Council Decision 87/95/EEC and Decision No 1673/2006/EC of the European Parliament and of the Council (OJ L 316, 14.11.2012, p. 12). The [Machinery Regulation] applies the same definition of AI system as this Regulation. (64) Given the more extensive experience of professional pre-market certifiers in the field of product safety and the different nature of risks involved, it is appropriate to limit, at least in an initial phase of application of this Regulation, the scope of application of third-party conformity assessment for high-risk AI systems other than those related to products. Therefore, the conformity assessment of such systems should be carried out as a general rule by the provider under its own responsibility, with the only exception of AI systems intended to be used for the remote biometric identification of persons, for which the involvement of a notified body in the conformity assessment should be foreseen, to the extent they are not prohibited. (65) In order to carry out third-party conformity assessment for AI systems intended to be used for the remote biometric identification of persons, notified bodies should be designated under this Regulation by the national competent authorities, provided they are compliant with a set of requirements, notably on independence, competence and absence of conflicts of interests. (66) In line with the commonly established notion of substantial modification for products regulated by Union",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 62,
    "text": "harmonisation legislation, it is appropriate that an AI system undergoes a new conformity assessment whenever a change occurs which may affect the compliance of the system with this Regulation or when the intended purpose of the system changes. In addition, as regards AI systems which continue to learn after being placed on the market or put into service (i.e. they automatically adapt how functions are carried out), it is necessary to provide rules establishing that changes to the algorithm and its performance that have been pre-determined by the provider and assessed at the moment of the conformity assessment should not constitute a substantial modification. (67) High-risk AI systems should bear the CE marking to indicate their conformity with this Regulation so that they can move freely within the internal market. Member States should not create unjustified obstacles to the placing on the market or putting into service of high-risk AI systems that comply with the requirements laid down in this Regulation and bear the CE marking. (68) Under certain conditions, rapid availability of innovative technologies may be crucial for health and safety of persons and for society as a whole. It is thus appropriate that under exceptional reasons of public security or protection of life and health of natural persons and the protection of industrial and commercial property, Member States could authorise the placing on the market or putting into service of AI systems which have not undergone a conformity assessment. (69) In order to facilitate the work of the Commission and the Member States in the artificial intelligence field as well as to increase the transparency towards the public, providers of high-risk AI systems other than those related to products falling within the scope of relevant existing Union harmonisation legislation, should be required to register their high-risk AI system",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      35
    ],
    "titles": [],
    "chunk_index": 63,
    "text": "in a EU database, to be established and managed by the Commission. The Commission should be the controller of that database, in accordance with Regulation (EU) 2018/1725 of the European Parliament and of the Council55. In order to ensure the full functionality of the database, when deployed, the procedure for setting the database should include the elaboration of functional specifications by the Commission and an independent audit report. (70) Certain AI systems intended to interact with natural persons or to generate content may pose specific risks of impersonation or deception irrespective of whether they qualify as high-risk or not. In certain circumstances, the use of these systems should therefore be subject to specific transparency obligations without prejudice to the requirements and obligations for high-risk AI systems. In particular, natural persons should be notified that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use. Moreover, natural persons should be notified when they are exposed to an emotion recognition system or a biometric categorisation system. Such information and notifications should be provided in accessible formats for persons with disabilities. Further, users, who use an AI system to generate or manipulate image, audio or video content that appreciably resembles existing persons, places or events and would falsely appear to a person to be authentic, should disclose that the content has been artificially created or manipulated by labelling the artificial intelligence output accordingly and disclosing its artificial origin. (71) Artificial intelligence is a rapidly developing family of technologies that requires novel forms of regulatory oversight and a safe space for experimentation, while ensuring responsible innovation and integration of appropriate safeguards and risk mitigation measures. To ensure a legal framework that is innovation-friendly, future- proof and resilient to disruption, national competent authorities from one or",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      36
    ],
    "titles": [
      "83(2) of Regulation 2016/679 and Article 57 of Directive 2016/680."
    ],
    "chunk_index": 64,
    "text": "more Member States should be encouraged to establish artificial intelligence regulatory sandboxes to facilitate the development and testing of innovative AI systems under strict regulatory oversight before these systems are placed on the market or otherwise put into service. (72) The objectives of the regulatory sandboxes should be to foster AI innovation by establishing a controlled experimentation and testing environment in the development and pre-marketing phase with a view to ensuring compliance of the innovative AI systems with this Regulation and other relevant Union and Member States legislation; to enhance legal certainty for innovators and the competent authorities oversight and understanding of the opportunities, emerging risks and the impacts of AI use, and to accelerate access to markets, including by removing barriers for small and medium enterprises (SMEs) and start-ups. To ensure uniform implementation across the Union and economies of scale, it is appropriate to establish common rules for the regulatory sandboxes implementation and a framework for cooperation between the relevant authorities involved in the supervision of the sandboxes. This Regulation should provide the legal basis for the use of personal data collected for other purposes for developing certain AI systems in the public interest within the AI regulatory sandbox, in line with Article 6(4) of Regulation (EU) 2016/679, and Article 6 of Regulation (EU) 2018/1725, and without prejudice to Article 4(2) of Directive (EU) 2016/680. Participants in the sandbox should ensure appropriate safeguards and cooperate with the competent authorities, including by following their guidance and acting Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1). expeditiously and",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 65,
    "text": "in good faith to mitigate any high-risks to safety and fundamental rights that may arise during the development and experimentation in the sandbox. The conduct of the participants in the sandbox should be taken into account when competent authorities decide whether to impose an administrative fine under Article 83(2) of Regulation 2016/679 and Article 57 of Directive 2016/680. (73) In order to promote and protect innovation, it is important that the interests of small- scale providers and users of AI systems are taken into particular account. To this objective, Member States should develop initiatives, which are targeted at those operators, including on awareness raising and information communication. Moreover, the specific interests and needs of small-scale providers shall be taken into account when Notified Bodies set conformity assessment fees. Translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, notably those of a smaller scale. Member States should possibly ensure that one of the languages determined and accepted by them for relevant providers documentation and for communication with operators is one which is broadly understood by the largest possible number of cross- border users. (74) In order to minimise the risks to implementation resulting from lack of knowledge and expertise in the market as well as to facilitate compliance of providers and notified bodies with their obligations under this Regulation, the AI-on demand platform, the European Digital Innovation Hubs and the Testing and Experimentation Facilities established by the Commission and the Member States at national or EU level should possibly contribute to the implementation of this Regulation. Within their respective mission and fields of competence, they may provide in particular technical and scientific support to providers and notified bodies. (75) It is appropriate that the Commission facilitates, to the extent possible,",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      37
    ],
    "titles": [
      "2013/36/EU of the European Parliament and of the Council56, it is also appropriate to"
    ],
    "chunk_index": 66,
    "text": "access to Testing and Experimentation Facilities to bodies, groups or laboratories established or accredited pursuant to any relevant Union harmonisation legislation and which fulfil tasks in the context of conformity assessment of products or devices covered by that Union harmonisation legislation. This is notably the case for expert panels, expert laboratories and reference laboratories in the field of medical devices pursuant to Regulation (EU) 2017/745 and Regulation (EU) 2017/746. (76) In order to facilitate a smooth, effective and harmonised implementation of this Regulation a European Artificial Intelligence Board should be established. The Board should be responsible for a number of advisory tasks, including issuing opinions, recommendations, advice or guidance on matters related to the implementation of this Regulation, including on technical specifications or existing standards regarding the requirements established in this Regulation and providing advice to and assisting the Commission on specific questions related to artificial intelligence. (77) Member States hold a key role in the application and enforcement of this Regulation. In this respect, each Member State should designate one or more national competent authorities for the purpose of supervising the application and implementation of this Regulation. In order to increase organisation efficiency on the side of Member States and to set an official point of contact vis- -vis the public and other counterparts at Member State and Union levels, in each Member State one national authority should be designated as national supervisory authority. (78) In order to ensure that providers of high-risk AI systems can take into account the experience on the use of high-risk AI systems for improving their systems and the design and development process or can take any possible corrective action in a timely manner, all providers should have a post-market monitoring system in place. This system is also key to ensure that the possible",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 67,
    "text": "risks emerging from AI systems which continue to learn after being placed on the market or put into service can be more efficiently and timely addressed. In this context, providers should also be required to have a system in place to report to the relevant authorities any serious incidents or any breaches to national and Union law protecting fundamental rights resulting from the use of their AI systems. (79) In order to ensure an appropriate and effective enforcement of the requirements and obligations set out by this Regulation, which is Union harmonisation legislation, the system of market surveillance and compliance of products established by Regulation (EU) 2019/1020 should apply in its entirety. Where necessary for their mandate, national public authorities or bodies, which supervise the application of Union law protecting fundamental rights, including equality bodies, should also have access to any documentation created under this Regulation. (80) Union legislation on financial services includes internal governance and risk management rules and requirements which are applicable to regulated financial institutions in the course of provision of those services, including when they make use of AI systems. In order to ensure coherent application and enforcement of the obligations under this Regulation and relevant rules and requirements of the Union financial services legislation, the authorities responsible for the supervision and enforcement of the financial services legislation, including where applicable the European Central Bank, should be designated as competent authorities for the purpose of supervising the implementation of this Regulation, including for market surveillance activities, as regards AI systems provided or used by regulated and supervised financial institutions. To further enhance the consistency between this Regulation and the rules applicable to credit institutions regulated under Directive 2013/36/EU of the European Parliament and of the Council56, it is also appropriate to integrate the conformity assessment procedure",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      38
    ],
    "titles": [],
    "chunk_index": 68,
    "text": "and some of the providers procedural obligations in relation to risk management, post marketing monitoring and documentation into the existing obligations and procedures under Directive 2013/36/EU. In order to avoid overlaps, limited derogations should also be envisaged in relation to the quality management system of providers and the monitoring obligation placed on users of high-risk AI systems to the extent that these apply to credit institutions regulated by Directive 2013/36/EU. (81) The development of AI systems other than high-risk AI systems in accordance with the requirements of this Regulation may lead to a larger uptake of trustworthy artificial intelligence in the Union. Providers of non-high-risk AI systems should be encouraged to create codes of conduct intended to foster the voluntary application of the mandatory requirements applicable to high-risk AI systems. Providers should also be encouraged to apply on a voluntary basis additional requirements related, for example, to environmental sustainability, accessibility to persons with disability, stakeholders participation in the design and development of AI systems, and diversity of the development teams. The Commission may develop initiatives, including of a sectorial Directive 2013/36/EU of the European Parliament and of the Council of 26 June 2013 on access to the activity of credit institutions and the prudential supervision of credit institutions and investment firms, amending Directive 2002/87/EC and repealing Directives 2006/48/EC and 2006/49/EC (OJ L 176, 27.6.2013, p. 338). nature, to facilitate the lowering of technical barriers hindering cross-border exchange of data for AI development, including on data access infrastructure, semantic and technical interoperability of different types of data. (82) It is important that AI systems related to products that are not high-risk in accordance with this Regulation and thus are not required to comply with the requirements set out herein are nevertheless safe when placed on the market or put into service.",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 69,
    "text": "To contribute to this objective, the Directive 2001/95/EC of the European Parliament and of the Council57 would apply as a safety net. (83) In order to ensure trustful and constructive cooperation of competent authorities on Union and national level, all parties involved in the application of this Regulation should respect the confidentiality of information and data obtained in carrying out their tasks. (84) Member States should take all necessary measures to ensure that the provisions of this Regulation are implemented, including by laying down effective, proportionate and dissuasive penalties for their infringement. For certain specific infringements, Member States should take into account the margins and criteria set out in this Regulation. The European Data Protection Supervisor should have the power to impose fines on Union institutions, agencies and bodies falling within the scope of this Regulation. (85) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the techniques and approaches referred to in Annex I to define AI systems, the Union harmonisation legislation listed in Annex II, the high-risk AI systems listed in Annex III, the provisions regarding technical documentation listed in Annex IV, the content of the EU declaration of conformity in Annex V, the provisions regarding the conformity assessment procedures in Annex VI and VII and the provisions establishing the high-risk AI systems to which the conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation should apply. It is of particular importance that the Commission carry out appropriate consultations during its preparatory work, including at expert level, and that those consultations be conducted in accordance with the principles laid down in the Interinstitutional Agreement of 13 April",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      39
    ],
    "titles": [
      "85]. However, the infrastructure related to the governance and the conformity"
    ],
    "chunk_index": 70,
    "text": "2016 on Better Law-Making58. In particular, to ensure equal participation in the preparation of delegated acts, the European Parliament and the Council receive all documents at the same time as Member States experts, and their experts systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts. (86) In order to ensure uniform conditions for the implementation of this Regulation, implementing powers should be conferred on the Commission. Those powers should be exercised in accordance with Regulation (EU) No 182/2011 of the European Parliament and of the Council59. (87) Since the objective of this Regulation cannot be sufficiently achieved by the Member States and can rather, by reason of the scale or effects of the action, be better achieved Directive 2001/95/EC of the European Parliament and of the Council of 3 December 2001 on general product safety (OJ L 11, 15.1.2002, p. 4). OJ L 123, 12.5.2016, p. 1. Regulation (EU) No 182/2011 of the European Parliament and of the Council of 16 February 2011 laying down the rules and general principles concerning mechanisms for control by the Member States of the Commission's exercise of implementing powers (OJ L 55, 28.2.2011, p.13). at Union level, the Union may adopt measures in accordance with the principle of subsidiarity as set out in Article 5 TEU. In accordance with the principle of proportionality as set out in that Article, this Regulation does not go beyond what is necessary in order to achieve that objective. (88) This Regulation should apply from [OP please insert the date established in Art. 85]. However, the infrastructure related to the governance and the conformity assessment system should be operational before that date, therefore the provisions on notified bodies and governance structure should apply from [OP please insert the date three months following",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      40
    ],
    "titles": [
      "2."
    ],
    "chunk_index": 71,
    "text": "the entry into force of this Regulation]. In addition, Member States should lay down and notify to the Commission the rules on penalties, including administrative fines, and ensure that they are properly and effectively implemented by the date of application of this Regulation. Therefore the provisions on penalties should apply from [OP please insert the date twelve months following the entry into force of this Regulation]. (89) The European Data Protection Supervisor and the European Data Protection Board were consulted in accordance with Article 42(2) of Regulation (EU) 2018/1725 and delivered an opinion on [ ] . HAVE ADOPTED THIS REGULATION: TITLE I GENERAL PROVISIONS Article 1 Subject matter This Regulation lays down: (a) harmonised rules for the placing on the market, the putting into service and the use of artificial intelligence systems ( AI systems ) in the Union; (a) prohibitions of certain artificial intelligence practices; (b) specific requirements for high-risk AI systems and obligations for operators of such systems; (c) harmonised transparency rules for AI systems intended to interact with natural persons, emotion recognition systems and biometric categorisation systems, and AI systems used to generate or manipulate image, audio or video content; (d) rules on market monitoring and surveillance. Article 2 Scope 1. This Regulation applies to: (a) providers placing on the market or putting into service AI systems in the Union, irrespective of whether those providers are established within the Union or in a third country; (b) users of AI systems located within the Union; (c) providers and users of AI systems that are located in a third country, where the output produced by the system is used in the Union; 2. For high-risk AI systems that are safety components of products or systems, or which are themselves products or systems, falling within the scope of the",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 72,
    "text": "following acts, only Article 84 of this Regulation shall apply: (a) Regulation (EC) 300/2008; (b) Regulation (EU) No 167/2013; (c) Regulation (EU) No 168/2013; (d) Directive 2014/90/EU; (e) Directive (EU) 2016/797; (f) Regulation (EU) 2018/858; (g) Regulation (EU) 2018/1139; (h) Regulation (EU) 2019/2144. 3. This Regulation shall not apply to AI systems developed or used exclusively for military purposes. 4. This Regulation shall not apply to public authorities in a third country nor to international organisations falling within the scope of this Regulation pursuant to paragraph 1, where those authorities or organisations use AI systems in the framework of international agreements for law enforcement and judicial cooperation with the Union or with one or more Member States. 5. This Regulation shall not affect the application of the provisions on the liability of intermediary service providers set out in Chapter II, Section IV of Directive 2000/31/EC of the European Parliament and of the Council60 [as to be replaced by the corresponding provisions of the Digital Services Act]. Article 3 Definitions For the purpose of this Regulation, the following definitions apply: (1) artificial intelligence system (AI system) means software that is developed with one or more of the techniques and approaches listed in Annex I and can, for a given set of human-defined objectives, generate outputs such as content, predictions, recommendations, or decisions influencing the environments they interact with; (1) provider means a natural or legal person, public authority, agency or other body that develops an AI system or that has an AI system developed with a view to placing it on the market or putting it into service under its own name or trademark, whether for payment or free of charge; Directive 2000/31/EC of the European Parliament and of the Council of 8 June 2000 on certain legal aspects of information",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      41
    ],
    "titles": [],
    "chunk_index": 73,
    "text": "society services, in particular electronic commerce, in the Internal Market ('Directive on electronic commerce') (OJ L 178, 17.7.2000, p. 1). (3) small-scale provider means a provider that is a micro or small enterprise within the meaning of Commission Recommendation 2003/361/EC61; (4) user means any natural or legal person, public authority, agency or other body using an AI system under its authority, except where the AI system is used in the course of a personal non-professional activity; (5) authorised representative means any natural or legal person established in the Union who has received a written mandate from a provider of an AI system to, respectively, perform and carry out on its behalf the obligations and procedures established by this Regulation; (6) importer means any natural or legal person established in the Union that places on the market or puts into service an AI system that bears the name or trademark of a natural or legal person established outside the Union; (7) distributor means any natural or legal person in the supply chain, other than the provider or the importer, that makes an AI system available on the Union market without affecting its properties; (8) operator means the provider, the user, the authorised representative, the importer and the distributor; (9) placing on the market means the first making available of an AI system on the Union market; (10) making available on the market means any supply of an AI system for distribution or use on the Union market in the course of a commercial activity, whether in return for payment or free of charge; (11) putting into service means the supply of an AI system for first use directly to the user or for own use on the Union market for its intended purpose; (12) intended purpose means the use for which",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      42
    ],
    "titles": [],
    "chunk_index": 74,
    "text": "an AI system is intended by the provider, including the specific context and conditions of use, as specified in the information supplied by the provider in the instructions for use, promotional or sales materials and statements, as well as in the technical documentation; (13) reasonably foreseeable misuse means the use of an AI system in a way that is not in accordance with its intended purpose, but which may result from reasonably foreseeable human behaviour or interaction with other systems; (14) safety component of a product or system means a component of a product or of a system which fulfils a safety function for that product or system or the failure or malfunctioning of which endangers the health and safety of persons or property; (15) instructions for use means the information provided by the provider to inform the user of in particular an AI system s intended purpose and proper use, inclusive of the specific geographical, behavioural or functional setting within which the high-risk AI system is intended to be used; (16) recall of an AI system means any measure aimed at achieving the return to the provider of an AI system made available to users; Commission Recommendation of 6 May 2003 concerning the definition of micro, small and medium- sized enterprises (OJ L 124, 20.5.2003, p. 36). (17) withdrawal of an AI system means any measure aimed at preventing the distribution, display and offer of an AI system; (18) performance of an AI system means the ability of an AI system to achieve its intended purpose; (19) notifying authority means the national authority responsible for setting up and carrying out the necessary procedures for the assessment, designation and notification of conformity assessment bodies and for their monitoring; (20) conformity assessment means the process of verifying whether the requirements set",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 75,
    "text": "out in Title III, Chapter 2 of this Regulation relating to an AI system have been fulfilled; (21) conformity assessment body means a body that performs third-party conformity assessment activities, including testing, certification and inspection; (22) notified body means a conformity assessment body designated in accordance with this Regulation and other relevant Union harmonisation legislation; (23) substantial modification means a change to the AI system following its placing on the market or putting into service which affects the compliance of the AI system with the requirements set out in Title III, Chapter 2 of this Regulation or results in a modification to the intended purpose for which the AI system has been assessed; (24) CE marking of conformity (CE marking) means a marking by which a provider indicates that an AI system is in conformity with the requirements set out in Title III, Chapter 2 of this Regulation and other applicable Union legislation harmonising the conditions for the marketing of products ( Union harmonisation legislation ) providing for its affixing; (25) post-market monitoring means all activities carried out by providers of AI systems to proactively collect and review experience gained from the use of AI systems they place on the market or put into service for the purpose of identifying any need to immediately apply any necessary corrective or preventive actions; (26) market surveillance authority means the national authority carrying out the activities and taking the measures pursuant to Regulation (EU) 2019/1020; (27) harmonised standard means a European standard as defined in Article 2(1)(c) of Regulation (EU) No 1025/2012; (28) common specifications means a document, other than a standard, containing technical solutions providing a means to, comply with certain requirements and obligations established under this Regulation; (29) training data means data used for training an AI system through fitting its",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      43
    ],
    "titles": [],
    "chunk_index": 76,
    "text": "learnable parameters, including the weights of a neural network; (30) validation data means data used for providing an evaluation of the trained AI system and for tuning its non-learnable parameters and its learning process, among other things, in order to prevent overfitting; whereas the validation dataset can be a separate dataset or part of the training dataset, either as a fixed or variable split; (31) testing data means data used for providing an independent evaluation of the trained and validated AI system in order to confirm the expected performance of that system before its placing on the market or putting into service; (32) input data means data provided to or directly acquired by an AI system on the basis of which the system produces an output; (33) biometric data means personal data resulting from specific technical processing relating to the physical, physiological or behavioural characteristics of a natural person, which allow or confirm the unique identification of that natural person, such as facial images or dactyloscopic data; (34) emotion recognition system means an AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data; (35) biometric categorisation system means an AI system for the purpose of assigning natural persons to specific categories, such as sex, age, hair colour, eye colour, tattoos, ethnic origin or sexual or political orientation, on the basis of their biometric data; (36) remote biometric identification system means an AI system for the purpose of identifying natural persons at a distance through the comparison of a person s biometric data with the biometric data contained in a reference database, and without prior knowledge of the user of the AI system whether the person will be present and can be identified ; (37) real-time remote biometric identification",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      44
    ],
    "titles": [
      "TITLE II"
    ],
    "chunk_index": 77,
    "text": "system means a remote biometric identification system whereby the capturing of biometric data, the comparison and the identification all occur without a significant delay. This comprises not only instant identification, but also limited short delays in order to avoid circumvention. (38) post remote biometric identification system means a remote biometric identification system other than a real-time remote biometric identification system; (39) publicly accessible space means any physical place accessible to the public, regardless of whether certain conditions for access may apply; (40) law enforcement authority means: (a) any public authority competent for the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security; or (b) any other body or entity entrusted by Member State law to exercise public authority and public powers for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security; (41) law enforcement means activities carried out by law enforcement authorities for the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security; (42) national supervisory authority means the authority to which a Member State assigns the responsibility for the implementation and application of this Regulation, for coordinating the activities entrusted to that Member State, for acting as the single contact point for the Commission, and for representing the Member State at the European Artificial Intelligence Board; (43) national competent authority means the national supervisory authority, the notifying authority and the market surveillance authority; (44) serious incident means any incident that directly or indirectly leads, might have led or might lead to any of the",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 78,
    "text": "following: (a) the death of a person or serious damage to a person s health, to property or the environment, (b) a serious and irreversible disruption of the management and operation of critical infrastructure. Article 4 Amendments to Annex I The Commission is empowered to adopt delegated acts in accordance with Article 73 to amend the list of techniques and approaches listed in Annex I, in order to update that list to market and technological developments on the basis of characteristics that are similar to the techniques and approaches listed therein. TITLE II PROHIBITED ARTIFICIAL INTELLIGENCE PRACTICES Article 5 1. The following artificial intelligence practices shall be prohibited: (a) the placing on the market, putting into service or use of an AI system that deploys subliminal techniques beyond a person s consciousness in order to materially distort a person s behaviour in a manner that causes or is likely to cause that person or another person physical or psychological harm; (b) the placing on the market, putting into service or use of an AI system that exploits any of the vulnerabilities of a specific group of persons due to their age, physical or mental disability, in order to materially distort the behaviour of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harm; (c) the placing on the market, putting into service or use of AI systems by public authorities or on their behalf for the evaluation or classification of the trustworthiness of natural persons over a certain period of time based on their social behaviour or known or predicted personal or personality characteristics, with the social score leading to either or both of the following: (i) detrimental or unfavourable treatment of certain natural persons",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      45
    ],
    "titles": [
      "2."
    ],
    "chunk_index": 79,
    "text": "or whole groups thereof in social contexts which are unrelated to the contexts in which the data was originally generated or collected; (ii) detrimental or unfavourable treatment of certain natural persons or whole groups thereof that is unjustified or disproportionate to their social behaviour or its gravity; (d) the use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement, unless and in as far as such use is strictly necessary for one of the following objectives: (i) the targeted search for specific potential victims of crime, including missing children; (ii) the prevention of a specific, substantial and imminent threat to the life or physical safety of natural persons or of a terrorist attack; (iii) the detection, localisation, identification or prosecution of a perpetrator or suspect of a criminal offence referred to in Article 2(2) of Council Framework Decision 2002/584/JHA62 and punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least three years, as determined by the law of that Member State. 2. The use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement for any of the objectives referred to in paragraph 1 point d) shall take into account the following elements: (a) the nature of the situation giving rise to the possible use, in particular the seriousness, probability and scale of the harm caused in the absence of the use of the system; (b) the consequences of the use of the system for the rights and freedoms of all persons concerned, in particular the seriousness, probability and scale of those consequences. In addition, the use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement for any of",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      46
    ],
    "titles": [
      "TITLE III"
    ],
    "chunk_index": 80,
    "text": "the objectives referred to in paragraph 1 point d) shall comply with necessary and proportionate safeguards and conditions in relation to the use, in particular as regards the temporal, geographic and personal limitations. 3. As regards paragraphs 1, point (d) and 2, each individual use for the purpose of law enforcement of a real-time remote biometric identification system in publicly accessible spaces shall be subject to a prior authorisation granted by a judicial authority or by an independent administrative authority of the Member State in which the use is to take place, issued upon a reasoned request and in accordance with the detailed rules of national law referred to in paragraph 4. However, in a duly justified situation of urgency, the use of the system may be commenced without an authorisation and the authorisation may be requested only during or after the use. The competent judicial or administrative authority shall only grant the authorisation where it is satisfied, based on objective evidence or clear indications presented to it, that the use of the real-time remote biometric identification system at issue is necessary for and proportionate to achieving one of the objectives specified in paragraph 1, point (d), as identified in the request. In deciding on the request, the competent judicial or administrative authority shall take into account the elements referred to in paragraph 2. 4. A Member State may decide to provide for the possibility to fully or partially authorise the use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement within the limits and under the Council Framework Decision 2002/584/JHA of 13 June 2002 on the European arrest warrant and the surrender procedures between Member States (OJ L 190, 18.7.2002, p. 1). conditions listed in paragraphs 1, point (d), 2 and 3.",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 81,
    "text": "That Member State shall lay down in its national law the necessary detailed rules for the request, issuance and exercise of, as well as supervision relating to, the authorisations referred to in paragraph 3. Those rules shall also specify in respect of which of the objectives listed in paragraph 1, point (d), including which of the criminal offences referred to in point (iii) thereof, the competent authorities may be authorised to use those systems for the purpose of law enforcement. TITLE III HIGH-RISK AI SYSTEMS CHAPTER 1 CLASSIFICATION OF AI SYSTEMS AS HIGH-RISK Article 6 Classification rules for high-risk AI systems 1. Irrespective of whether an AI system is placed on the market or put into service independently from the products referred to in points (a) and (b), that AI system shall be considered high-risk where both of the following conditions are fulfilled: (a) the AI system is intended to be used as a safety component of a product, or is itself a product, covered by the Union harmonisation legislation listed in Annex II; (b) the product whose safety component is the AI system, or the AI system itself as a product, is required to undergo a third-party conformity assessment with a view to the placing on the market or putting into service of that product pursuant to the Union harmonisation legislation listed in Annex II. 2. In addition to the high-risk AI systems referred to in paragraph 1, AI systems referred to in Annex III shall also be considered high-risk. Article 7 Amendments to Annex III 1. The Commission is empowered to adopt delegated acts in accordance with Article 73 to update the list in Annex III by adding high-risk AI systems where both of the following conditions are fulfilled: (a) the AI systems are intended to be used",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      47
    ],
    "titles": [
      "CHAPTER 2"
    ],
    "chunk_index": 82,
    "text": "in any of the areas listed in points 1 to 8 of Annex III; (b) the AI systems pose a risk of harm to the health and safety, or a risk of adverse impact on fundamental rights, that is, in respect of its severity and probability of occurrence, equivalent to or greater than the risk of harm or of adverse impact posed by the high-risk AI systems already referred to in Annex III. 2. When assessing for the purposes of paragraph 1 whether an AI system poses a risk of harm to the health and safety or a risk of adverse impact on fundamental rights that is equivalent to or greater than the risk of harm posed by the high-risk AI systems already referred to in Annex III, the Commission shall take into account the following criteria: (a) the intended purpose of the AI system; (b) the extent to which an AI system has been used or is likely to be used; (c) the extent to which the use of an AI system has already caused harm to the health and safety or adverse impact on the fundamental rights or has given rise to significant concerns in relation to the materialisation of such harm or adverse impact, as demonstrated by reports or documented allegations submitted to national competent authorities; (d) the potential extent of such harm or such adverse impact, in particular in terms of its intensity and its ability to affect a plurality of persons; (e) the extent to which potentially harmed or adversely impacted persons are dependent on the outcome produced with an AI system, in particular because for practical or legal reasons it is not reasonably possible to opt-out from that outcome; (f) the extent to which potentially harmed or adversely impacted persons are in a vulnerable",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      48
    ],
    "titles": [
      "3."
    ],
    "chunk_index": 83,
    "text": "position in relation to the user of an AI system, in particular due to an imbalance of power, knowledge, economic or social circumstances, or age; (g) the extent to which the outcome produced with an AI system is easily reversible, whereby outcomes having an impact on the health or safety of persons shall not be considered as easily reversible; (h) the extent to which existing Union legislation provides for: (i) effective measures of redress in relation to the risks posed by an AI system, with the exclusion of claims for damages; (ii) effective measures to prevent or substantially minimise those risks. CHAPTER 2 REQUIREMENTS FOR HIGH-RISK AI SYSTEMS Article 8 Compliance with the requirements 1. High-risk AI systems shall comply with the requirements established in this Chapter. 2. The intended purpose of the high-risk AI system and the risk management system referred to in Article 9 shall be taken into account when ensuring compliance with those requirements. Article 9 Risk management system 1. A risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems. 2. The risk management system shall consist of a continuous iterative process run throughout the entire lifecycle of a high-risk AI system, requiring regular systematic updating. It shall comprise the following steps: (a) identification and analysis of the known and foreseeable risks associated with each high-risk AI system; (b) estimation and evaluation of the risks that may emerge when the high-risk AI system is used in accordance with its intended purpose and under conditions of reasonably foreseeable misuse; (c) evaluation of other possibly arising risks based on the analysis of data gathered from the post-market monitoring system referred to in Article 61; (d) adoption of suitable risk management measures in accordance with the provisions of the following paragraphs. 3. The",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 84,
    "text": "risk management measures referred to in paragraph 2, point (d) shall give due consideration to the effects and possible interactions resulting from the combined application of the requirements set out in this Chapter 2. They shall take into account the generally acknowledged state of the art, including as reflected in relevant harmonised standards or common specifications. 4. The risk management measures referred to in paragraph 2, point (d) shall be such that any residual risk associated with each hazard as well as the overall residual risk of the high-risk AI systems is judged acceptable, provided that the high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse. Those residual risks shall be communicated to the user. In identifying the most appropriate risk management measures, the following shall be ensured: (a) elimination or reduction of risks as far as possible through adequate design and development; (b) where appropriate, implementation of adequate mitigation and control measures in relation to risks that cannot be eliminated; (c) provision of adequate information pursuant to Article 13, in particular as regards the risks referred to in paragraph 2, point (b) of this Article, and, where appropriate, training to users. In eliminating or reducing risks related to the use of the high-risk AI system, due consideration shall be given to the technical knowledge, experience, education, training to be expected by the user and the environment in which the system is intended to be used. 5. High-risk AI systems shall be tested for the purposes of identifying the most appropriate risk management measures. Testing shall ensure that high-risk AI systems perform consistently for their intended purpose and they are in compliance with the requirements set out in this Chapter. 6. Testing procedures shall be suitable to achieve the intended purpose",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      49
    ],
    "titles": [
      "8."
    ],
    "chunk_index": 85,
    "text": "of the AI system and do not need to go beyond what is necessary to achieve that purpose. 7. The testing of the high-risk AI systems shall be performed, as appropriate, at any point in time throughout the development process, and, in any event, prior to the placing on the market or the putting into service. Testing shall be made against preliminarily defined metrics and probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI system. 8. When implementing the risk management system described in paragraphs 1 to 7, specific consideration shall be given to whether the high-risk AI system is likely to be accessed by or have an impact on children. 9. For credit institutions regulated by Directive 2013/36/EU, the aspects described in paragraphs 1 to 8 shall be part of the risk management procedures established by those institutions pursuant to Article 74 of that Directive. Article 10 Data and data governance 1. High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5. 2. Training, validation and testing data sets shall be subject to appropriate data governance and management practices. Those practices shall concern in particular, (a) the relevant design choices; (b) data collection; (c) relevant data preparation processing operations, such as annotation, labelling, cleaning, enrichment and aggregation; (d) the formulation of relevant assumptions, notably with respect to the information that the data are supposed to measure and represent; (e) a prior assessment of the availability, quantity and suitability of the data sets that are needed; (f) examination in view of possible biases; (g) the identification of any possible data gaps or shortcomings, and how those",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      50
    ],
    "titles": [
      "6."
    ],
    "chunk_index": 86,
    "text": "gaps and shortcomings can be addressed. 3. Training, validation and testing data sets shall be relevant, representative, free of errors and complete. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons on which the high-risk AI system is intended to be used. These characteristics of the data sets may be met at the level of individual data sets or a combination thereof. 4. Training, validation and testing data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, behavioural or functional setting within which the high- risk AI system is intended to be used. 5. To the extent that it is strictly necessary for the purposes of ensuring bias monitoring, detection and correction in relation to the high-risk AI systems, the providers of such systems may process special categories of personal data referred to in Article 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680 and Article 10(1) of Regulation (EU) 2018/1725, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons, including technical limitations on the re-use and use of state-of-the-art security and privacy-preserving measures, such as pseudonymisation, or encryption where anonymisation may significantly affect the purpose pursued. 6. Appropriate data governance and management practices shall apply for the development of high-risk AI systems other than those which make use of techniques involving the training of models in order to ensure that those high-risk AI systems comply with paragraph 2. Article 11 Technical documentation 1. The technical documentation of a high-risk AI system shall be drawn up before that system is placed on the market or put into service and shall be kept up-to date. The technical documentation shall be drawn",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 87,
    "text": "up in such a way to demonstrate that the high-risk AI system complies with the requirements set out in this Chapter and provide national competent authorities and notified bodies with all the necessary information to assess the compliance of the AI system with those requirements. It shall contain, at a minimum, the elements set out in Annex IV. 2. Where a high-risk AI system related to a product, to which the legal acts listed in Annex II, section A apply, is placed on the market or put into service one single technical documentation shall be drawn up containing all the information set out in Annex IV as well as the information required under those legal acts. 3. The Commission is empowered to adopt delegated acts in accordance with Article 73 to amend Annex IV where necessary to ensure that, in the light of technical progress, the technical documentation provides all the necessary information to assess the compliance of the system with the requirements set out in this Chapter. Article 12 Record-keeping 1. High-risk AI systems shall be designed and developed with capabilities enabling the automatic recording of events ( logs ) while the high-risk AI systems is operating. Those logging capabilities shall conform to recognised standards or common specifications. 2. The logging capabilities shall ensure a level of traceability of the AI system s functioning throughout its lifecycle that is appropriate to the intended purpose of the system. 3. In particular, logging capabilities shall enable the monitoring of the operation of the high-risk AI system with respect to the occurrence of situations that may result in the AI system presenting a risk within the meaning of Article 65(1) or lead to a substantial modification, and facilitate the post-market monitoring referred to in Article 61. 4. For high-risk AI systems referred",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      51
    ],
    "titles": [
      "1."
    ],
    "chunk_index": 88,
    "text": "to in paragraph 1, point (a) of Annex III, the logging capabilities shall provide, at a minimum: (a) recording of the period of each use of the system (start date and time and end date and time of each use); (b) the reference database against which input data has been checked by the system; (c) the input data for which the search has led to a match; (d) the identification of the natural persons involved in the verification of the results, as referred to in Article 14 (5). Article 13 Transparency and provision of information to users 1. High-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable users to interpret the system s output and use it appropriately. An appropriate type and degree of transparency shall be ensured, with a view to achieving compliance with the relevant obligations of the user and of the provider set out in Chapter 3 of this Title. 2. High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to users. 3. The information referred to in paragraph 2 shall specify: (a) the identity and the contact details of the provider and, where applicable, of its authorised representative; (b) the characteristics, capabilities and limitations of performance of the high-risk AI system, including: (i) its intended purpose; (ii) the level of accuracy, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity; (iii) any known or foreseeable circumstance,",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      52
    ],
    "titles": [
      "1."
    ],
    "chunk_index": 89,
    "text": "related to the use of the high- risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights; (iv) its performance as regards the persons or groups of persons on which the system is intended to be used; (v) when appropriate, specifications for the input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the AI system. (c) the changes to the high-risk AI system and its performance which have been pre-determined by the provider at the moment of the initial conformity assessment, if any; (d) the human oversight measures referred to in Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of AI systems by the users; (e) the expected lifetime of the high-risk AI system and any necessary maintenance and care measures to ensure the proper functioning of that AI system, including as regards software updates. Article 14 Human oversight 1. High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use. 2. Human oversight shall aim at preventing or minimising the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter. 3. Human oversight shall be ensured through either one or all of the following measures: (a) identified and built, when technically feasible,",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 90,
    "text": "into the high-risk AI system by the provider before it is placed on the market or put into service; (b) identified by the provider before placing the high-risk AI system on the market or putting it into service and that are appropriate to be implemented by the user. 4. The measures referred to in paragraph 3 shall enable the individuals to whom human oversight is assigned to do the following, as appropriate to the circumstances: (a) fully understand the capacities and limitations of the high-risk AI system and be able to duly monitor its operation, so that signs of anomalies, dysfunctions and unexpected performance can be detected and addressed as soon as possible; (b) remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system ( automation bias ), in particular for high-risk AI systems used to provide information or recommendations for decisions to be taken by natural persons; (c) be able to correctly interpret the high-risk AI system s output, taking into account in particular the characteristics of the system and the interpretation tools and methods available; (d) be able to decide, in any particular situation, not to use the high-risk AI system or otherwise disregard, override or reverse the output of the high-risk AI system; (e) be able to intervene on the operation of the high-risk AI system or interrupt the system through a stop button or a similar procedure. 5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the user on the basis of the identification resulting from the system unless this has been verified and confirmed by at least two",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      53
    ],
    "titles": [
      "2."
    ],
    "chunk_index": 91,
    "text": "natural persons. Article 15 Accuracy, robustness and cybersecurity 1. High-risk AI systems shall be designed and developed in such a way that they achieve, in the light of their intended purpose, an appropriate level of accuracy, robustness and cybersecurity, and perform consistently in those respects throughout their lifecycle. 2. The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying instructions of use. 3. High-risk AI systems shall be resilient as regards errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems. The robustness of high-risk AI systems may be achieved through technical redundancy solutions, which may include backup or fail-safe plans. High-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way to ensure that possibly biased outputs due to outputs used as an input for future operations ( feedback loops ) are duly addressed with appropriate mitigation measures. 4. High-risk AI systems shall be resilient as regards attempts by unauthorised third parties to alter their use or performance by exploiting the system vulnerabilities. The technical solutions aimed at ensuring the cybersecurity of high-risk AI systems shall be appropriate to the relevant circumstances and the risks. The technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent and control for attacks trying to manipulate the training dataset ( data poisoning ), inputs designed to cause the model to make a mistake ( adversarial examples ), or model flaws. CHAPTER 3 OBLIGATIONS OF PROVIDERS AND USERS OF HIGH-RISK AI SYSTEMS AND OTHER PARTIES Article 16 Obligations of providers of high-risk AI systems Providers of high-risk",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      54
    ],
    "titles": [
      "1."
    ],
    "chunk_index": 92,
    "text": "AI systems shall: (a) ensure that their high-risk AI systems are compliant with the requirements set out in Chapter 2 of this Title; (b) have a quality management system in place which complies with Article 17; (c) draw-up the technical documentation of the high-risk AI system; (d) when under their control, keep the logs automatically generated by their high-risk AI systems; (e) ensure that the high-risk AI system undergoes the relevant conformity assessment procedure, prior to its placing on the market or putting into service; (f) comply with the registration obligations referred to in Article 51; (g) take the necessary corrective actions, if the high-risk AI system is not in conformity with the requirements set out in Chapter 2 of this Title; (h) inform the national competent authorities of the Member States in which they made the AI system available or put it into service and, where applicable, the notified body of the non-compliance and of any corrective actions taken; (i) to affix the CE marking to their high-risk AI systems to indicate the conformity with this Regulation in accordance with Article 49; (j) upon request of a national competent authority, demonstrate the conformity of the high-risk AI system with the requirements set out in Chapter 2 of this Title. Article 17 Quality management system 1. Providers of high-risk AI systems shall put a quality management system in place that ensures compliance with this Regulation. That system shall be documented in a systematic and orderly manner in the form of written policies, procedures and instructions, and shall include at least the following aspects: (a) a strategy for regulatory compliance, including compliance with conformity assessment procedures and procedures for the management of modifications to the high-risk AI system; (b) techniques, procedures and systematic actions to be used for the design, design",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      55
    ],
    "titles": [
      "2."
    ],
    "chunk_index": 93,
    "text": "control and design verification of the high-risk AI system; (c) techniques, procedures and systematic actions to be used for the development, quality control and quality assurance of the high-risk AI system; (d) examination, test and validation procedures to be carried out before, during and after the development of the high-risk AI system, and the frequency with which they have to be carried out; (e) technical specifications, including standards, to be applied and, where the relevant harmonised standards are not applied in full, the means to be used to ensure that the high-risk AI system complies with the requirements set out in Chapter 2 of this Title; (f) systems and procedures for data management, including data collection, data analysis, data labelling, data storage, data filtration, data mining, data aggregation, data retention and any other operation regarding the data that is performed before and for the purposes of the placing on the market or putting into service of high-risk AI systems; (g) the risk management system referred to in Article 9; (h) the setting-up, implementation and maintenance of a post-market monitoring system, in accordance with Article 61; (i) procedures related to the reporting of serious incidents and of malfunctioning in accordance with Article 62; (j) the handling of communication with national competent authorities, competent authorities, including sectoral ones, providing or supporting the access to data, notified bodies, other operators, customers or other interested parties; (k) systems and procedures for record keeping of all relevant documentation and information; (l) resource management, including security of supply related measures; (m) an accountability framework setting out the responsibilities of the management and other staff with regard to all aspects listed in this paragraph. 2. The implementation of aspects referred to in paragraph 1 shall be proportionate to the size of the provider s organisation. 3. For",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 94,
    "text": "providers that are credit institutions regulated by Directive 2013/36/ EU, the obligation to put a quality management system in place shall be deemed to be fulfilled by complying with the rules on internal governance arrangements, processes and mechanisms pursuant to Article 74 of that Directive. In that context, any harmonised standards referred to in Article 40 of this Regulation shall be taken into account. Article 18 Obligation to draw up technical documentation 1. Providers of high-risk AI systems shall draw up the technical documentation referred to in Article 11 in accordance with Annex IV. 2. Providers that are credit institutions regulated by Directive 2013/36/EU shall maintain the technical documentation as part of the documentation concerning internal governance, arrangements, processes and mechanisms pursuant to Article 74 of that Directive. Article 19 Conformity assessment 1. Providers of high-risk AI systems shall ensure that their systems undergo the relevant conformity assessment procedure in accordance with Article 43, prior to their placing on the market or putting into service. Where the compliance of the AI systems with the requirements set out in Chapter 2 of this Title has been demonstrated following that conformity assessment, the providers shall draw up an EU declaration of conformity in accordance with Article 48 and affix the CE marking of conformity in accordance with Article 49. 2. For high-risk AI systems referred to in point 5(b) of Annex III that are placed on the market or put into service by providers that are credit institutions regulated by Directive 2013/36/EU, the conformity assessment shall be carried out as part of the procedure referred to in Articles 97 to101 of that Directive. Article 20 Automatically generated logs 1. Providers of high-risk AI systems shall keep the logs automatically generated by their high-risk AI systems, to the extent such logs are under",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      56
    ],
    "titles": [
      "1."
    ],
    "chunk_index": 95,
    "text": "their control by virtue of a contractual arrangement with the user or otherwise by law. The logs shall be kept for a period that is appropriate in the light of the intended purpose of high-risk AI system and applicable legal obligations under Union or national law. 2. Providers that are credit institutions regulated by Directive 2013/36/EU shall maintain the logs automatically generated by their high-risk AI systems as part of the documentation under Articles 74 of that Directive. Article 21 Corrective actions Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system which they have placed on the market or put into service is not in conformity with this Regulation shall immediately take the necessary corrective actions to bring that system into conformity, to withdraw it or to recall it, as appropriate. They shall inform the distributors of the high-risk AI system in question and, where applicable, the authorised representative and importers accordingly. Article 22 Duty of information Where the high-risk AI system presents a risk within the meaning of Article 65(1) and that risk is known to the provider of the system, that provider shall immediately inform the national competent authorities of the Member States in which it made the system available and, where applicable, the notified body that issued a certificate for the high-risk AI system, in particular of the non-compliance and of any corrective actions taken. Article 23 Cooperation with competent authorities Providers of high-risk AI systems shall, upon request by a national competent authority, provide that authority with all the information and documentation necessary to demonstrate the conformity of the high-risk AI system with the requirements set out in Chapter 2 of this Title, in an official Union language determined by the Member State concerned. Upon a reasoned request",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      57
    ],
    "titles": [
      "1."
    ],
    "chunk_index": 96,
    "text": "from a national competent authority, providers shall also give that authority access to the logs automatically generated by the high-risk AI system, to the extent such logs are under their control by virtue of a contractual arrangement with the user or otherwise by law. Article 24 Obligations of product manufacturers Where a high-risk AI system related to products to which the legal acts listed in Annex II, section A, apply, is placed on the market or put into service together with the product manufactured in accordance with those legal acts and under the name of the product manufacturer, the manufacturer of the product shall take the responsibility of the compliance of the AI system with this Regulation and, as far as the AI system is concerned, have the same obligations imposed by the present Regulation on the provider. Article 25 Authorised representatives 1. Prior to making their systems available on the Union market, where an importer cannot be identified, providers established outside the Union shall, by written mandate, appoint an authorised representative which is established in the Union. 2. The authorised representative shall perform the tasks specified in the mandate received from the provider. The mandate shall empower the authorised representative to carry out the following tasks: (a) keep a copy of the EU declaration of conformity and the technical documentation at the disposal of the national competent authorities and national authorities referred to in Article 63(7); (b) provide a national competent authority, upon a reasoned request, with all the information and documentation necessary to demonstrate the conformity of a high-risk AI system with the requirements set out in Chapter 2 of this Title, including access to the logs automatically generated by the high-risk AI system to the extent such logs are under the control of the provider by virtue",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 97,
    "text": "of a contractual arrangement with the user or otherwise by law; (c) cooperate with competent national authorities, upon a reasoned request, on any action the latter takes in relation to the high-risk AI system. Article 26 Obligations of importers 1. Before placing a high-risk AI system on the market, importers of such system shall ensure that: (a) the appropriate conformity assessment procedure has been carried out by the provider of that AI system (b) the provider has drawn up the technical documentation in accordance with Annex IV; (c) the system bears the required conformity marking and is accompanied by the required documentation and instructions of use. 2. Where an importer considers or has reason to consider that a high-risk AI system is not in conformity with this Regulation, it shall not place that system on the market until that AI system has been brought into conformity. Where the high-risk AI system presents a risk within the meaning of Article 65(1), the importer shall inform the provider of the AI system and the market surveillance authorities to that effect. 3. Importers shall indicate their name, registered trade name or registered trade mark, and the address at which they can be contacted on the high-risk AI system or, where that is not possible, on its packaging or its accompanying documentation, as applicable. 4. Importers shall ensure that, while a high-risk AI system is under their responsibility, where applicable, storage or transport conditions do not jeopardise its compliance with the requirements set out in Chapter 2 of this Title. 5. Importers shall provide national competent authorities, upon a reasoned request, with all necessary information and documentation to demonstrate the conformity of a high-risk AI system with the requirements set out in Chapter 2 of this Title in a language which can be easily",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      58
    ],
    "titles": [
      "1."
    ],
    "chunk_index": 98,
    "text": "understood by that national competent authority, including access to the logs automatically generated by the high-risk AI system to the extent such logs are under the control of the provider by virtue of a contractual arrangement with the user or otherwise by law. They shall also cooperate with those authorities on any action national competent authority takes in relation to that system. Article 27 Obligations of distributors 1. Before making a high-risk AI system available on the market, distributors shall verify that the high-risk AI system bears the required CE conformity marking, that it is accompanied by the required documentation and instruction of use, and that the provider and the importer of the system, as applicable, have complied with the obligations set out in this Regulation. 2. Where a distributor considers or has reason to consider that a high-risk AI system is not in conformity with the requirements set out in Chapter 2 of this Title, it shall not make the high-risk AI system available on the market until that system has been brought into conformity with those requirements. Furthermore, where the system presents a risk within the meaning of Article 65(1), the distributor shall inform the provider or the importer of the system, as applicable, to that effect. 3. Distributors shall ensure that, while a high-risk AI system is under their responsibility, where applicable, storage or transport conditions do not jeopardise the compliance of the system with the requirements set out in Chapter 2 of this Title. 4. A distributor that considers or has reason to consider that a high-risk AI system which it has made available on the market is not in conformity with the requirements set out in Chapter 2 of this Title shall take the corrective actions necessary to bring that system into conformity with those",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      59
    ],
    "titles": [
      "1."
    ],
    "chunk_index": 99,
    "text": "requirements, to withdraw it or recall it or shall ensure that the provider, the importer or any relevant operator, as appropriate, takes those corrective actions. Where the high-risk AI system presents a risk within the meaning of Article 65(1), the distributor shall immediately inform the national competent authorities of the Member States in which it has made the product available to that effect, giving details, in particular, of the non-compliance and of any corrective actions taken. 5. Upon a reasoned request from a national competent authority, distributors of high- risk AI systems shall provide that authority with all the information and documentation necessary to demonstrate the conformity of a high-risk system with the requirements set out in Chapter 2 of this Title. Distributors shall also cooperate with that national competent authority on any action taken by that authority. Article 28 Obligations of distributors, importers, users or any other third-party 1. Any distributor, importer, user or other third-party shall be considered a provider for the purposes of this Regulation and shall be subject to the obligations of the provider under Article 16, in any of the following circumstances: (a) they place on the market or put into service a high-risk AI system under their name or trademark; (b) they modify the intended purpose of a high-risk AI system already placed on the market or put into service; (c) they make a substantial modification to the high-risk AI system. 2. Where the circumstances referred to in paragraph 1, point (b) or (c), occur, the provider that initially placed the high-risk AI system on the market or put it into service shall no longer be considered a provider for the purposes of this Regulation. Article 29 Obligations of users of high-risk AI systems 1. Users of high-risk AI systems shall use such systems",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 100,
    "text": "in accordance with the instructions of use accompanying the systems, pursuant to paragraphs 2 and 5. 2. The obligations in paragraph 1 are without prejudice to other user obligations under Union or national law and to the user s discretion in organising its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider. 3. Without prejudice to paragraph 1, to the extent the user exercises control over the input data, that user shall ensure that input data is relevant in view of the intended purpose of the high-risk AI system. 4. Users shall monitor the operation of the high-risk AI system on the basis of the instructions of use. When they have reasons to consider that the use in accordance with the instructions of use may result in the AI system presenting a risk within the meaning of Article 65(1) they shall inform the provider or distributor and suspend the use of the system. They shall also inform the provider or distributor when they have identified any serious incident or any malfunctioning within the meaning of Article 62 and interrupt the use of the AI system. In case the user is not able to reach the provider, Article 62 shall apply mutatis mutandis. For users that are credit institutions regulated by Directive 2013/36/EU, the monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by complying with the rules on internal governance arrangements, processes and mechanisms pursuant to Article 74 of that Directive. 5. Users of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system, to the extent such logs are under their control. The logs shall be kept for a period that is appropriate in the light of the intended purpose of the",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      60
    ],
    "titles": [
      "3."
    ],
    "chunk_index": 101,
    "text": "high- risk AI system and applicable legal obligations under Union or national law. Users that are credit institutions regulated by Directive 2013/36/EU shall maintain the logs as part of the documentation concerning internal governance arrangements, processes and mechanisms pursuant to Article 74 of that Directive. 6. Users of high-risk AI systems shall use the information provided under Article 13 to comply with their obligation to carry out a data protection impact assessment under Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, where applicable. CHAPTER 4 NOTIFIYING AUTHORITIES AND NOTIFIED BODIES Article 30 Notifying authorities 1. Each Member State shall designate or establish a notifying authority responsible for setting up and carrying out the necessary procedures for the assessment, designation and notification of conformity assessment bodies and for their monitoring. 2. Member States may designate a national accreditation body referred to in Regulation (EC) No 765/2008 as a notifying authority. 3. Notifying authorities shall be established, organised and operated in such a way that no conflict of interest arises with conformity assessment bodies and the objectivity and impartiality of their activities are safeguarded. 4. Notifying authorities shall be organised in such a way that decisions relating to the notification of conformity assessment bodies are taken by competent persons different from those who carried out the assessment of those bodies. 5. Notifying authorities shall not offer or provide any activities that conformity assessment bodies perform or any consultancy services on a commercial or competitive basis. 6. Notifying authorities shall safeguard the confidentiality of the information they obtain. 7. Notifying authorities shall have a sufficient number of competent personnel at their disposal for the proper performance of their tasks. 8. Notifying authorities shall make sure that conformity assessments are carried out in a proportionate manner, avoiding unnecessary burdens",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      61
    ],
    "titles": [
      "3."
    ],
    "chunk_index": 102,
    "text": "for providers and that notified bodies perform their activities taking due account of the size of an undertaking, the sector in which it operates, its structure and the degree of complexity of the AI system in question. Article 31 Application of a conformity assessment body for notification 1. Conformity assessment bodies shall submit an application for notification to the notifying authority of the Member State in which they are established. 2. The application for notification shall be accompanied by a description of the conformity assessment activities, the conformity assessment module or modules and the artificial intelligence technologies for which the conformity assessment body claims to be competent, as well as by an accreditation certificate, where one exists, issued by a national accreditation body attesting that the conformity assessment body fulfils the requirements laid down in Article 33. Any valid document related to existing designations of the applicant notified body under any other Union harmonisation legislation shall be added. 3. Where the conformity assessment body concerned cannot provide an accreditation certificate, it shall provide the notifying authority with the documentary evidence necessary for the verification, recognition and regular monitoring of its compliance with the requirements laid down in Article 33. For notified bodies which are designated under any other Union harmonisation legislation, all documents and certificates linked to those designations may be used to support their designation procedure under this Regulation, as appropriate. Article 32 Notification procedure 1. Notifying authorities may notify only conformity assessment bodies which have satisfied the requirements laid down in Article 33. 2. Notifying authorities shall notify the Commission and the other Member States using the electronic notification tool developed and managed by the Commission. 3. The notification shall include full details of the conformity assessment activities, the conformity assessment module or modules and the artificial intelligence",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 103,
    "text": "technologies concerned. 4. The conformity assessment body concerned may perform the activities of a notified body only where no objections are raised by the Commission or the other Member States within one month of a notification. 5. Notifying authorities shall notify the Commission and the other Member States of any subsequent relevant changes to the notification. Article 33 Notified bodies 1. Notified bodies shall verify the conformity of high-risk AI system in accordance with the conformity assessment procedures referred to in Article 43. 2. Notified bodies shall satisfy the organisational, quality management, resources and process requirements that are necessary to fulfil their tasks. 3. The organisational structure, allocation of responsibilities, reporting lines and operation of notified bodies shall be such as to ensure that there is confidence in the performance by and in the results of the conformity assessment activities that the notified bodies conduct. 4. Notified bodies shall be independent of the provider of a high-risk AI system in relation to which it performs conformity assessment activities. Notified bodies shall also be independent of any other operator having an economic interest in the high- risk AI system that is assessed, as well as of any competitors of the provider. 5. Notified bodies shall be organised and operated so as to safeguard the independence, objectivity and impartiality of their activities. Notified bodies shall document and implement a structure and procedures to safeguard impartiality and to promote and apply the principles of impartiality throughout their organisation, personnel and assessment activities. 6. Notified bodies shall have documented procedures in place ensuring that their personnel, committees, subsidiaries, subcontractors and any associated body or personnel of external bodies respect the confidentiality of the information which comes into their possession during the performance of conformity assessment activities, except when disclosure is required by law. The",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      62
    ],
    "titles": [
      "10."
    ],
    "chunk_index": 104,
    "text": "staff of notified bodies shall be bound to observe professional secrecy with regard to all information obtained in carrying out their tasks under this Regulation, except in relation to the notifying authorities of the Member State in which their activities are carried out. 7. Notified bodies shall have procedures for the performance of activities which take due account of the size of an undertaking, the sector in which it operates, its structure, the degree of complexity of the AI system in question. 8. Notified bodies shall take out appropriate liability insurance for their conformity assessment activities, unless liability is assumed by the Member State concerned in accordance with national law or that Member State is directly responsible for the conformity assessment. 9. Notified bodies shall be capable of carrying out all the tasks falling to them under this Regulation with the highest degree of professional integrity and the requisite competence in the specific field, whether those tasks are carried out by notified bodies themselves or on their behalf and under their responsibility. 10. Notified bodies shall have sufficient internal competences to be able to effectively evaluate the tasks conducted by external parties on their behalf. To that end, at all times and for each conformity assessment procedure and each type of high-risk AI system in relation to which they have been designated, the notified body shall have permanent availability of sufficient administrative, technical and scientific personnel who possess experience and knowledge relating to the relevant artificial intelligence technologies, data and data computing and to the requirements set out in Chapter 2 of this Title. 11. Notified bodies shall participate in coordination activities as referred to in Article 38. They shall also take part directly or be represented in European standardisation organisations, or ensure that they are aware and up to",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      63
    ],
    "titles": [
      "1."
    ],
    "chunk_index": 105,
    "text": "date in respect of relevant standards. 12. Notified bodies shall make available and submit upon request all relevant documentation, including the providers documentation, to the notifying authority referred to in Article 30 to allow it to conduct its assessment, designation, notification, monitoring and surveillance activities and to facilitate the assessment outlined in this Chapter. Article 34 Subsidiaries of and subcontracting by notified bodies 1. Where a notified body subcontracts specific tasks connected with the conformity assessment or has recourse to a subsidiary, it shall ensure that the subcontractor or the subsidiary meets the requirements laid down in Article 33 and shall inform the notifying authority accordingly. 2. Notified bodies shall take full responsibility for the tasks performed by subcontractors or subsidiaries wherever these are established. 3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of the provider. 4. Notified bodies shall keep at the disposal of the notifying authority the relevant documents concerning the assessment of the qualifications of the subcontractor or the subsidiary and the work carried out by them under this Regulation. Article 35 Identification numbers and lists of notified bodies designated under this Regulation 1. The Commission shall assign an identification number to notified bodies. It shall assign a single number, even where a body is notified under several Union acts. 2. The Commission shall make publicly available the list of the bodies notified under this Regulation, including the identification numbers that have been assigned to them and the activities for which they have been notified. The Commission shall ensure that the list is kept up to date. Article 36 Changes to notifications 1. Where a notifying authority has suspicions or has been informed that a notified body no longer meets the requirements laid down in Article 33, or that it",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 106,
    "text": "is failing to fulfil its obligations, that authority shall without delay investigate the matter with the utmost diligence. In that context, it shall inform the notified body concerned about the objections raised and give it the possibility to make its views known. If the notifying authority comes to the conclusion that the notified body investigation no longer meets the requirements laid down in Article 33 or that it is failing to fulfil its obligations, it shall restrict, suspend or withdraw the notification as appropriate, depending on the seriousness of the failure. It shall also immediately inform the Commission and the other Member States accordingly. 2. In the event of restriction, suspension or withdrawal of notification, or where the notified body has ceased its activity, the notifying authority shall take appropriate steps to ensure that the files of that notified body are either taken over by another notified body or kept available for the responsible notifying authorities at their request. Article 37 Challenge to the competence of notified bodies 1. The Commission shall, where necessary, investigate all cases where there are reasons to doubt whether a notified body complies with the requirements laid down in Article 33. 2. The Notifying authority shall provide the Commission, on request, with all relevant information relating to the notification of the notified body concerned. 3. The Commission shall ensure that all confidential information obtained in the course of its investigations pursuant to this Article is treated confidentially. 4. Where the Commission ascertains that a notified body does not meet or no longer meets the requirements laid down in Article 33, it shall adopt a reasoned decision requesting the notifying Member State to take the necessary corrective measures, including withdrawal of notification if necessary. That implementing act shall be adopted in accordance with the examination procedure",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      64
    ],
    "titles": [
      "CHAPTER 5"
    ],
    "chunk_index": 107,
    "text": "referred to in Article 74(2). Article 38 Coordination of notified bodies 1. The Commission shall ensure that, with regard to the areas covered by this Regulation, appropriate coordination and cooperation between notified bodies active in the conformity assessment procedures of AI systems pursuant to this Regulation are put in place and properly operated in the form of a sectoral group of notified bodies. 2. Member States shall ensure that the bodies notified by them participate in the work of that group, directly or by means of designated representatives. Article 39 Conformity assessment bodies of third countries Conformity assessment bodies established under the law of a third country with which the Union has concluded an agreement may be authorised to carry out the activities of notified Bodies under this Regulation. CHAPTER 5 STANDARDS, CONFORMITY ASSESSMENT, CERTIFICATES, REGISTRATION Article 40 Harmonised standards High-risk AI systems which are in conformity with harmonised standards or parts thereof the references of which have been published in the Official Journal of the European Union shall be presumed to be in conformity with the requirements set out in Chapter 2 of this Title, to the extent those standards cover those requirements. Article 41 Common specifications 1. Where harmonised standards referred to in Article 40 do not exist or where the Commission considers that the relevant harmonised standards are insufficient or that there is a need to address specific safety or fundamental right concerns, the Commission may, by means of implementing acts, adopt common specifications in respect of the requirements set out in Chapter 2 of this Title. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 74(2). 2. The Commission, when preparing the common specifications referred to in paragraph 1, shall gather the views of relevant bodies or expert groups",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      65
    ],
    "titles": [
      "2."
    ],
    "chunk_index": 108,
    "text": "established under relevant sectorial Union law. 3. High-risk AI systems which are in conformity with the common specifications referred to in paragraph 1 shall be presumed to be in conformity with the requirements set out in Chapter 2 of this Title, to the extent those common specifications cover those requirements. 4. Where providers do not comply with the common specifications referred to in paragraph 1, they shall duly justify that they have adopted technical solutions that are at least equivalent thereto. Article 42 Presumption of conformity with certain requirements 1. Taking into account their intended purpose, high-risk AI systems that have been trained and tested on data concerning the specific geographical, behavioural and functional setting within which they are intended to be used shall be presumed to be in compliance with the requirement set out in Article 10(4). 2. High-risk AI systems that have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 of the European Parliament and of the Council63 and the references of which have been published in the Official Journal of the European Union shall be presumed to be in compliance with the cybersecurity requirements set out in Article 15 of this Regulation in so far as the cybersecurity certificate or statement of conformity or parts thereof cover those requirements. Article 43 Conformity assessment 1. For high-risk AI systems listed in point 1 of Annex III, where, in demonstrating the compliance of a high-risk AI system with the requirements set out in Chapter 2 of this Title, the provider has applied harmonised standards referred to in Article 40, or, where applicable, common specifications referred to in Article 41, the provider shall follow one of the following procedures: (a) the conformity assessment procedure based on internal",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 109,
    "text": "control referred to in Annex VI; (b) the conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation, with the involvement of a notified body, referred to in Annex VII. Where, in demonstrating the compliance of a high-risk AI system with the requirements set out in Chapter 2 of this Title, the provider has not applied or has applied only in part harmonised standards referred to in Article 40, or where such harmonised standards do not exist and common specifications referred to in Article 41 are not available, the provider shall follow the conformity assessment procedure set out in Annex VII. For the purpose of the conformity assessment procedure referred to in Annex VII, the provider may choose any of the notified bodies. However, when the system is intended to be put into service by law enforcement, immigration or asylum authorities as well as EU institutions, bodies or agencies, the market surveillance authority referred to in Article 63(5) or (6), as applicable, shall act as a notified body. 2. For high-risk AI systems referred to in points 2 to 8 of Annex III, providers shall follow the conformity assessment procedure based on internal control as referred to in Annex VI, which does not provide for the involvement of a notified body. For high-risk AI systems referred to in point 5(b) of Annex III, placed on the market or put into service by credit institutions regulated by Directive 2013/36/EU, the conformity assessment shall be carried out as part of the procedure referred to in Articles 97 to101 of that Directive. 3. For high-risk AI systems, to which legal acts listed in Annex II, section A, apply, the provider shall follow the relevant conformity assessment as required under those legal acts. The requirements set out",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      66
    ],
    "titles": [
      "4."
    ],
    "chunk_index": 110,
    "text": "in Chapter 2 of this Title shall apply to those Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA (the European Union Agency for Cybersecurity) and on information and communications technology cybersecurity certification and repealing Regulation (EU) No 526/2013 (Cybersecurity Act) (OJ L 151, 7.6.2019, p. 1). high-risk AI systems and shall be part of that assessment. Points 4.3., 4.4., 4.5. and the fifth paragraph of point 4.6 of Annex VII shall also apply. For the purpose of that assessment, notified bodies which have been notified under those legal acts shall be entitled to control the conformity of the high-risk AI systems with the requirements set out in Chapter 2 of this Title, provided that the compliance of those notified bodies with requirements laid down in Article 33(4), (9) and (10) has been assessed in the context of the notification procedure under those legal acts. Where the legal acts listed in Annex II, section A, enable the manufacturer of the product to opt out from a third-party conformity assessment, provided that that manufacturer has applied all harmonised standards covering all the relevant requirements, that manufacturer may make use of that option only if he has also applied harmonised standards or, where applicable, common specifications referred to in Article 41, covering the requirements set out in Chapter 2 of this Title. 4. High-risk AI systems shall undergo a new conformity assessment procedure whenever they are substantially modified, regardless of whether the modified system is intended to be further distributed or continues to be used by the current user. For high-risk AI systems that continue to learn after being placed on the market or put into service, changes to the high-risk AI system and its performance that have been pre-determined by the provider at",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 111,
    "text": "the moment of the initial conformity assessment and are part of the information contained in the technical documentation referred to in point 2(f) of Annex IV, shall not constitute a substantial modification. 5. The Commission is empowered to adopt delegated acts in accordance with Article 73 for the purpose of updating Annexes VI and Annex VII in order to introduce elements of the conformity assessment procedures that become necessary in light of technical progress. 6. The Commission is empowered to adopt delegated acts to amend paragraphs 1 and 2 in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity assessment procedure referred to in Annex VII or parts thereof. The Commission shall adopt such delegated acts taking into account the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in preventing or minimizing the risks to health and safety and protection of fundamental rights posed by such systems as well as the availability of adequate capacities and resources among notified bodies. Article 44 Certificates 1. Certificates issued by notified bodies in accordance with Annex VII shall be drawn- up in an official Union language determined by the Member State in which the notified body is established or in an official Union language otherwise acceptable to the notified body. 2. Certificates shall be valid for the period they indicate, which shall not exceed five years. On application by the provider, the validity of a certificate may be extended for further periods, each not exceeding five years, based on a re-assessment in accordance with the applicable conformity assessment procedures. 3. Where a notified body finds that an AI system no longer meets the requirements set out in Chapter 2 of this Title, it shall, taking account",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      67
    ],
    "titles": [
      "1."
    ],
    "chunk_index": 112,
    "text": "of the principle of proportionality, suspend or withdraw the certificate issued or impose any restrictions on it, unless compliance with those requirements is ensured by appropriate corrective action taken by the provider of the system within an appropriate deadline set by the notified body. The notified body shall give reasons for its decision. Article 45 Appeal against decisions of notified bodies Member States shall ensure that an appeal procedure against decisions of the notified bodies is available to parties having a legitimate interest in that decision. Article 46 Information obligations of notified bodies 1. Notified bodies shall inform the notifying authority of the following: (a) any Union technical documentation assessment certificates, any supplements to those certificates, quality management system approvals issued in accordance with the requirements of Annex VII; (b) any refusal, restriction, suspension or withdrawal of a Union technical documentation assessment certificate or a quality management system approval issued in accordance with the requirements of Annex VII; (c) any circumstances affecting the scope of or conditions for notification; (d) any request for information which they have received from market surveillance authorities regarding conformity assessment activities; (e) on request, conformity assessment activities performed within the scope of their notification and any other activity performed, including cross-border activities and subcontracting. 2. Each notified body shall inform the other notified bodies of: (a) quality management system approvals which it has refused, suspended or withdrawn, and, upon request, of quality system approvals which it has issued; (b) EU technical documentation assessment certificates or any supplements thereto which it has refused, withdrawn, suspended or otherwise restricted, and, upon request, of the certificates and/or supplements thereto which it has issued. 3. Each notified body shall provide the other notified bodies carrying out similar conformity assessment activities covering the same artificial intelligence technologies with relevant information",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      68
    ],
    "titles": [
      "2."
    ],
    "chunk_index": 113,
    "text": "on issues relating to negative and, on request, positive conformity assessment results. Article 47 Derogation from conformity assessment procedure 1. By way of derogation from Article 43, any market surveillance authority may authorise the placing on the market or putting into service of specific high-risk AI systems within the territory of the Member State concerned, for exceptional reasons of public security or the protection of life and health of persons, environmental protection and the protection of key industrial and infrastructural assets. That authorisation shall be for a limited period of time, while the necessary conformity assessment procedures are being carried out, and shall terminate once those procedures have been completed. The completion of those procedures shall be undertaken without undue delay. 2. The authorisation referred to in paragraph 1 shall be issued only if the market surveillance authority concludes that the high-risk AI system complies with the requirements of Chapter 2 of this Title. The market surveillance authority shall inform the Commission and the other Member States of any authorisation issued pursuant to paragraph 1. 3. Where, within 15 calendar days of receipt of the information referred to in paragraph 2, no objection has been raised by either a Member State or the Commission in respect of an authorisation issued by a market surveillance authority of a Member State in accordance with paragraph 1, that authorisation shall be deemed justified. 4. Where, within 15 calendar days of receipt of the notification referred to in paragraph 2, objections are raised by a Member State against an authorisation issued by a market surveillance authority of another Member State, or where the Commission considers the authorisation to be contrary to Union law or the conclusion of the Member States regarding the compliance of the system as referred to in paragraph 2 to be",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 114,
    "text": "unfounded, the Commission shall without delay enter into consultation with the relevant Member State; the operator(s) concerned shall be consulted and have the possibility to present their views. In view thereof, the Commission shall decide whether the authorisation is justified or not. The Commission shall address its decision to the Member State concerned and the relevant operator or operators. 5. If the authorisation is considered unjustified, this shall be withdrawn by the market surveillance authority of the Member State concerned. 6. By way of derogation from paragraphs 1 to 5, for high-risk AI systems intended to be used as safety components of devices, or which are themselves devices, covered by Regulation (EU) 2017/745 and Regulation (EU) 2017/746, Article 59 of Regulation (EU) 2017/745 and Article 54 of Regulation (EU) 2017/746 shall apply also with regard to the derogation from the conformity assessment of the compliance with the requirements set out in Chapter 2 of this Title. Article 48 EU declaration of conformity 1. The provider shall draw up a written EU declaration of conformity for each AI system and keep it at the disposal of the national competent authorities for 10 years after the AI system has been placed on the market or put into service. The EU declaration of conformity shall identify the AI system for which it has been drawn up. A copy of the EU declaration of conformity shall be given to the relevant national competent authorities upon request. 2. The EU declaration of conformity shall state that the high-risk AI system in question meets the requirements set out in Chapter 2 of this Title. The EU declaration of conformity shall contain the information set out in Annex V and shall be translated into an official Union language or languages required by the Member State(s) in which",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      69
    ],
    "titles": [
      "4."
    ],
    "chunk_index": 115,
    "text": "the high-risk AI system is made available. 3. Where high-risk AI systems are subject to other Union harmonisation legislation which also requires an EU declaration of conformity, a single EU declaration of conformity shall be drawn up in respect of all Union legislations applicable to the high-risk AI system. The declaration shall contain all the information required for identification of the Union harmonisation legislation to which the declaration relates. 4. By drawing up the EU declaration of conformity, the provider shall assume responsibility for compliance with the requirements set out in Chapter 2 of this Title. The provider shall keep the EU declaration of conformity up-to-date as appropriate. 5. The Commission shall be empowered to adopt delegated acts in accordance with Article 73 for the purpose of updating the content of the EU declaration of conformity set out in Annex V in order to introduce elements that become necessary in light of technical progress. Article 49 CE marking of conformity 1. The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems. Where that is not possible or not warranted on account of the nature of the high-risk AI system, it shall be affixed to the packaging or to the accompanying documentation, as appropriate. 2. The CE marking referred to in paragraph 1 of this Article shall be subject to the general principles set out in Article 30 of Regulation (EC) No 765/2008. 3. Where applicable, the CE marking shall be followed by the identification number of the notified body responsible for the conformity assessment procedures set out in Article 43. The identification number shall also be indicated in any promotional material which mentions that the high-risk AI system fulfils the requirements for CE marking. Article 50 Document retention The provider shall, for a period ending 10",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      70
    ],
    "titles": [
      "TITLE IV"
    ],
    "chunk_index": 116,
    "text": "years after the AI system has been placed on the market or put into service, keep at the disposal of the national competent authorities: (a) the technical documentation referred to in Article 11; (b) the documentation concerning the quality management system referred to Article 17; (c) the documentation concerning the changes approved by notified bodies where applicable; (d) the decisions and other documents issued by the notified bodies where applicable; (e) the EU declaration of conformity referred to in Article 48. Article 51 Registration Before placing on the market or putting into service a high-risk AI system referred to in Article 6(2), the provider or, where applicable, the authorised representative shall register that system in the EU database referred to in Article 60. TITLE IV TRANSPARENCY OBLIGATIONS FOR CERTAIN AI SYSTEMS Article 52 Transparency obligations for certain AI systems 1. Providers shall ensure that AI systems intended to interact with natural persons are designed and developed in such a way that natural persons are informed that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use. This obligation shall not apply to AI systems authorised by law to detect, prevent, investigate and prosecute criminal offences, unless those systems are available for the public to report a criminal offence. 2. Users of an emotion recognition system or a biometric categorisation system shall inform of the operation of the system the natural persons exposed thereto. This obligation shall not apply to AI systems used for biometric categorisation, which are permitted by law to detect, prevent and investigate criminal offences. 3. Users of an AI system that generates or manipulates image, audio or video content that appreciably resembles existing persons, objects, places or other entities or events and would falsely appear to a person",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      71
    ],
    "titles": [
      "3."
    ],
    "chunk_index": 117,
    "text": "to be authentic or truthful ( deep fake ), shall disclose that the content has been artificially generated or manipulated. However, the first subparagraph shall not apply where the use is authorised by law to detect, prevent, investigate and prosecute criminal offences or it is necessary for the exercise of the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to appropriate safeguards for the rights and freedoms of third parties. 4. Paragraphs 1, 2 and 3 shall not affect the requirements and obligations set out in Title III of this Regulation. TITLE V MEASURES IN SUPPORT OF INNOVATION Article 53 AI regulatory sandboxes 1. AI regulatory sandboxes established by one or more Member States competent authorities or the European Data Protection Supervisor shall provide a controlled environment that facilitates the development, testing and validation of innovative AI systems for a limited time before their placement on the market or putting into service pursuant to a specific plan. This shall take place under the direct supervision and guidance by the competent authorities with a view to ensuring compliance with the requirements of this Regulation and, where relevant, other Union and Member States legislation supervised within the sandbox. 2. Member States shall ensure that to the extent the innovative AI systems involve the processing of personal data or otherwise fall under the supervisory remit of other national authorities or competent authorities providing or supporting access to data, the national data protection authorities and those other national authorities are associated to the operation of the AI regulatory sandbox. 3. The AI regulatory sandboxes shall not affect the supervisory and corrective powers of the competent authorities. Any significant risks to health and safety and fundamental",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 118,
    "text": "rights identified during the development and testing of such systems shall result in immediate mitigation and, failing that, in the suspension of the development and testing process until such mitigation takes place. 4. Participants in the AI regulatory sandbox shall remain liable under applicable Union and Member States liability legislation for any harm inflicted on third parties as a result from the experimentation taking place in the sandbox. 5. Member States competent authorities that have established AI regulatory sandboxes shall coordinate their activities and cooperate within the framework of the European Artificial Intelligence Board. They shall submit annual reports to the Board and the Commission on the results from the implementation of those scheme, including good practices, lessons learnt and recommendations on their setup and, where relevant, on the application of this Regulation and other Union legislation supervised within the sandbox. 6. The modalities and the conditions of the operation of the AI regulatory sandboxes, including the eligibility criteria and the procedure for the application, selection, participation and exiting from the sandbox, and the rights and obligations of the participants shall be set out in implementing acts. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 74(2). Article 54 Further processing of personal data for developing certain AI systems in the public interest in the AI regulatory sandbox 1. In the AI regulatory sandbox personal data lawfully collected for other purposes shall be processed for the purposes of developing and testing certain innovative AI systems in the sandbox under the following conditions: (a) the innovative AI systems shall be developed for safeguarding substantial public interest in one or more of the following areas: (i) the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      72
    ],
    "titles": [
      "2."
    ],
    "chunk_index": 119,
    "text": "and the prevention of threats to public security, under the control and responsibility of the competent authorities. The processing shall be based on Member State or Union law; (ii) public safety and public health, including disease prevention, control and treatment; (iii) a high level of protection and improvement of the quality of the environment; (b) the data processed are necessary for complying with one or more of the requirements referred to in Title III, Chapter 2 where those requirements cannot be effectively fulfilled by processing anonymised, synthetic or other non-personal data; (c) there are effective monitoring mechanisms to identify if any high risks to the fundamental rights of the data subjects may arise during the sandbox experimentation as well as response mechanism to promptly mitigate those risks and, where necessary, stop the processing; (d) any personal data to be processed in the context of the sandbox are in a functionally separate, isolated and protected data processing environment under the control of the participants and only authorised persons have access to that data; (e) any personal data processed are not be transmitted, transferred or otherwise accessed by other parties; (f) any processing of personal data in the context of the sandbox do not lead to measures or decisions affecting the data subjects; (g) any personal data processed in the context of the sandbox are deleted once the participation in the sandbox has terminated or the personal data has reached the end of its retention period; (h) the logs of the processing of personal data in the context of the sandbox are kept for the duration of the participation in the sandbox and 1 year after its termination, solely for the purpose of and only as long as necessary for fulfilling accountability and documentation obligations under this Article or other application Union",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      73
    ],
    "titles": [
      "TITLE VI"
    ],
    "chunk_index": 120,
    "text": "or Member States legislation; (i) complete and detailed description of the process and rationale behind the training, testing and validation of the AI system is kept together with the testing results as part of the technical documentation in Annex IV; (j) a short summary of the AI project developed in the sandbox, its objectives and expected results published on the website of the competent authorities. 2. Paragraph 1 is without prejudice to Union or Member States legislation excluding processing for other purposes than those explicitly mentioned in that legislation. Article 55 Measures for small-scale providers and users 1. Member States shall undertake the following actions: (a) provide small-scale providers and start-ups with priority access to the AI regulatory sandboxes to the extent that they fulfil the eligibility conditions; (b) organise specific awareness raising activities about the application of this Regulation tailored to the needs of the small-scale providers and users; (c) where appropriate, establish a dedicated channel for communication with small-scale providers and user and other innovators to provide guidance and respond to queries about the implementation of this Regulation. 2. The specific interests and needs of the small-scale providers shall be taken into account when setting the fees for conformity assessment under Article 43, reducing those fees proportionately to their size and market size. TITLE VI GOVERNANCE CHAPTER 1 EUROPEAN ARTIFICIAL INTELLIGENCE BOARD Article 56 Establishment of the European Artificial Intelligence Board 1. A European Artificial Intelligence Board (the Board ) is established. 2. The Board shall provide advice and assistance to the Commission in order to: (a) contribute to the effective cooperation of the national supervisory authorities and the Commission with regard to matters covered by this Regulation; (b) coordinate and contribute to guidance and analysis by the Commission and the national supervisory authorities and other competent authorities",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      74
    ],
    "titles": [
      "CHAPTER 2"
    ],
    "chunk_index": 121,
    "text": "on emerging issues across the internal market with regard to matters covered by this Regulation; (c) assist the national supervisory authorities and the Commission in ensuring the consistent application of this Regulation. Article 57 Structure of the Board 1. The Board shall be composed of the national supervisory authorities, who shall be represented by the head or equivalent high-level official of that authority, and the European Data Protection Supervisor. Other national authorities may be invited to the meetings, where the issues discussed are of relevance for them. 2. The Board shall adopt its rules of procedure by a simple majority of its members, following the consent of the Commission. The rules of procedure shall also contain the operational aspects related to the execution of the Board s tasks as listed in Article 58. The Board may establish sub-groups as appropriate for the purpose of examining specific questions. 3. The Board shall be chaired by the Commission. The Commission shall convene the meetings and prepare the agenda in accordance with the tasks of the Board pursuant to this Regulation and with its rules of procedure. The Commission shall provide administrative and analytical support for the activities of the Board pursuant to this Regulation. 4. The Board may invite external experts and observers to attend its meetings and may hold exchanges with interested third parties to inform its activities to an appropriate extent. To that end the Commission may facilitate exchanges between the Board and other Union bodies, offices, agencies and advisory groups. Article 58 Tasks of the Board When providing advice and assistance to the Commission in the context of Article 56(2), the Board shall in particular: (a) collect and share expertise and best practices among Member States; (b) contribute to uniform administrative practices in the Member States, including for the",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 122,
    "text": "functioning of regulatory sandboxes referred to in Article 53; (c) issue opinions, recommendations or written contributions on matters related to the implementation of this Regulation, in particular (i) on technical specifications or existing standards regarding the requirements set out in Title III, Chapter 2, (ii) on the use of harmonised standards or common specifications referred to in Articles 40 and 41, (iii) on the preparation of guidance documents, including the guidelines concerning the setting of administrative fines referred to in Article 71. CHAPTER 2 NATIONAL COMPETENT AUTHORITIES Article 59 Designation of national competent authorities 1. National competent authorities shall be established or designated by each Member State for the purpose of ensuring the application and implementation of this Regulation. National competent authorities shall be organised so as to safeguard the objectivity and impartiality of their activities and tasks. 2. Each Member State shall designate a national supervisory authority among the national competent authorities. The national supervisory authority shall act as notifying authority and market surveillance authority unless a Member State has organisational and administrative reasons to designate more than one authority. 3. Member States shall inform the Commission of their designation or designations and, where applicable, the reasons for designating more than one authority. 4. Member States shall ensure that national competent authorities are provided with adequate financial and human resources to fulfil their tasks under this Regulation. In particular, national competent authorities shall have a sufficient number of personnel permanently available whose competences and expertise shall include an in-depth understanding of artificial intelligence technologies, data and data computing, fundamental rights, health and safety risks and knowledge of existing standards and legal requirements. 5. Member States shall report to the Commission on an annual basis on the status of the financial and human resources of the national competent authorities with",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      75
    ],
    "titles": [
      "7."
    ],
    "chunk_index": 123,
    "text": "an assessment of their adequacy. The Commission shall transmit that information to the Board for discussion and possible recommendations. 6. The Commission shall facilitate the exchange of experience between national competent authorities. 7. National competent authorities may provide guidance and advice on the implementation of this Regulation, including to small-scale providers. Whenever national competent authorities intend to provide guidance and advice with regard to an AI system in areas covered by other Union legislation, the competent national authorities under that Union legislation shall be consulted, as appropriate. Member States may also establish one central contact point for communication with operators. 8. When Union institutions, agencies and bodies fall within the scope of this Regulation, the European Data Protection Supervisor shall act as the competent authority for their supervision. TITLE VII EU DATABASE FOR STAND-ALONE HIGH-RISK AI SYSTEMS Article 60 EU database for stand-alone high-risk AI systems 1. The Commission shall, in collaboration with the Member States, set up and maintain a EU database containing information referred to in paragraph 2 concerning high-risk AI systems referred to in Article 6(2) which are registered in accordance with Article 51. 2. The data listed in Annex VIII shall be entered into the EU database by the providers. The Commission shall provide them with technical and administrative support. 3. Information contained in the EU database shall be accessible to the public. 4. The EU database shall contain personal data only insofar as necessary for collecting and processing information in accordance with this Regulation. That information shall include the names and contact details of natural persons who are responsible for registering the system and have the legal authority to represent the provider. 5. The Commission shall be the controller of the EU database. It shall also ensure to providers adequate technical and administrative support. TITLE",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      76
    ],
    "titles": [
      "2."
    ],
    "chunk_index": 124,
    "text": "VIII POST-MARKET MONITORING, INFORMATION SHARING, MARKET SURVEILLANCE CHAPTER 1 POST-MARKET MONITORING Article 61 Post-market monitoring by providers and post-market monitoring plan for high-risk AI systems 1. Providers shall establish and document a post-market monitoring system in a manner that is proportionate to the nature of the artificial intelligence technologies and the risks of the high-risk AI system. 2. The post-market monitoring system shall actively and systematically collect, document and analyse relevant data provided by users or collected through other sources on the performance of high-risk AI systems throughout their lifetime, and allow the provider to evaluate the continuous compliance of AI systems with the requirements set out in Title III, Chapter 2. 3. The post-market monitoring system shall be based on a post-market monitoring plan. The post-market monitoring plan shall be part of the technical documentation referred to in Annex IV. The Commission shall adopt an implementing act laying down detailed provisions establishing a template for the post-market monitoring plan and the list of elements to be included in the plan. 4. For high-risk AI systems covered by the legal acts referred to in Annex II, where a post-market monitoring system and plan is already established under that legislation, the elements described in paragraphs 1, 2 and 3 shall be integrated into that system and plan as appropriate. The first subparagraph shall also apply to high-risk AI systems referred to in point 5(b) of Annex III placed on the market or put into service by credit institutions regulated by Directive 2013/36/EU. CHAPTER 2 SHARING OF INFORMATION ON INCIDENTS AND MALFUNCTIONING Article 62 Reporting of serious incidents and of malfunctioning 1. Providers of high-risk AI systems placed on the Union market shall report any serious incident or any malfunctioning of those systems which constitutes a breach of obligations under Union law",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      77
    ],
    "titles": [
      "CHAPTER 3"
    ],
    "chunk_index": 125,
    "text": "intended to protect fundamental rights to the market surveillance authorities of the Member States where that incident or breach occurred. Such notification shall be made immediately after the provider has established a causal link between the AI system and the incident or malfunctioning or the reasonable likelihood of such a link, and, in any event, not later than 15 days after the providers becomes aware of the serious incident or of the malfunctioning. 2. Upon receiving a notification related to a breach of obligations under Union law intended to protect fundamental rights, the market surveillance authority shall inform the national public authorities or bodies referred to in Article 64(3). The Commission shall develop dedicated guidance to facilitate compliance with the obligations set out in paragraph 1. That guidance shall be issued 12 months after the entry into force of this Regulation, at the latest. 3. For high-risk AI systems referred to in point 5(b) of Annex III which are placed on the market or put into service by providers that are credit institutions regulated by Directive 2013/36/EU and for high-risk AI systems which are safety components of devices, or are themselves devices, covered by Regulation (EU) 2017/745 and Regulation (EU) 2017/746, the notification of serious incidents or malfunctioning shall be limited to those that that constitute a breach of obligations under Union law intended to protect fundamental rights. CHAPTER 3 ENFORCEMENT Article 63 Market surveillance and control of AI systems in the Union market 1. Regulation (EU) 2019/1020 shall apply to AI systems covered by this Regulation. However, for the purpose of the effective enforcement of this Regulation: (a) any reference to an economic operator under Regulation (EU) 2019/1020 shall be understood as including all operators identified in Title III, Chapter 3 of this Regulation; (b) any reference to a",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 126,
    "text": "product under Regulation (EU) 2019/1020 shall be understood as including all AI systems falling within the scope of this Regulation. 2. The national supervisory authority shall report to the Commission on a regular basis the outcomes of relevant market surveillance activities. The national supervisory authority shall report, without delay, to the Commission and relevant national competition authorities any information identified in the course of market surveillance activities that may be of potential interest for the application of Union law on competition rules. 3. For high-risk AI systems, related to products to which legal acts listed in Annex II, section A apply, the market surveillance authority for the purposes of this Regulation shall be the authority responsible for market surveillance activities designated under those legal acts. 4. For AI systems placed on the market, put into service or used by financial institutions regulated by Union legislation on financial services, the market surveillance authority for the purposes of this Regulation shall be the relevant authority responsible for the financial supervision of those institutions under that legislation. 5. For AI systems listed in point 1(a) in so far as the systems are used for law enforcement purposes, points 6 and 7 of Annex III, Member States shall designate as market surveillance authorities for the purposes of this Regulation either the competent data protection supervisory authorities under Directive (EU) 2016/680, or Regulation 2016/679 or the national competent authorities supervising the activities of the law enforcement, immigration or asylum authorities putting into service or using those systems. 6. Where Union institutions, agencies and bodies fall within the scope of this Regulation, the European Data Protection Supervisor shall act as their market surveillance authority. 7. Member States shall facilitate the coordination between market surveillance authorities designated under this Regulation and other relevant national authorities or bodies",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      78
    ],
    "titles": [
      "1."
    ],
    "chunk_index": 127,
    "text": "which supervise the application of Union harmonisation legislation listed in Annex II or other Union legislation that might be relevant for the high-risk AI systems referred to in Annex III. Article 64 Access to data and documentation 1. Access to data and documentation in the context of their activities, the market surveillance authorities shall be granted full access to the training, validation and testing datasets used by the provider, including through application programming interfaces ( API ) or other appropriate technical means and tools enabling remote access. 2. Where necessary to assess the conformity of the high-risk AI system with the requirements set out in Title III, Chapter 2 and upon a reasoned request, the market surveillance authorities shall be granted access to the source code of the AI system. 3. National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights in relation to the use of high-risk AI systems referred to in Annex III shall have the power to request and access any documentation created or maintained under this Regulation when access to that documentation is necessary for the fulfilment of the competences under their mandate within the limits of their jurisdiction. The relevant public authority or body shall inform the market surveillance authority of the Member State concerned of any such request. 4. By 3 months after the entering into force of this Regulation, each Member State shall identify the public authorities or bodies referred to in paragraph 3 and make a list publicly available on the website of the national supervisory authority. Member States shall notify the list to the Commission and all other Member States and keep the list up to date. 5. Where the documentation referred to in paragraph 3 is insufficient to ascertain whether a",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      79
    ],
    "titles": [
      "3."
    ],
    "chunk_index": 128,
    "text": "breach of obligations under Union law intended to protect fundamental rights has occurred, the public authority or body referred to paragraph 3 may make a reasoned request to the market surveillance authority to organise testing of the high- risk AI system through technical means. The market surveillance authority shall organise the testing with the close involvement of the requesting public authority or body within reasonable time following the request. 6. Any information and documentation obtained by the national public authorities or bodies referred to in paragraph 3 pursuant to the provisions of this Article shall be treated in compliance with the confidentiality obligations set out in Article 70. Article 65 Procedure for dealing with AI systems presenting a risk at national level 1. AI systems presenting a risk shall be understood as a product presenting a risk defined in Article 3, point 19 of Regulation (EU) 2019/1020 insofar as risks to the health or safety or to the protection of fundamental rights of persons are concerned. 2. Where the market surveillance authority of a Member State has sufficient reasons to consider that an AI system presents a risk as referred to in paragraph 1, they shall carry out an evaluation of the AI system concerned in respect of its compliance with all the requirements and obligations laid down in this Regulation. When risks to the protection of fundamental rights are present, the market surveillance authority shall also inform the relevant national public authorities or bodies referred to in Article 64(3). The relevant operators shall cooperate as necessary with the market surveillance authorities and the other national public authorities or bodies referred to in Article 64(3). Where, in the course of that evaluation, the market surveillance authority finds that the AI system does not comply with the requirements and obligations laid",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 129,
    "text": "down in this Regulation, it shall without delay require the relevant operator to take all appropriate corrective actions to bring the AI system into compliance, to withdraw the AI system from the market, or to recall it within a reasonable period, commensurate with the nature of the risk, as it may prescribe. The market surveillance authority shall inform the relevant notified body accordingly. Article 18 of Regulation (EU) 2019/1020 shall apply to the measures referred to in the second subparagraph. 3. Where the market surveillance authority considers that non-compliance is not restricted to its national territory, it shall inform the Commission and the other Member States of the results of the evaluation and of the actions which it has required the operator to take. 4. The operator shall ensure that all appropriate corrective action is taken in respect of all the AI systems concerned that it has made available on the market throughout the Union. 5. Where the operator of an AI system does not take adequate corrective action within the period referred to in paragraph 2, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AI system's being made available on its national market, to withdraw the product from that market or to recall it. That authority shall inform the Commission and the other Member States, without delay, of those measures. 6. The information referred to in paragraph 5 shall include all available details, in particular the data necessary for the identification of the non-compliant AI system, the origin of the AI system, the nature of the non-compliance alleged and the risk involved, the nature and duration of the national measures taken and the arguments put forward by the relevant operator. In particular, the market surveillance authorities shall indicate whether the non-compliance is",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      80
    ],
    "titles": [
      "9."
    ],
    "chunk_index": 130,
    "text": "due to one or more of the following: (a) a failure of the AI system to meet requirements set out in Title III, Chapter 2; (b) shortcomings in the harmonised standards or common specifications referred to in Articles 40 and 41 conferring a presumption of conformity. 7. The market surveillance authorities of the Member States other than the market surveillance authority of the Member State initiating the procedure shall without delay inform the Commission and the other Member States of any measures adopted and of any additional information at their disposal relating to the non-compliance of the AI system concerned, and, in the event of disagreement with the notified national measure, of their objections. 8. Where, within three months of receipt of the information referred to in paragraph 5, no objection has been raised by either a Member State or the Commission in respect of a provisional measure taken by a Member State, that measure shall be deemed justified. This is without prejudice to the procedural rights of the concerned operator in accordance with Article 18 of Regulation (EU) 2019/1020. 9. The market surveillance authorities of all Member States shall ensure that appropriate restrictive measures are taken in respect of the product concerned, such as withdrawal of the product from their market, without delay. Article 66 Union safeguard procedure 1. Where, within three months of receipt of the notification referred to in Article 65(5), objections are raised by a Member State against a measure taken by another Member State, or where the Commission considers the measure to be contrary to Union law, the Commission shall without delay enter into consultation with the relevant Member State and operator or operators and shall evaluate the national measure. On the basis of the results of that evaluation, the Commission shall decide whether the",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 131,
    "text": "national measure is justified or not within 9 months from the notification referred to in Article 65(5) and notify such decision to the Member State concerned. 2. If the national measure is considered justified, all Member States shall take the measures necessary to ensure that the non-compliant AI system is withdrawn from their market, and shall inform the Commission accordingly. If the national measure is considered unjustified, the Member State concerned shall withdraw the measure. 3. Where the national measure is considered justified and the non-compliance of the AI system is attributed to shortcomings in the harmonised standards or common specifications referred to in Articles 40 and 41 of this Regulation, the Commission shall apply the procedure provided for in Article 11 of Regulation (EU) No 1025/2012. Article 67 Compliant AI systems which present a risk 1. Where, having performed an evaluation under Article 65, the market surveillance authority of a Member State finds that although an AI system is in compliance with this Regulation, it presents a risk to the health or safety of persons, to the compliance with obligations under Union or national law intended to protect fundamental rights or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that the AI system concerned, when placed on the market or put into service, no longer presents that risk, to withdraw the AI system from the market or to recall it within a reasonable period, commensurate with the nature of the risk, as it may prescribe. 2. The provider or other relevant operators shall ensure that corrective action is taken in respect of all the AI systems concerned that they have made available on the market throughout the Union within the timeline prescribed by the market surveillance authority",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      81
    ],
    "titles": [
      "5."
    ],
    "chunk_index": 132,
    "text": "of the Member State referred to in paragraph 1. 3. The Member State shall immediately inform the Commission and the other Member States. That information shall include all available details, in particular the data necessary for the identification of the AI system concerned, the origin and the supply chain of the AI system, the nature of the risk involved and the nature and duration of the national measures taken. 4. The Commission shall without delay enter into consultation with the Member States and the relevant operator and shall evaluate the national measures taken. On the basis of the results of that evaluation, the Commission shall decide whether the measure is justified or not and, where necessary, propose appropriate measures. 5. The Commission shall address its decision to the Member States. Article 68 Formal non-compliance 1. Where the market surveillance authority of a Member State makes one of the following findings, it shall require the relevant provider to put an end to the non- compliance concerned: (a) the conformity marking has been affixed in violation of Article 49; (b) the conformity marking has not been affixed; (c) the EU declaration of conformity has not been drawn up; (d) the EU declaration of conformity has not been drawn up correctly; (e) the identification number of the notified body, which is involved in the conformity assessment procedure, where applicable, has not been affixed; 2. Where the non-compliance referred to in paragraph 1 persists, the Member State concerned shall take all appropriate measures to restrict or prohibit the high-risk AI system being made available on the market or ensure that it is recalled or withdrawn from the market. TITLE IX CODES OF CONDUCT Article 69 Codes of conduct 1. The Commission and the Member States shall encourage and facilitate the drawing up of codes",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      82
    ],
    "titles": [
      "4."
    ],
    "chunk_index": 133,
    "text": "of conduct intended to foster the voluntary application to AI systems other than high-risk AI systems of the requirements set out in Title III, Chapter 2 on the basis of technical specifications and solutions that are appropriate means of ensuring compliance with such requirements in light of the intended purpose of the systems. 2. The Commission and the Board shall encourage and facilitate the drawing up of codes of conduct intended to foster the voluntary application to AI systems of requirements related for example to environmental sustainability, accessibility for persons with a disability, stakeholders participation in the design and development of the AI systems and diversity of development teams on the basis of clear objectives and key performance indicators to measure the achievement of those objectives. 3. Codes of conduct may be drawn up by individual providers of AI systems or by organisations representing them or by both, including with the involvement of users and any interested stakeholders and their representative organisations. Codes of conduct may cover one or more AI systems taking into account the similarity of the intended purpose of the relevant systems. 4. The Commission and the Board shall take into account the specific interests and needs of the small-scale providers and start-ups when encouraging and facilitating the drawing up of codes of conduct. TITLE X CONFIDENTIALITY AND PENALTIES Article 70 Confidentiality 1. National competent authorities and notified bodies involved in the application of this Regulation shall respect the confidentiality of information and data obtained in carrying out their tasks and activities in such a manner as to protect, in particular: (a) intellectual property rights, and confidential business information or trade secrets of a natural or legal person, including source code, except the cases referred to in Article 5 of Directive 2016/943 on the protection of undisclosed",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      83
    ],
    "titles": [
      "1."
    ],
    "chunk_index": 134,
    "text": "know-how and business information (trade secrets) against their unlawful acquisition, use and disclosure apply. (b) the effective implementation of this Regulation, in particular for the purpose of inspections, investigations or audits;(c) public and national security interests; (c) integrity of criminal or administrative proceedings. 2. Without prejudice to paragraph 1, information exchanged on a confidential basis between the national competent authorities and between national competent authorities and the Commission shall not be disclosed without the prior consultation of the originating national competent authority and the user when high-risk AI systems referred to in points 1, 6 and 7 of Annex III are used by law enforcement, immigration or asylum authorities, when such disclosure would jeopardise public and national security interests. When the law enforcement, immigration or asylum authorities are providers of high- risk AI systems referred to in points 1, 6 and 7 of Annex III, the technical documentation referred to in Annex IV shall remain within the premises of those authorities. Those authorities shall ensure that the market surveillance authorities referred to in Article 63(5) and (6), as applicable, can, upon request, immediately access the documentation or obtain a copy thereof. Only staff of the market surveillance authority holding the appropriate level of security clearance shall be allowed to access that documentation or any copy thereof. 3. Paragraphs 1 and 2 shall not affect the rights and obligations of the Commission, Member States and notified bodies with regard to the exchange of information and the dissemination of warnings, nor the obligations of the parties concerned to provide information under criminal law of the Member States. 4. The Commission and Member States may exchange, where necessary, confidential information with regulatory authorities of third countries with which they have concluded bilateral or multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality. Article",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 135,
    "text": "71 Penalties 1. In compliance with the terms and conditions laid down in this Regulation, Member States shall lay down the rules on penalties, including administrative fines, applicable to infringements of this Regulation and shall take all measures necessary to ensure that they are properly and effectively implemented. The penalties provided for shall be effective, proportionate, and dissuasive. They shall take into particular account the interests of small-scale providers and start-up and their economic viability. 2. The Member States shall notify the Commission of those rules and of those measures and shall notify it, without delay, of any subsequent amendment affecting them. 3. The following infringements shall be subject to administrative fines of up to 30 000 000 EUR or, if the offender is company, up to 6 % of its total worldwide annual turnover for the preceding financial year, whichever is higher: (a) non-compliance with the prohibition of the artificial intelligence practices referred to in Article 5; (b) non-compliance of the AI system with the requirements laid down in Article 10. 4. The non-compliance of the AI system with any requirements or obligations under this Regulation, other than those laid down in Articles 5 and 10, shall be subject to administrative fines of up to 20 000 000 EUR or, if the offender is a company, up to 4 % of its total worldwide annual turnover for the preceding financial year, whichever is higher. 5. The supply of incorrect, incomplete or misleading information to notified bodies and national competent authorities in reply to a request shall be subject to administrative fines of up to 10 000 000 EUR or, if the offender is a company, up to 2 % of its total worldwide annual turnover for the preceding financial year, whichever is higher. 6. When deciding on the amount",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      84
    ],
    "titles": [
      "1."
    ],
    "chunk_index": 136,
    "text": "of the administrative fine in each individual case, all relevant circumstances of the specific situation shall be taken into account and due regard shall be given to the following: (a) the nature, gravity and duration of the infringement and of its consequences; (b) whether administrative fines have been already applied by other market surveillance authorities to the same operator for the same infringement. (c) the size and market share of the operator committing the infringement; 7. Each Member State shall lay down rules on whether and to what extent administrative fines may be imposed on public authorities and bodies established in that Member State. 8. Depending on the legal system of the Member States, the rules on administrative fines may be applied in such a manner that the fines are imposed by competent national courts of other bodies as applicable in those Member States. The application of such rules in those Member States shall have an equivalent effect. Article 72 Administrative fines on Union institutions, agencies and bodies 1. The European Data Protection Supervisor may impose administrative fines on Union institutions, agencies and bodies falling within the scope of this Regulation. When deciding whether to impose an administrative fine and deciding on the amount of the administrative fine in each individual case, all relevant circumstances of the specific situation shall be taken into account and due regard shall be given to the following: (a) the nature, gravity and duration of the infringement and of its consequences; (b) the cooperation with the European Data Protection Supervisor in order to remedy the infringement and mitigate the possible adverse effects of the infringement, including compliance with any of the measures previously ordered by the European Data Protection Supervisor against the Union institution or agency or body concerned with regard to the same subject",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      85
    ],
    "titles": [
      "2."
    ],
    "chunk_index": 137,
    "text": "matter; (c) any similar previous infringements by the Union institution, agency or body; 2. The following infringements shall be subject to administrative fines of up to 500 000 EUR: (a) non-compliance with the prohibition of the artificial intelligence practices referred to in Article 5; (b) non-compliance of the AI system with the requirements laid down in Article 10. 3. The non-compliance of the AI system with any requirements or obligations under this Regulation, other than those laid down in Articles 5 and 10, shall be subject to administrative fines of up to 250 000 EUR. 4. Before taking decisions pursuant to this Article, the European Data Protection Supervisor shall give the Union institution, agency or body which is the subject of the proceedings conducted by the European Data Protection Supervisor the opportunity of being heard on the matter regarding the possible infringement. The European Data Protection Supervisor shall base his or her decisions only on elements and circumstances on which the parties concerned have been able to comment. Complainants, if any, shall be associated closely with the proceedings. 5. The rights of defense of the parties concerned shall be fully respected in the proceedings. They shall be entitled to have access to the European Data Protection Supervisor s file, subject to the legitimate interest of individuals or undertakings in the protection of their personal data or business secrets. 6. Funds collected by imposition of fines in this Article shall be the income of the general budget of the Union. TITLE XI DELEGATION OF POWER AND COMMITTEE PROCEDURE Article 73 Exercise of the delegation 1. The power to adopt delegated acts is conferred on the Commission subject to the conditions laid down in this Article. 2. The delegation of power referred to in Article 4, Article 7(1), Article 11(3), Article 43(5)",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 138,
    "text": "and (6) and Article 48(5) shall be conferred on the Commission for an indeterminate period of time from [entering into force of the Regulation]. 3. The delegation of power referred to in Article 4, Article 7(1), Article 11(3), Article 43(5) and (6) and Article 48(5) may be revoked at any time by the European Parliament or by the Council. A decision of revocation shall put an end to the delegation of power specified in that decision. It shall take effect the day following that of its publication in the Official Journal of the European Union or at a later date specified therein. It shall not affect the validity of any delegated acts already in force. 4. As soon as it adopts a delegated act, the Commission shall notify it simultaneously to the European Parliament and to the Council. 5. Any delegated act adopted pursuant to Article 4, Article 7(1), Article 11(3), Article 43(5) and (6) and Article 48(5) shall enter into force only if no objection has been expressed by either the European Parliament or the Council within a period of three months of notification of that act to the European Parliament and the Council or if, before the expiry of that period, the European Parliament and the Council have both informed the Commission that they will not object. That period shall be extended by three months at the initiative of the European Parliament or of the Council. Article 74 Committee procedure 1. The Commission shall be assisted by a committee. That committee shall be a committee within the meaning of Regulation (EU) No 182/2011. 2. Where reference is made to this paragraph, Article 5 of Regulation (EU) No 182/2011 shall apply. TITLE XII FINAL PROVISIONS Article 75 Amendment to Regulation (EC) No 300/2008 In Article 4(3) of Regulation (EC)",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      86
    ],
    "titles": [],
    "chunk_index": 139,
    "text": "No 300/2008, the following subparagraph is added: When adopting detailed measures related to technical specifications and procedures for approval and use of security equipment concerning Artificial Intelligence systems in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Chapter 2, Title III of that Regulation shall be taken into account. __________ * Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ ). Article 76 Amendment to Regulation (EU) No 167/2013 In Article 17(5) of Regulation (EU) No 167/2013, the following subparagraph is added: When adopting delegated acts pursuant to the first subparagraph concerning artificial intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. __________ * Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ ). Article 77 Amendment to Regulation (EU) No 168/2013 In Article 22(5) of Regulation (EU) No 168/2013, the following subparagraph is added: When adopting delegated acts pursuant to the first subparagraph concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX on [Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. __________ * Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ ). Article 78 Amendment to Directive 2014/90/EU In Article 8 of Directive 2014/90/EU, the following paragraph is added: 4. For Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, when carrying out its activities pursuant to paragraph 1 and when adopting technical specifications and testing standards",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      87
    ],
    "titles": [
      "1 concerning Artificial Intelligence systems which are safety components in the meaning of"
    ],
    "chunk_index": 140,
    "text": "in accordance with paragraphs 2 and 3, the Commission shall take into account the requirements set out in Title III, Chapter 2 of that Regulation. __________ * Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ ). . Article 79 Amendment to Directive (EU) 2016/797 In Article 5 of Directive (EU) 2016/797, the following paragraph is added: 12. When adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to paragraph 11 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. __________ * Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ ). . Article 80 Amendment to Regulation (EU) 2018/858 In Article 5 of Regulation (EU) 2018/858 the following paragraph is added: 4. When adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council *, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. __________ * Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ ). . Article 81 Amendment to Regulation (EU) 2018/1139 Regulation (EU) 2018/1139 is amended as follows: (1) In Article 17, the following paragraph is added: 3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. __________ * Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ ). (2) In",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      88
    ],
    "titles": [
      "1."
    ],
    "chunk_index": 141,
    "text": "Article 19, the following paragraph is added: 4. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. (3) In Article 43, the following paragraph is added: 4. When adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. (4) In Article 47, the following paragraph is added: 3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. (5) In Article 57, the following paragraph is added: When adopting those implementing acts concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. (6) In Article 58, the following paragraph is added: 3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] , the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. . Article 82 Amendment to Regulation (EU) 2019/2144 In Article 11 of Regulation (EU) 2019/2144, the following paragraph is added: 3. When adopting the implementing acts pursuant to paragraph 2, concerning artificial intelligence systems",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 142,
    "text": "which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. __________ * Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ ). . Article 83 AI systems already placed on the market or put into service 1. This Regulation shall not apply to the AI systems which are components of the large- scale IT systems established by the legal acts listed in Annex IX that have been placed on the market or put into service before [12 months after the date of application of this Regulation referred to in Article 85(2)], unless the replacement or amendment of those legal acts leads to a significant change in the design or intended purpose of the AI system or AI systems concerned. The requirements laid down in this Regulation shall be taken into account, where applicable, in the evaluation of each large-scale IT systems established by the legal acts listed in Annex IX to be undertaken as provided for in those respective acts. 2. This Regulation shall apply to the high-risk AI systems, other than the ones referred to in paragraph 1, that have been placed on the market or put into service before [date of application of this Regulation referred to in Article 85(2)], only if, from that date, those systems are subject to significant changes in their design or intended purpose. Article 84 Evaluation and review 1. The Commission shall assess the need for amendment of the list in Annex III once a year following the entry into force of this Regulation. 2. By [three years after the date of application of this Regulation referred to in Article 85(2)] and every four years thereafter,",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      89
    ],
    "titles": [
      "71(1), applied by Member States to infringements of the provisions of this"
    ],
    "chunk_index": 143,
    "text": "the Commission shall submit a report on the evaluation and review of this Regulation to the European Parliament and to the Council. The reports shall be made public. 3. The reports referred to in paragraph 2 shall devote specific attention to the following: (a) the status of the financial and human resources of the national competent authorities in order to effectively perform the tasks assigned to them under this Regulation; (b) the state of penalties, and notably administrative fines as referred to in Article 71(1), applied by Member States to infringements of the provisions of this Regulation. 4. Within [three years after the date of application of this Regulation referred to in Article 85(2)] and every four years thereafter, the Commission shall evaluate the impact and effectiveness of codes of conduct to foster the application of the requirements set out in Title III, Chapter 2 and possibly other additional requirements for AI systems other than high-risk AI systems. 5. For the purpose of paragraphs 1 to 4 the Board, the Member States and national competent authorities shall provide the Commission with information on its request. 6. In carrying out the evaluations and reviews referred to in paragraphs 1 to 4 the Commission shall take into account the positions and findings of the Board, of the European Parliament, of the Council, and of other relevant bodies or sources. 7. The Commission shall, if necessary, submit appropriate proposals to amend this Regulation, in particular taking into account developments in technology and in the light of the state of progress in the information society. Article 85 Entry into force and application 1. This Regulation shall enter into force on the twentieth day following that of its publication in the Official Journal of the European Union. 2. This Regulation shall apply from [24 months",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      90
    ],
    "titles": [
      "LEGISLATIVE FINANCIAL STATEMENT"
    ],
    "chunk_index": 144,
    "text": "following the entering into force of the Regulation]. 3. By way of derogation from paragraph 2: (a) Title III, Chapter 4 and Title VI shall apply from [three months following the entry into force of this Regulation]; (b) Article 71 shall apply from [twelve months following the entry into force of this Regulation]. This Regulation shall be binding in its entirety and directly applicable in all Member States. Done at Brussels, For the European Parliament For the Council The President The President LEGISLATIVE FINANCIAL STATEMENT 1. FRAMEWORK OF THE PROPOSAL/INITIATIVE 1.1. Title of the proposal/initiative 1.2. Policy area(s) concerned 1.3. The proposal/initiative relates to: 1.4. Objective(s) 1.4.1. General objective(s) 1.4.2. Specific objective(s) 1.4.3. Expected result(s) and impact 1.4.4. Indicators of performance 1.5. Grounds for the proposal/initiative 1.5.1. Requirement(s) to be met in the short or long term including a detailed timeline for roll-out of the implementation of the initiative 1.5.2. Added value of Union involvement (it may result from different factors, e.g. coordination gains, legal certainty, greater effectiveness or complementarities). For the purposes of this point 'added value of Union involvement' is the value resulting from Union intervention which is additional to the value that would have been otherwise created by Member States alone 1.5.3. Lessons learned from similar experiences in the past 1.5.4. Compatibility with the Multiannual Financial Framework and possible synergies with other appropriate instruments 1.5.5 Assessment of the different available financing options, including scope for redeployment 1.6. Duration and financial impact of the proposal/initiative 1.7. Management mode(s) planned 2. MANAGEMENT MEASURES 2.1. Monitoring and reporting rules 2.2. Management and control system 2.2.1. Justification of the management mode(s), the funding implementation mechanism(s), the payment modalities and the control strategy proposed 2.2.2. Information concerning the risks identified and the internal control system(s) set up to mitigate them 2.2.3. Estimation",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      91,
      92
    ],
    "titles": [
      "2.3. Measures to prevent fraud and irregularities",
      "LEGISLATIVE FINANCIAL STATEMENT"
    ],
    "chunk_index": 145,
    "text": "and justification of the cost-effectiveness of the controls (ratio of \"control costs value of the related funds managed\"), and assessment of the expected levels of risk of error (at payment & at closure) 2.3. Measures to prevent fraud and irregularities 3. ESTIMATED FINANCIAL IMPACT OF THE PROPOSAL/INITIATIVE 3.1. Heading(s) of the multiannual financial framework and expenditure budget line(s) affected 3.2. Estimated financial impact of the proposal on appropriations 3.2.1. Summary of estimated impact on operational appropriations 3.2.2. Estimated output funded with operational appropriations 3.2.3. Summary of estimated impact on administrative appropriations 3.2.4. Compatibility with the current multiannual financial framework 3.2.5. Third-party contributions 3.3. Estimated impact on revenue LEGISLATIVE FINANCIAL STATEMENT 1. FRAMEWORK OF THE PROPOSAL/INITIATIVE 1.1. Title of the proposal/initiative Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts 1.2. Policy area(s) concerned Communications Networks, Content and Technology; Internal Market, Industry, Entrepreneurship and SMEs; The budgetary impact concerns the new tasks entrusted with the Commission, including the support to the EU AI Board; Activity: Shaping Europe's digital future. 1.3. The proposal/initiative relates to: X a new action a new action following a pilot project/preparatory action64 the extension of an existing action an action redirected towards a new action 1.4. Objective(s) 1.4.1. General objective(s) The general objective of the intervention is to ensure the proper functioning of the single market by creating the conditions for the development and use of trustworthy artificial intelligence in the Union. 1.4.2. Specific objective(s) Specific objective No 1 To set requirements specific to AI systems and obligations on all value chain participants in order to ensure that AI systems placed on the market and used are safe and respect existing law on fundamental rights and Union values; Specific objective No",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      93,
      94
    ],
    "titles": [
      "1.4.3."
    ],
    "chunk_index": 146,
    "text": "2 To ensure legal certainty to facilitate investment and innovation in AI by making it clear what essential requirements, obligations, as well as conformity and compliance procedures must be followed to place or use an AI system in the Union market; Specific objective No 3 To enhance governance and effective enforcement of existing law on fundamental rights and safety requirements applicable to AI systems by providing new powers, resources and clear rules for relevant authorities on conformity assessment and ex As referred to in Article 54(2)(a) or (b) of the Financial Regulation post monitoring procedures and the division of governance and supervision tasks between national and EU levels; Specific objective No 4 To facilitate the development of a single market for lawful, safe and trustworthy AI applications and prevent market fragmentation by taking EU action to set minimum requirement for AI systems to be placed and used in the Union market in compliance with existing law on fundamental rights and safety. 1.4.3. Expected result(s) and impact Specify the effects which the proposal/initiative should have on the beneficiaries/groups targeted. AI suppliers should benefit from a minimal but clear set of requirements, creating legal certainty and ensuring access to the entire single market. AI users should benefit from legal certainty that the high-risk AI systems they buy comply with European laws and values. Consumers should benefit by reducing the risk of violations of their safety or fundamental rights. 1.4.4. Indicators of performance Specify the indicators for monitoring implementation of the proposal/initiative. Indicator 1 Number of serious incidents or AI performances which constitute a serious incident or a breach of fundamental rights obligations (semi-annual) by fields of applications and calculated a) in absolute terms, b) as share of applications deployed and c) as share of citizens concerned. Indicator 2 a) Total AI investment",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [
      95
    ],
    "titles": [
      "1.5.3."
    ],
    "chunk_index": 147,
    "text": "in the EU (annual) b) Total AI investment by Member State (annual) c) Share of companies using AI (annual) d) Share of SMEs using AI (annual) a) and b) will be calculated based on official sources and benchmarked against private estimates c) and d) will be collected by regular company surveys 1.5. Grounds for the proposal/initiative 1.5.1. Requirement(s) to be met in the short or long term including a detailed timeline for roll-out of the implementation of the initiative The Regulation should be fully applicable one year and a half after its adoption. However, elements of the governance structure should be in place before then. In particular, Member States shall have appointed existing authorities and/or established new authorities performing the tasks set out in the legislation earlier, and the EU AI Board should be set-up and effective. By the time of applicability, the European database of AI systems should be fully operative. In parallel to the adoption process, it is therefore necessary to develop the database, so that its development has come to an end when the regulation enters into force. 1.5.2. Added value of Union involvement (it may result from different factors, e.g. coordination gains, legal certainty, greater effectiveness or complementarities). For the purposes of this point 'added value of Union involvement' is the value resulting from Union intervention which is additional to the value that would have been otherwise created by Member States alone. An emerging patchy framework of potentially divergent national rules will hamper the seamless provision of AI systems across the EU and is ineffective in ensuring the safety and protection of fundamental rights and Union values across the different Member States. A common EU legislative action on AI could boost the internal market and has great potential to provide European industry with a competitive edge",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 148,
    "text": "at the global scene and economies of scale that cannot be achieved by individual Member States alone. 1.5.3. Lessons learned from similar experiences in the past The E-commerce Directive 2000/31/EC provides the core framework for the functioning of the single market and the supervision of digital services and sets a basic structure for a general cooperation mechanism among Member States, covering in principle all requirements applicable to digital services. The evaluation of the Directive pointed to shortcomings in several aspects of this cooperation mechanism, including important procedural aspects such as the lack of clear timeframes for response from Member States coupled with a general lack of responsiveness to requests from their counterparts. This has led over the years to a lack of trust between Member States in addressing concerns about providers offering digital services cross-border. The evaluation of the Directive showed the need to define a differentiated set of rules and requirements at European level. For this reason, the implementation of the specific obligations laid down in this Regulation would require a specific cooperation mechanism at EU level, with a governance structure ensuring coordination of specific responsible bodies at EU level. 1.5.4. Compatibility with the Multiannual Financial Framework and possible synergies with other appropriate instruments The Regulation Laying Down Harmonised Rules on Artificial Intelligence and Amending Certain Union Legislative Acts defines a new common framework of requirements applicable to AI systems, which goes well beyond the framework provided by existing legislation. For this reason, a new national and European regulatory and coordination function needs to be established with this proposal. As regards possible synergies with other appropriate instruments, the role of notifying authorities at national level can be performed by national authorities fulfilling similar functions sunder other EU regulations. Moreover, by increasing trust in AI and thus encouraging investment in",
    "n_words": 300
  },
  {
    "pdf": "eu_ai_act_regulation.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 149,
    "text": "development and adoption of AI, it complements Digital Europe, for which promoting the diffusion of AI is one of five priorities. 1.5.5. Assessment of the different available financing options, including scope for redeployment The staff will be redeployed. The other costs will be supported from the DEP. envelope, given that the objective of this regulation ensuring trustworthy AI contributes directly to one key objective of Digital Europe accelerating AI development and deployment in Europe.",
    "n_words": 74
  },
  {
    "pdf": "oecd_legal_0449_en.pdf",
    "pages": [
      3
    ],
    "titles": [],
    "chunk_index": 1,
    "text": "Date(s) Adopted on 22/05/2019 Background Information The Recommendation on Artificial Intelligence (AI) the first intergovernmental standard on AI was adopted by the OECD Council at Ministerial level on 22 May 2019 on the proposal of the Committee on Digital Economy Policy (CDEP). The Recommendation aims to foster innovation and trust in AI by promoting the responsible stewardship of trustworthy AI while ensuring respect for human rights and democratic values. Complementing existing OECD standards in areas such as privacy, digital security risk management, and responsible business conduct, the Recommendation focuses on AI-specific issues and sets a standard that is implementable and sufficiently flexible to stand the test of time in this rapidly evolving field. The Recommendation identifies five complementary values-based principles for the responsible stewardship of trustworthy AI and calls on AI actors to promote and implement them: inclusive growth, sustainable development and well-being; human-centred values and fairness; transparency and explainability; robustness, security and safety; and accountability. In addition to and consistent with these value-based principles, the Recommendation also provides five recommendations to policy-makers pertaining to national policies and international co-operation for trustworthy AI, namely: investing in AI research and development; fostering a digital ecosystem for AI; shaping an enabling policy environment for AI; building human capacity and preparing for labour market transformation; and international co-operation for trustworthy AI. The Recommendation also includes a provision for the development of metrics to measure AI research, development and deployment, and for building an evidence base to assess progress in its implementation. The OECD s work on Artificial Intelligence and rationale for developing the OECD Recommendation on Artificial Intelligence Artificial Intelligence (AI) is a general-purpose technology that has the potential to improve the welfare and well-being of people, to contribute to positive sustainable global economic activity, to increase innovation and productivity, and to help",
    "n_words": 300
  },
  {
    "pdf": "oecd_legal_0449_en.pdf",
    "pages": [
      4
    ],
    "titles": [
      "1. Principles for responsible stewardship of trustworthy AI: the first section sets out five"
    ],
    "chunk_index": 2,
    "text": "respond to key global challenges. It is deployed in many sectors ranging from production, finance and transport to healthcare and security. Alongside benefits, AI also raises challenges for our societies and economies, notably regarding economic shifts and inequalities, competition, transitions in the labour market, and implications for democracy and human rights. The OECD has undertaken empirical and policy activities on AI in support of the policy debate over the past two years, starting with a Technology Foresight Forum on AI in 2016 and an international conference on AI: Intelligent Machines, Smart Policies in 2017. The Organisation also conducted analytical and measurement work that provides an overview of the AI technical landscape, maps economic and social impacts of AI technologies and their applications, identifies major policy considerations, and describes AI initiatives from governments and other stakeholders at national and international levels. This work has demonstrated the need to shape a stable policy environment at the international level to foster trust in and adoption of AI in society. Against this background, the OECD Committee on Digital Economy Policy (CDEP) agreed to develop a draft Council Recommendation to promote a human-centric approach to trustworthy AI, that fosters research, preserves economic incentives to innovate, and applies to all stakeholders. Complementing existing OECD standards already relevant to AI such as those on privacy and data protection, digital security risk management, and responsible business conduct the Recommendation focuses on policy issues that are specific to AI and strives to set a standard that is implementable and flexible enough to stand the test of time in a rapidly evolving field. The Recommendation contains five high-level values-based principles and five recommendations for national policies and international co-operation. It also proposes a common understanding of key terms, such as AI system and AI actors , for the purposes of",
    "n_words": 300
  },
  {
    "pdf": "oecd_legal_0449_en.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 3,
    "text": "the Recommendation. More specifically, the Recommendation includes two substantive sections: 1. Principles for responsible stewardship of trustworthy AI: the first section sets out five complementary principles relevant to all stakeholders: i) inclusive growth, sustainable development and well-being; ii) human-centred values and fairness; iii) transparency and explainability; iv) robustness, security and safety; and v) accountability. This section further calls on AI actors to promote and implement these principles according to their roles. 2. National policies and international co-operation for trustworthy AI: consistent with the five aforementioned principles, this section provides five recommendations to Members and non-Members having adhered to the draft Recommendation (hereafter the Adherents ) to implement in their national policies and international co-operation: i) investing in AI research and development; ii) fostering a digital ecosystem for AI; iii) shaping an enabling policy environment for AI; iv) building human capacity and preparing for labour market transformation; and v) international co-operation for trustworthy AI. An inclusive and participatory process for developing the Recommendation The development of the Recommendation was participatory in nature, incorporating input from a broad range of sources throughout the process. In May 2018, the CDEP agreed to form an expert group to scope principles to foster trust in and adoption of AI, with a view to developing a draft Council Recommendation in the course of 2019. The AI Group of experts at the OECD (AIGO) was subsequently established, comprising over 50 experts from different disciplines and different sectors (government, industry, civil society, trade unions, the technical community and academia) - September 2018 and February 2019 the group held four meetings: in Paris, France, in September and November 2018, in Cambridge, MA, United States, at the Massachusetts Institute of Technology (MIT) in January 2019, back to back with the MIT AI Policy Congress, and finally in Dubai, United Arab",
    "n_words": 300
  },
  {
    "pdf": "oecd_legal_0449_en.pdf",
    "pages": [
      5
    ],
    "titles": [
      "AI"
    ],
    "chunk_index": 4,
    "text": "Emirates, at the World Government Summit in February 2019. The work benefited from the diligence, engagement and substantive contributions of the experts participating in AIGO, as well as from their multi-stakeholder and multidisciplinary backgrounds. Drawing on the final output document of the AIGO, a draft Recommendation was developed in the CDEP and with the consultation of other relevant OECD bodies. The CDEP approved a final draft Recommendation and agreed to transmit it to the OECD Council for adoption in a special meeting on 14-15 March 2019. The OECD Council adopted the Recommendation at its meeting at Ministerial level on 22-23 May 2019. Follow-up, monitoring of implementation and dissemination tools The OECD Recommendation on AI provides the first intergovernmental standard for AI policies and a foundation on which to conduct further analysis and develop tools to support governments in their implementation efforts. In this regard, it instructs the CDEP to monitor the implementation of the Recommendation and report to the Council on its implementation and continued relevance five years after its adoption and regularly thereafter. The CDEP is also instructed to continue its work on AI, building on this Recommendation, and taking into account work in other international fora, such as UNESCO, the Council of Europe and the initiative to build an International Panel on AI (see panel-on-artificial-intelligence). In order to support implementation of the Recommendation, the Council instructed the CDEP to develop practical guidance for implementation, to provide a forum for exchanging information on AI policy and activities, and to foster multi-stakeholder and interdisciplinary dialogue. This will be achieved largely through the OECD AI Policy Observatory, an inclusive hub for public policy on AI that aims to help countries encourage, nurture and monitor the responsible development of trustworthy artificial intelligence systems for the benefit of society. It will combine resources",
    "n_words": 300
  },
  {
    "pdf": "oecd_legal_0449_en.pdf",
    "pages": [
      6
    ],
    "titles": [
      "THE COUNCIL,"
    ],
    "chunk_index": 5,
    "text": "from across the OECD with those of partners from all stakeholder groups to provide multidisciplinary, evidence-based policy analysis on AI. The Observatory is planned to be launched late 2019 and will include a live database of AI strategies, policies and initiatives that countries and other stakeholders can share and update, enabling the comparison of their key elements in an interactive manner. It will also be continuously updated with AI metrics, measurements, policies and good practices that could lead to further updates in the practical guidance for implementation. The Recommendation is open to non-OECD Member adherence, underscoring the global relevance of OECD AI policy work as well as the Recommendation s call for international co-operation. For further information please consult: oecd.ai. Contact information: ai@oecd.org. THE COUNCIL, HAVING REGARD to Article 5 b) of the Convention on the Organisation for Economic Co-operation and Development of 14 December 1960; HAVING REGARD to the OECD Guidelines for Multinational Enterprises [OECD/LEGAL/0144]; Recommendation of the Council concerning Guidelines Governing the Protection of Privacy and Transborder Flows of Personal Data [OECD/LEGAL/0188]; Recommendation of the Council concerning Guidelines for Cryptography Policy [OECD/LEGAL/0289]; Recommendation of the Council for Enhanced Access and More Effective Use of Public Sector Information [OECD/LEGAL/0362]; Recommendation of the Council on Digital Security Risk Management for Economic and Social Prosperity [OECD/LEGAL/0415]; Recommendation of the Council on Consumer Protection in E-commerce [OECD/LEGAL/0422]; Declaration on the Digital Economy: Innovation, Growth and Social Prosperity (Canc n Declaration) [OECD/LEGAL/0426]; Declaration on Strengthening SMEs and Entrepreneurship for Productivity and Inclusive Growth [OECD/LEGAL/0439]; as well as the 2016 Ministerial Statement on Building more Resilient and Inclusive Labour Markets, adopted at the OECD Labour and Employment Ministerial Meeting; HAVING REGARD to the Sustainable Development Goals set out in the 2030 Agenda for Sustainable Development adopted by the United Nations General Assembly (A/RES/70/1) as",
    "n_words": 300
  },
  {
    "pdf": "oecd_legal_0449_en.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 6,
    "text": "well as the 1948 Universal Declaration of Human Rights; HAVING REGARD to the important work being carried out on artificial intelligence (hereafter, AI ) in other international governmental and non-governmental fora; RECOGNISING that AI has pervasive, far-reaching and global implications that are transforming societies, economic sectors and the world of work, and are likely to increasingly do so in the future; RECOGNISING that AI has the potential to improve the welfare and well-being of people, to contribute to positive sustainable global economic activity, to increase innovation and productivity, and to help respond to key global challenges; RECOGNISING that, at the same time, these transformations may have disparate effects within, and between societies and economies, notably regarding economic shifts, competition, transitions in the labour market, inequalities, and implications for democracy and human rights, privacy and data protection, and digital security; RECOGNISING that trust is a key enabler of digital transformation; that, although the nature of future AI applications and their implications may be hard to foresee, the trustworthiness of AI systems is a key factor for the diffusion and adoption of AI; and that a well-informed whole-of-society public debate is necessary for capturing the beneficial potential of the technology, while limiting the risks associated with it; UNDERLINING that certain existing national and international legal, regulatory and policy frameworks already have relevance to AI, including those related to human rights, consumer and personal data protection, intellectual property rights, responsible business conduct, and competition, while noting that the appropriateness of some frameworks may need to be assessed and new approaches developed; RECOGNISING that given the rapid development and implementation of AI, there is a need for a stable policy environment that promotes a human-centric approach to trustworthy AI, that fosters research, preserves economic incentives to innovate, and that applies to all stakeholders according",
    "n_words": 300
  },
  {
    "pdf": "oecd_legal_0449_en.pdf",
    "pages": [
      7
    ],
    "titles": [
      "I."
    ],
    "chunk_index": 7,
    "text": "to their role and the context; CONSIDERING that embracing the opportunities offered, and addressing the challenges raised, by AI applications, and empowering stakeholders to engage is essential to fostering adoption of trustworthy AI in society, and to turning AI trustworthiness into a competitive parameter in the global marketplace; On the proposal of the Committee on Digital Economy Policy: I. AGREES that for the purpose of this Recommendation the following terms should be understood as follows: AI system: An AI system is a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing real or virtual environments. AI systems are designed to operate with varying levels of autonomy. AI system lifecycle: AI system lifecycle phases involve: i) design, data and models ; which is a context-dependent sequence encompassing planning and design, data collection and processing, as well as model building; ii) verification and validation ; iii) deployment ; and iv) operation and monitoring . These phases often take place in an iterative manner and are not necessarily sequential. The decision to retire an AI system from operation may occur at any point during the operation and monitoring phase. AI knowledge: AI knowledge refers to the skills and resources, such as data, code, algorithms, models, research, know-how, training programmes, governance, processes and best practices, required to understand and participate in the AI system lifecycle. AI actors: AI actors are those who play an active role in the AI system lifecycle, including organisations and individuals that deploy or operate AI. Stakeholders: Stakeholders encompass all organisations and individuals involved in, or affected by, AI systems, directly or indirectly. AI actors are a subset of stakeholders. Section 1: Principles for responsible stewardship of trustworthy AI II. RECOMMENDS that Members and non-Members adhering to this Recommendation (hereafter the Adherents",
    "n_words": 300
  },
  {
    "pdf": "oecd_legal_0449_en.pdf",
    "pages": [
      8
    ],
    "titles": [
      "1.3."
    ],
    "chunk_index": 8,
    "text": ") promote and implement the following principles for responsible stewardship of trustworthy AI, which are relevant to all stakeholders. III. CALLS ON all AI actors to promote and implement, according to their respective roles, the following Principles for responsible stewardship of trustworthy AI. IV. UNDERLINES that the following principles are complementary and should be considered as a whole. 1.1. Inclusive growth, sustainable development and well-being Stakeholders should proactively engage in responsible stewardship of trustworthy AI in pursuit of beneficial outcomes for people and the planet, such as augmenting human capabilities and enhancing creativity, advancing inclusion of underrepresented populations, reducing economic, social, gender and other inequalities, and protecting natural environments, thus invigorating inclusive growth, sustainable development and well-being. 1.2. Human-centred values and fairness a) AI actors should respect the rule of law, human rights and democratic values, throughout the AI system lifecycle. These include freedom, dignity and autonomy, privacy and data protection, non- discrimination and equality, diversity, fairness, social justice, and internationally recognised labour rights. b) To this end, AI actors should implement mechanisms and safeguards, such as capacity for human determination, that are appropriate to the context and consistent with the state of art. 1.3. Transparency and explainability AI Actors should commit to transparency and responsible disclosure regarding AI systems. To this end, they should provide meaningful information, appropriate to the context, and consistent with the state of art: i. to foster a general understanding of AI systems, ii. to make stakeholders aware of their interactions with AI systems, including in the workplace, iii. to enable those affected by an AI system to understand the outcome, and, iv. to enable those adversely affected by an AI system to challenge its outcome based on plain and easy-to-understand information on the factors, and the logic that served as the basis for the",
    "n_words": 300
  },
  {
    "pdf": "oecd_legal_0449_en.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 9,
    "text": "prediction, recommendation or decision. 1.4. Robustness, security and safety a) AI systems should be robust, secure and safe throughout their entire lifecycle so that, in conditions of normal use, foreseeable use or misuse, or other adverse conditions, they function appropriately and do not pose unreasonable safety risk. b) To this end, AI actors should ensure traceability, including in relation to datasets, processes and decisions made during the AI system lifecycle, to enable analysis of the AI system s outcomes and responses to inquiry, appropriate to the context and consistent with the state of art. c) AI actors should, based on their roles, the context, and their ability to act, apply a systematic risk management approach to each phase of the AI system lifecycle on a continuous basis to address risks related to AI systems, including privacy, digital security, safety and bias. 1.5. Accountability AI actors should be accountable for the proper functioning of AI systems and for the respect of the above principles, based on their roles, the context, and consistent with the state of art. Section 2: National policies and international co-operation for trustworthy AI V. RECOMMENDS that Adherents implement the following recommendations, consistent with the principles in section 1, in their national policies and international co-operation, with special attention to small and medium-sized enterprises (SMEs). 2.1. Investing in AI research and development a) Governments should consider long-term public investment, and encourage private investment, in research and development, including interdisciplinary efforts, to spur innovation in trustworthy AI that focus on challenging technical issues and on AI-related social, legal and ethical implications and policy issues. b) Governments should also consider public investment and encourage private investment in open datasets that are representative and respect privacy and data protection to support an environment for AI research and development that is free",
    "n_words": 300
  },
  {
    "pdf": "oecd_legal_0449_en.pdf",
    "pages": [
      9
    ],
    "titles": [
      "2.3."
    ],
    "chunk_index": 10,
    "text": "of inappropriate bias and to improve interoperability and use of standards. 2.2. Fostering a digital ecosystem for AI Governments should foster the development of, and access to, a digital ecosystem for trustworthy AI. Such an ecosystem includes in particular digital technologies and infrastructure, and mechanisms for sharing AI knowledge, as appropriate. In this regard, governments should consider promoting mechanisms, such as data trusts, to support the safe, fair, legal and ethical sharing of data. 2.3. Shaping an enabling policy environment for AI a) Governments should promote a policy environment that supports an agile transition from the research and development stage to the deployment and operation stage for trustworthy AI systems. To this effect, they should consider using experimentation to provide a controlled environment in which AI systems can be tested, and scaled-up, as appropriate. b) Governments should review and adapt, as appropriate, their policy and regulatory frameworks and assessment mechanisms as they apply to AI systems to encourage innovation and competition for trustworthy AI. 2.4. Building human capacity and preparing for labour market transformation a) Governments should work closely with stakeholders to prepare for the transformation of the world of work and of society. They should empower people to effectively use and interact with AI systems across the breadth of applications, including by equipping them with the necessary skills. b) Governments should take steps, including through social dialogue, to ensure a fair transition for workers as AI is deployed, such as through training programmes along the working life, support for those affected by displacement, and access to new opportunities in the labour market. c) Governments should also work closely with stakeholders to promote the responsible use of AI at work, to enhance the safety of workers and the quality of jobs, to foster entrepreneurship and productivity, and aim to ensure",
    "n_words": 300
  },
  {
    "pdf": "oecd_legal_0449_en.pdf",
    "pages": [],
    "titles": [],
    "chunk_index": 11,
    "text": "that the benefits from AI are broadly and fairly shared. 2.5. International co-operation for trustworthy AI a) Governments, including developing countries and with stakeholders, should actively co-operate to advance these principles and to progress on responsible stewardship of trustworthy AI. b) Governments should work together in the OECD and other global and regional fora to foster the sharing of AI knowledge, as appropriate. They should encourage international, cross-sectoral and open multi-stakeholder initiatives to garner long-term expertise on AI. c) Governments should promote the development of multi-stakeholder, consensus-driven global technical standards for interoperable and trustworthy AI. d) Governments should also encourage the development, and their own use, of internationally comparable metrics to measure AI research, development and deployment, and gather the evidence base to assess progress in the implementation of these principles. VI. INVITES the Secretary-General and Adherents to disseminate this Recommendation. VII. INVITES non-Adherents to take due account of, and adhere to, this Recommendation. VIII. INSTRUCTS the Committee on Digital Economy Policy: a) to continue its important work on artificial intelligence building on this Recommendation and taking into account work in other international fora, and to further develop the measurement framework for evidence-based AI policies; b) to develop and iterate further practical guidance on the implementation of this Recommendation, and to report to the Council on progress made no later than end December 2019; c) to provide a forum for exchanging information on AI policy and activities including experience with the implementation of this Recommendation, and to foster multi-stakeholder and interdisciplinary dialogue to promote trust in and adoption of AI; and",
    "n_words": 261
  }
]